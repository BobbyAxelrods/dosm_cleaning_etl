{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea337d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time \n",
    "import traceback \n",
    "\n",
    "import sys \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import shutil\n",
    "import openpyxl\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d9f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE RUN THIS QUERY TO SIMULATE UPDATE IN TABLE BEFORE RUNNING SCRIPT \n",
    "# INSERT INTO reference_data.\"USER_INPUT\" (\"year\", halfyear) VALUES(2021, 2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c05492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get user input from UI in db input \n",
    "# Schema name and condition values\n",
    "db_params = {\n",
    "    'host': '10.251.49.51',\n",
    "    'database': 'postgres',\n",
    "    'user': 'admin',\n",
    "    'password': 'admin'\n",
    "}\n",
    "\n",
    "schema_name = 'reference_data'\n",
    "table_name = 'USER_INPUT'\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# cursor.execute((f'SELECT * from {schema_name}.\"{table_name}\" ORDER BY timestamp_column ')\n",
    "cursor.execute(f'''\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS row_num\n",
    "        FROM {schema_name}.\"{table_name}\"\n",
    "    ) AS numbered\n",
    "''')\n",
    "rows= cursor.fetchall()\n",
    "\n",
    "columns = [x[0] for x in cursor.description]\n",
    "df = pd.DataFrame(rows,columns=columns)\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "               \n",
    "#add function to truncate table each time new data coming in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3519ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e76e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the latest user input data numbered by max row \n",
    "max_row_num = df['row_num'].max()\n",
    "max_row = df[df['row_num'] == max_row_num]\n",
    "#MODULE 0 : QUERY SQL TABLE BASED ON USER INPUT VALUE FROM FRONT END \n",
    "HALFYEAR = max_row['halfyear'].values[0]\n",
    "YEAR = max_row['year'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9ceb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We are cleaning STB_HALFYEAR_{HALFYEAR}_{YEAR} Survey Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0293c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query table that contain halfyear and years selectfion \n",
    "\n",
    "import psycopg2\n",
    "\n",
    "def get_tables_with_quarter_year(cursor, schema_name, quarter, year):\n",
    "    # Get a list of all table names in the specified schema\n",
    "    cursor.execute(f\"SELECT table_name FROM information_schema.tables WHERE table_schema = '{schema_name}'\")\n",
    "    table_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    # List to store the tables that meet the condition\n",
    "    result_tables = []\n",
    "\n",
    "    for table_name in table_names:\n",
    "        # Check if the table has the specified columns 'halfyear' and 'year'\n",
    "        cursor.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_schema = '{schema_name}' AND table_name = '{table_name}'\")\n",
    "        column_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        if 'HALFYEAR' in column_names and 'YEARS' in column_names:\n",
    "            # Execute a query to check if the table has any records with quarter=1 and year=2021\n",
    "            cursor.execute(f'''SELECT 1 FROM {schema_name}.\"{table_name}\" WHERE \"HALFYEAR\" = {HALFYEAR} AND \"YEARS\" = {YEAR} LIMIT 1''')\n",
    "            if cursor.fetchone() is not None:\n",
    "                result_tables.append(table_name)\n",
    "\n",
    "    return result_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5b7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine_specific = create_engine('postgresql+psycopg2://admin:admin@10.251.49.51:5432/postgres',connect_args={'options':'-csearch_path={}'.format('production_indicator_viz')})\n",
    "\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'host': '10.251.49.51',\n",
    "    'database': 'postgres',\n",
    "    'user': 'admin',\n",
    "    'password': 'admin'\n",
    "}\n",
    "\n",
    "# Schema name and condition values\n",
    "\n",
    "schema_name = 'production_micro_final_stb_quarterly'\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get the tables that meet the condition\n",
    "result_tables = get_tables_with_quarter_year(cursor, schema_name, HALFYEAR, YEAR)\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Tables with {HALFYEAR} and {YEAR}:\")\n",
    "print(result_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3f35f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store data in dicitonary for wrangling \n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#store all dataframe in after loop in a dictionary \n",
    "dataframe_dict = {}\n",
    "\n",
    "#From result table , query table name and store the value in DF \n",
    "for table_name in result_tables:\n",
    "    query = f'''SELECT * FROM {schema_name}.\"{table_name}\"'''\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "    columns = [y[0] for y in cursor.description]\n",
    "    df = pd.DataFrame(data,columns=columns)\n",
    "    \n",
    "    dataframe_dict[table_name] = df\n",
    "    \n",
    "    print(f'{table_name} has been stored in dictionary for quality check')\n",
    "    \n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f23670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization \n",
    "\n",
    "# Create a new dictionary to store the updated DataFrames\n",
    "updated_dataframe_dict = {}\n",
    "\n",
    "# Loop through the tables in result_tables\n",
    "for x in result_tables:\n",
    "    # Update columns of the DataFrame and store it in the new dictionary\n",
    "    updated_dataframe = dataframe_dict[x].copy()  # Create a copy of the original DataFrame\n",
    "    updated_dataframe.columns = updated_dataframe.columns.str.upper().str.replace(r'[\\W_]+', '')\n",
    "    \n",
    "    # Store the updated DataFrame in the new dictionary\n",
    "    updated_dataframe_dict[x] = updated_dataframe\n",
    "    \n",
    "    # Print a message\n",
    "    print(f'Columns for table {x} have been standardized to non-whitespace, no special characters, and all uppercase.')\n",
    "\n",
    "# Now, updated_dataframe_dict contains the updated DataFrames\n",
    "\n",
    "\n",
    "\n",
    "# OLD METHOD \n",
    "\n",
    "# for x in result_tables:\n",
    "#     dataframe_dict[x].columns = dataframe_dict[x].columns.str.upper().str.replace(r'[\\W_]+','')\n",
    "#     print(f'Column for table {x} has been standardize to non-whitespace, no special charac & all uppercase ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982f4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_size= []\n",
    "for table_name in result_tables:\n",
    "    if table_name in dataframe_dict:\n",
    "        table_shape = dataframe_dict[table_name].shape\n",
    "        column_size.append(table_shape[1])\n",
    "        print(f'Shape of {table_name}: {table_shape}')\n",
    "    else:\n",
    "        print(f'{table_name} not found in the dataframe_dict.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b3e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_max_size = max(column_size)\n",
    "print(f'From multi dataframe , the biggest column is {get_max_size} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b39d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging vertically 3 dataframe into 1 dataframe \n",
    "concatenated_dataframe = pd.concat(updated_dataframe_dict.values(), ignore_index=True)\n",
    "print(f'Multi final data for halfyear {HALFYEAR} for year {YEAR} have been merged into 1 data frame ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c08d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4923304",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_shape_new = concatenated_dataframe.shape\n",
    "col_diff = table_shape_new[1] - get_max_size\n",
    "print(f'Shape of merged dataframe: {table_shape_new} with column size of {table_shape_new[1]} and have {col_diff} differences between 3 dataframe  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60049e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = updated_dataframe_dict[result_tables[0]]\n",
    "df2 = updated_dataframe_dict[result_tables[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aa5bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pd.concat([df1, df2], axis=1).columns\n",
    "unique_columns = all_columns[~all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6563063",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_intersecting_columns = unique_columns[~unique_columns.isin(df1.columns) | ~unique_columns.isin(df2.columns)]\n",
    "\n",
    "if not non_intersecting_columns.empty:\n",
    "    view = pd.DataFrame(non_intersecting_columns, columns=['Non-Intersecting Columns'])\n",
    "    print(\"Columns that are not common/no intersection across multi final data:\")\n",
    "    print(view)\n",
    "else:\n",
    "    print(\"Both data source haves no discrepancies and merged effectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c56aa847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jr4 = concatenated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e8809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULE 1 : BASIC PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73ba7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "current_workingpath = os.getcwd()\n",
    "\n",
    "\n",
    "# #### PATH DECLARATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34c90a01",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### INPUT FOR HDFS\n",
    "\n",
    "t01_path = os.path.join(current_workingpath,'INPUT_T01')\n",
    "path_t02 = os.path.join(current_workingpath,'INPUT_T02')\n",
    "temp_path = os.path.join(current_workingpath,'INPUT_MAP_RINSTRATA')\n",
    "bppd_storage_path = os.path.join(current_workingpath,'INPUT_T03')\n",
    "bppd_database = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_8')\n",
    "pop_fac_check = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_7')\n",
    "popfac_path = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_7')\n",
    "bin_path = os.path.join(current_workingpath,'BIN')\n",
    "\n",
    "#### INPUT FOR NIFI SERVER ONLY \n",
    "\n",
    "# t01_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_T01'\n",
    "# path_t02 = '/home/hadoop/codes/prod_stb_monthly/INPUT_T02'\n",
    "# temp_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_RAWDATA_STB_JR4'\n",
    "# jr4_raw_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_RAWDATA_STB_JR2'\n",
    "# jr2_raw_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_T02'\n",
    "# bppd_storage_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_T03'\n",
    "# bppd_database = '/home/hadoop/codes/prod_stb_monthly/OUTPUT_QUALITYCHECK_8'\n",
    "# pop_fac_check = '/home/hadoop/codes/prod_stb_monthly/OUTPUT_QUALITYCHECK_7'\n",
    "# popfac_path ='/home/hadoop/codes/prod_stb_monthly/OUTPUT_QUALITYCHECK_7'\n",
    "# bin_path = '/home/hadoop/codes/prod_stb_monthly/BIN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2914ba2e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#read whatever files types being ingested \n",
    "def read_anything(path):\n",
    "    #Get all files avaialble in the path (path shall be only 1 file at a time to manage this )\n",
    "    get_working_files = [x for x in os.listdir(path)]\n",
    "#     get_working_files = x\n",
    "    #make this as a function later \n",
    "    if len(get_working_files) == 0:\n",
    "        print('My goodman, no files either csv nor excel found, please recheck in path the existence')\n",
    "        return None\n",
    "    \n",
    "    #Excel found \n",
    "    time_start = time.time()\n",
    "    get_types_available = os.path.splitext(get_working_files[0])[1]\n",
    "    file_name = os.path.splitext(get_working_files[0])[0]\n",
    "    if get_types_available.endswith('.xlsx'):\n",
    "        \n",
    "        time_start = time.time()\n",
    "        print('We found excel files, hence we will read it and save to df_master, hold a moment ....')\n",
    "        df_master = pd.read_excel(path+'/'+get_working_files[0],dtype=str).dropna(how='all')\n",
    "        int_columns = []\n",
    "        for col in df_master.columns:\n",
    "            if df_master[col].notnull().all() and df_master[col].str.isdigit().all():\n",
    "                int_columns.append(col)\n",
    "        \n",
    "        df_master[int_columns] = df_master[int_columns].astype(int)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        diff_time = time_end - time_start\n",
    "        print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "        return df_master\n",
    "    \n",
    "    elif get_types_available.endswith('.csv'):\n",
    "        print('We found csv files, hence we will read it and save to df_master, hold a moment ....')\n",
    "        time_start = time.time()  \n",
    "        try:\n",
    "            df_master = pd.read_csv(os.path.join(path, get_working_files[0]), skip_blank_lines=True,dtype=str).dropna(how='all')\n",
    "        except UnicodeDecodeError:\n",
    "            # If 'utf-8' fails, try 'ISO-8859-1' encoding\n",
    "            df_master = pd.read_csv(os.path.join(path, get_working_files[0]), encoding='ISO-8859-1', skip_blank_lines=True,dtype=str).dropna(how='all')\n",
    "        int_columns = []\n",
    "        for col in df_master.columns:\n",
    "            if df_master[col].notnull().all() and df_master[col].str.isdigit().all():\n",
    "                int_columns.append(col)\n",
    "        df_master[int_columns] = df_master[int_columns].astype(int)\n",
    "        time_end = time.time()\n",
    "        diff_time = time_end - time_start\n",
    "        print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "        return df_master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95868066",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = read_anything(path_t02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebd914f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Check if digit required is not enough, to add leading 0 to ensure no id 38 is complete\n",
    "def check_column_lengths(df01, columns, required_lengths):\n",
    "    for col, req_len in zip(columns, required_lengths):\n",
    "        df01[col] = df01[col].astype(str).str.zfill(req_len)\n",
    "        df01[col] = df01[col].str[:req_len]\n",
    "    print('Value in columns specified has been added with leading 0 to ensure 38 digit')\n",
    "    return df01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5bc5d32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Concatting value from selected column to generate NOID 38 \n",
    "\n",
    "def generate_new_noid(df,columns,noid_col_name):\n",
    "    df[noid_col_name] = df.loc[:,columns].astype(str).apply(''.join, axis=1)\n",
    "    print(f'{noid_col_name} has been generated by merging values from specified columns')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "147956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#check all value in columns shall be 38 (or specified by user)\n",
    "\n",
    "def check_digit_match(df,noid_col_name):\n",
    "    digit_generated = df[noid_col_name].apply(lambda x: len(str(x)))\n",
    "    counts_digit_unique = digit_generated.value_counts()\n",
    "    if len(counts_digit_unique) > 1: \n",
    "        print(f\"Recheck due to inconsistent digit in NOID {counts_digit_unique}\")\n",
    "    else: \n",
    "        counts_digit = digit_generated.unique()[0]\n",
    "        print(f'{counts_digit} consistent digits has been generated , does this tally with client requirement? ')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "666800c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Manipulate Here \n",
    "columns_sel = ['NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']\n",
    "required_lengths = [2, 2, 3, 3, 3, 3, 1, 4, 2, 1, 3, 2, 2, 1, 4, 2]\n",
    "df_jr4_new = check_column_lengths(df_jr4, columns_sel, required_lengths)\n",
    "noid_col_name = 'NOID_38'\n",
    "required_digits = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "031b96e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_new_2 = generate_new_noid(df_jr4_new,columns_sel,noid_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4731d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "check_digit_match(df_jr4_new_2,noid_col_name)\n",
    "\n",
    "\n",
    "# ## MODULE 2 : GROUPING ACCORDING TO CLIENT REQUIREMENT\n",
    "\n",
    "# \t- GROUP BY DALAM UMUR 5 TAHUN \n",
    "# \t- GROUP BY DALAM UMUR 10 TAHUN \n",
    "# \t- GROUP BY DALAM ETNIK SEMENANJUNG \n",
    "# \t- GROUP BY ETNIK SABAH (BUMIPUTRA SABAH)\n",
    "# \t- GROUP BY ETNIK SARAWAK \n",
    "# \t- GROUP BY CIT_NONCIT \n",
    "# \t- GROUP BY RIN_STRATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e38ce439",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4 = df_jr4_new_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44859f6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nan_indices = df_jr4.index[df_jr4['U'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b38bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #make columns upfronts detect dataypes and fill first with nan to ensure complete fillup\n",
    "# df_jr4['G1'] = np.nan\n",
    "# df_jr4['KU_5'] = np.nan\n",
    "# df_jr4['G2'] = np.nan\n",
    "# df_jr4['G3'] = np.nan\n",
    "# df_jr4['G4'] = np.nan\n",
    "# df_jr4['G5'] = np.nan\n",
    "# df_jr4['CIT_NONCIT'] = np.nan\n",
    "# df_jr4['RIN_STRATA'] = np.nan\n",
    "\n",
    "# # df_jr4['U'] = pd.to_numeric(df_jr4['U'], errors='coerce')\n",
    "# # df_jr4['KET'] = pd.to_numeric(df_jr4['KET'], errors='coerce')\n",
    "# # df_jr4['KW'] = pd.to_numeric(df_jr4['KW'], errors='coerce')\n",
    "# # df_jr4['ST'] = pd.to_numeric(df_jr4['ST'], errors='coerce')\n",
    "\n",
    "\n",
    "# df_jr4['U'] =df_jr4['U'].astype(int)\n",
    "# df_jr4['KET'] =df_jr4['KET'].astype(int)\n",
    "# df_jr4['KW'] =df_jr4['KW'].astype(int)\n",
    "# df_jr4['ST'] = df_jr4['ST'].astype(int) \n",
    "\n",
    "#make columns upfronts detect dataypes and fill first with nan to ensure complete fillup\n",
    "df_jr4['G1'] = np.nan\n",
    "df_jr4['KU_5'] = np.nan\n",
    "df_jr4['G2'] = np.nan\n",
    "df_jr4['G3'] = np.nan\n",
    "df_jr4['G4'] = np.nan\n",
    "df_jr4['G5'] = np.nan\n",
    "df_jr4['CIT_NONCIT'] = np.nan\n",
    "df_jr4['RIN_STRATA'] = np.nan\n",
    "\n",
    "# df_jr4['U'] = pd.to_numeric(df_jr4['U'], errors='coerce')\n",
    "# df_jr4['KET'] = pd.to_numeric(df_jr4['KET'], errors='coerce')\n",
    "# df_jr4['KW'] = pd.to_numeric(df_jr4['KW'], errors='coerce')\n",
    "# df_jr4['ST'] = pd.to_numeric(df_jr4['ST'], errors='coerce')\n",
    "\n",
    "# Convert 'U' column to numeric, replacing non-finite values with NaN\n",
    "df_jr4['U'] = pd.to_numeric(df_jr4['U'], errors='coerce')\n",
    "\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['U'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_jr4['KET'] = pd.to_numeric(df_jr4['KET'], errors='coerce')\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['KET'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_jr4['KW'] = pd.to_numeric(df_jr4['KW'], errors='coerce')\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['KW'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_jr4['ST'] = pd.to_numeric(df_jr4['ST'], errors='coerce')\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['ST'].fillna(0, inplace=True)\n",
    "\n",
    "# Now convert the 'U' column to integers\n",
    "\n",
    "df_jr4['U'] =df_jr4['U'].astype(int)\n",
    "\n",
    "df_jr4['KET'] =df_jr4['KET'].astype(int)\n",
    "df_jr4['KW'] =df_jr4['KW'].astype(int)\n",
    "df_jr4['ST'] = df_jr4['ST'].astype(int) \n",
    "\n",
    "# #### G1 Checks KU_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ce42a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conditions = [\n",
    "    df_jr4['U'].between(0, 4, inclusive='both'),\n",
    "    df_jr4['U'].between(5, 9, inclusive='both'),\n",
    "    df_jr4['U'].between(10, 14, inclusive='both'),\n",
    "    df_jr4['U'].between(15, 19, inclusive='both'),\n",
    "    df_jr4['U'].between(20, 24, inclusive='both'),\n",
    "    df_jr4['U'].between(25, 29, inclusive='both'),\n",
    "    df_jr4['U'].between(30, 34, inclusive='both'),\n",
    "    df_jr4['U'].between(35, 39, inclusive='both'),\n",
    "    df_jr4['U'].between(40, 44, inclusive='both'),\n",
    "    df_jr4['U'].between(45, 49, inclusive='both'),\n",
    "    df_jr4['U'].between(50, 54, inclusive='both'),\n",
    "    df_jr4['U'].between(55, 59, inclusive='both'),\n",
    "    df_jr4['U'].between(60, 64, inclusive='both'),\n",
    "    df_jr4['U'].between(65, 69, inclusive='both'),\n",
    "    df_jr4['U'].between(70, 74, inclusive='both'),\n",
    "    df_jr4['U'].between(75, 79, inclusive='both'),\n",
    "    df_jr4['U'].between(80, 84, inclusive='both'),\n",
    "    (df_jr4['U'] >= 85)\n",
    "]\n",
    "\n",
    "\n",
    "# Define values to fill in based on conditions\n",
    "values = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "\n",
    "# Use np.select to fill in values based on conditions\n",
    "df_jr4['KU_5'] = np.select(conditions, values, default=df_jr4['KU_5'])\n",
    "\n",
    "#Check for null values in new column created \n",
    "\n",
    "# df[['U','G1']]\n",
    "df_jr4[['KU_5']].notnull().value_counts()\n",
    "\n",
    "# add condition when theres null values, trigger alert \n",
    "if df_jr4['KU_5'].isnull().any().any():\n",
    "    print(\"Column KU_5 contains null values,please recheck data \")\n",
    "else:\n",
    "    print(\"Column KU_5 does not contain null values, proceed with next group\")\n",
    "##****** TBC \n",
    "\n",
    "\n",
    "# #### G2 Checks KU_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20ced6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    \n",
    "    df_jr4['U'].between(0, 9,inclusive = 'both'),\n",
    "    df_jr4['U'].between(10, 19,inclusive = 'both'),\n",
    "    df_jr4['U'].between(20, 29,inclusive = 'both'),\n",
    "    df_jr4['U'].between(30, 39,inclusive = 'both'),\n",
    "    df_jr4['U'].between(40, 49,inclusive = 'both'),\n",
    "    df_jr4['U'].between(50, 59,inclusive = 'both'),\n",
    "    df_jr4['U'].between(60, 69,inclusive = 'both'),\n",
    "    df_jr4['U'].between(70, 79,inclusive = 'both'),\n",
    "    df_jr4['U'].between(80, 89,inclusive = 'both'),\n",
    "    df_jr4['U'].between(90, 99,inclusive = 'both'),\n",
    "    df_jr4['U'].between(100, 109,inclusive = 'both'),\n",
    "    df_jr4['U'].between(110, 119,inclusive = 'both'),\n",
    "    df_jr4['U'].between(120, 129,inclusive = 'both')\n",
    "]\n",
    "\n",
    "\n",
    "values = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "df_jr4['G2'] = np.select(condition,values, default=df_jr4['G2'])\n",
    "\n",
    "if df_jr4['G2'].isnull().any().any():\n",
    "    print(\"Column G2 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G2 does not contain null values, proceed with next group\")\n",
    "\n",
    "\n",
    "# #### G3 Checks KET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e04331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['KET'] == 1100) | (df_jr4['KET'] == 3210)),\n",
    "    df_jr4['KET'].isin([2111, 2112, 2113, 2114, 2115, 2116, 2121, 2122, 2123, 2124, 2125, 2126, 2131, 2132, 2133, 2134, 2135, 2136, 3110, 3120, 3130, 3140, 3150, 3160, 3170, 3180, 3190, 3200, 3220, 3230, 3240, 3250, 3260, 3998, 4110, 4120, 4130, 4140, 4150, 4160, 4170, 4180, 4190, 4200, 4210, 4220, 4230, 4240, 4250, 4260, 4270, 4280, 4290, 4300, 4310, 4320, 4330, 4340, 4350, 4360, 4998]),\n",
    "    df_jr4['KET'].isin([5110, 5120, 5130, 5140, 5150, 5160, 5170, 5180, 5190, 5200, 5998]),\n",
    "    df_jr4['KET'].isin([6110, 6120, 6130, 6140, 6150, 6160, 6170, 6180, 6998]),\n",
    "    df_jr4['KET'].isin([7110, 7120,7130, 7140, 7150, 7160, 7170, 7180, 7190, 7200, 7210, 7220, 7230, 7998, 8110, 8120, 8130, 8140, 8150, 8160, 8998, 9110, 9120, 9130, 9140, 9150, 9998])\n",
    "]\n",
    " \n",
    "values = [1,2,3,4,5]\n",
    "df_jr4['G3'] = np.select(condition, values, default=df_jr4['G3'])\n",
    "if df_jr4['G3'].isnull().any().any():\n",
    "    print(\"Column G3 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G3 does not contain null values, proceed with next group\")\n",
    "    \n",
    "\n",
    "\n",
    "# #### G4 Checks KET_SAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcc50aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['KET'] == 1100) | (df_jr4['KET'] == 3210)),\n",
    "    ((df_jr4['KET'] == 3150) | (df_jr4['KET'] == 3190)),\n",
    "    ((df_jr4['KET'] == 3110)),\n",
    "    ((df_jr4['KET'] == 3220)),\n",
    "    df_jr4['KET'].isin([2111,2112,2113,2114,2115,2116, 2121,2122,2123,2124,2125,2126, 2131,2132,2133,2134,2135,2136, 3120, 3130, 3140, 3160, 3170, 3180, 3200, 3230, 3240, 3250, 3260, 3998, 4110, 4120, 4130, 4140, 4150, 4160, 4170, 4180, 4190, 4200, 4210, 4220, 4230, 4240, 4250, 4260, 4270, 4280, 4290, 4300, 4310, 4320, 4330, 4340, 4350, 4360, 4998]),\n",
    "    df_jr4['KET'].isin([ 5110, 5120, 5130, 5140, 5150, 5160, 5170, 5180, 5190, 5200, 5998]),\n",
    "    df_jr4['KET'].isin([6110, 6120, 6130, 6140, 6150, 6160, 6170, 6180, 6998, 7110, 7120,7130, 7140, 7150, 7160, 7170, 7180, 7190, 7200, 7210, 7220, 7230, 7998, 8110, 8120, 8130, 8140, 8150, 8160, 8998, 9110, 9120, 9130, 9140, 9150, 9998]),\n",
    "]\n",
    " \n",
    "values = [1,2,3,4,5,6,7]\n",
    "df_jr4['G4'] = np.select(condition, values, default=df_jr4['G4'])\n",
    "if df_jr4['G4'].isnull().any().any():\n",
    "    print(\"Column G4 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G4 does not contain null values, proceed with next group\")\n",
    "    \n",
    "\n",
    "\n",
    "# #### G5 Checks : KUMPULAN ETNIK SARAWAK  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f1af3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['KET'] == 1100) | (df_jr4['KET'] == 3210)),\n",
    "    ((df_jr4['KET'] == 4140)),\n",
    "    ((df_jr4['KET'] == 4110)),\n",
    "    ((df_jr4['KET'] == 4260)),\n",
    "    df_jr4['KET'].isin([2111,2112,2113,2114,2115,2116, 2121,2122,2123,2124,2125,2126, 2131,2132,2133,2134,2135,2136, 3110, 3120, 3130, 3140, 3150, 3160, 3170, 3180, 3190, 3200, 3220, 3230, 3240, 3250, 3260, 3998, 4120, 4130, 4150, 4160, 4170, 4180, 4190, 4200, 4210, 4220, 4230, 4240, 4250, 4270, 4280, 4290, 4300, 4310, 4320, 4330, 4340, 4350, 4360, 4998]),\n",
    "    df_jr4['KET'].isin([5110, 5120, 5130, 5140, 5150, 5160, 5170, 5180, 5190, 5200, 5998]),\n",
    "    df_jr4['KET'].isin([6110, 6120, 6130, 6140, 6150, 6160, 6170, 6180, 6998, 7110, 7120,7130, 7140, 7150, 7160, 7170, 7180, 7190, 7200, 7210, 7220, 7230, 7998, 8110, 8120, 8130, 8140, 8150, 8160, 8998, 9110, 9120, 9130, 9140, 9150, 9998]),\n",
    "]\n",
    " \n",
    "values = [1,2,3,4,5,6,7]\n",
    "df_jr4['G5'] = np.select(condition, values, default=df_jr4['G5'])\n",
    "if df_jr4['G5'].isnull().any().any():\n",
    "    print(\"Column G5 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G5 does not contain null values, proceed with next group\")\n",
    "    \n",
    "\n",
    "\n",
    "# #### G6 Checks : 6 : KUMPULAN CIT_NONCIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a32fcdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    df_jr4['KW'] == 458,\n",
    "    df_jr4['KW'] != 458\n",
    "\n",
    "]\n",
    "\n",
    "values = [1,2]\n",
    "\n",
    "df_jr4['CIT_NONCIT'] = np.select(condition,values, default=df_jr4['CIT_NONCIT'])\n",
    "mask = df_jr4[['KW','CIT_NONCIT']].isnull().any(axis=1)\n",
    "row_null = df_jr4.loc[mask]\n",
    "\n",
    "x=row_null[['KW','CIT_NONCIT']]\n",
    "\n",
    "if not x .empty:\n",
    "    print('Recheck this portion since it contain null values')\n",
    "else: \n",
    "    print(\"Column G6 does not contain null values, proceed with next group\")\n",
    "\n",
    "\n",
    "# #### G7 Checks :  KUMPULAN RIN_STRATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c283dc9f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['ST'] ==2 ) | (df_jr4['ST'] ==1 )),\n",
    "    ((df_jr4['ST'].between(3,9,inclusive = 'both')) | (df_jr4['ST'] ==0 ))\n",
    "]\n",
    "values = [1,2]\n",
    "\n",
    "df_jr4['RIN_STRATA'] = np.select(condition,values, default=df_jr4['RIN_STRATA'])\n",
    "\n",
    "mask = df_jr4[['ST','RIN_STRATA']].isnull().any(axis=1)\n",
    "row_null = df_jr4.loc[mask]\n",
    "\n",
    "x=row_null[['ST','RIN_STRATA']]\n",
    "\n",
    "if not x .empty:\n",
    "    print('Recheck this portion since it contain null values')\n",
    "else: \n",
    "    print(\"Column RIN_STRATA does not contain null values, proceed with next group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86f22353",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "end_time = time.time()\n",
    "end_time_1 = start_time - end_time\n",
    "\n",
    "print(f'Code took {end_time_1} seconds to finish phase 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2983dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_a = df_jr4\n",
    "\n",
    "\n",
    "# ## MODULE 3 : MARKING PRIMARY_FIRST FOR VARIABLES XM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af473fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#remove column name with spaces to standardize \n",
    "# columns_with_spaces = [col for col in df.columns if ' ' in col]\n",
    "columns_with_spaces = [col for col in df_jr4.columns if ' ' in col]\n",
    "new_columns = [col.replace(' ', '') for col in df_jr4.columns]\n",
    "df_jr4.columns = new_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41d70a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "XM = ['B', 'NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP']\n",
    "\n",
    "for col in XM:\n",
    "    if not df_jr4[col].dtype == 'object':\n",
    "        df_jr4[col] = df_jr4[col].astype(str)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "807f0a4d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4['PF_INPUT'] = df_jr4.apply(lambda row: ''.join(row[XM]), axis=1)\n",
    "\n",
    "#Indentify first duplicate value in column PF_MAIN\n",
    "mask_duplicate = df_jr4.duplicated(subset=['PF_INPUT'], keep='first')\n",
    "\n",
    "#Mark all value in PF_MAIN with value 2 as a starting values \n",
    "df_jr4['PF_OUTPUT'] = 2\n",
    "\n",
    "# The line of code conditions = [~mask_duplicate, mask_duplicate] creates a list of two boolean arrays\n",
    "# : ~mask_duplicate and mask_duplicate. \n",
    "#     The ~ operator is the bitwise NOT operator, which in this context is used to invert \n",
    "#     the boolean values in the mask_duplicate array.\n",
    "\n",
    "# Set values based on duplicate mask\n",
    "values = [1, 2]\n",
    "conditions = [~mask_duplicate, mask_duplicate]\n",
    "df_jr4['PF_OUTPUT'] = np.select(conditions, values, default=df_jr4['PF_OUTPUT'])\n",
    "\n",
    "# df_jr4[['PF_INPUT','PF_OUTPUT']].value_counts().sort_values('PF_INPUT')\n",
    "# print(df_jr4.groupby(['PF_INPUT','PF_OUTPUT']).size().reset_index(name='count').sort_values('PF_OUTPUT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e4abcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "end_time = time.time()\n",
    "end_time_2 = start_time - end_time\n",
    "\n",
    "print(f'Code took {end_time_2} seconds to finish phase 2 generating primary first XM ')\n",
    "\n",
    "\n",
    "# ##  MODULE 3 : ADJUSTED WEIGHT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78e616cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_bir = df_jr4[['NG','RIN_STRATA','PF_OUTPUT','PF_INPUT']]\n",
    "filtered_df = df_bir[df_bir['PF_OUTPUT'] == 1]\n",
    "df_bir1 = filtered_df.drop('PF_OUTPUT', axis=1)\n",
    "\n",
    "\n",
    "df_bir2 = df_bir[df_bir['PF_OUTPUT'] == 2]\n",
    "\n",
    "\n",
    "pivoted_pf = pd.pivot_table(df_bir1, values='PF_INPUT', index='NG', columns='RIN_STRATA', aggfunc='count')\n",
    "pivoted_dc = pd.pivot_table(df_bir2, values='PF_INPUT', index='NG', columns='RIN_STRATA', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89ad8b68",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df2 = pivoted_df.reset_index()\n",
    "df_pf = pivoted_pf.reset_index()\n",
    "\n",
    "#Dalam Bandar \n",
    "df_pf_01 = df_pf[['NG',1.0]]\n",
    "#Luar Bandar \n",
    "df_pf_02 = df_pf[['NG',2.0]]\n",
    "\n",
    "df_pf_01 = df_pf_01.reset_index()\n",
    "df_pf_02 = df_pf_02.reset_index()\n",
    "\n",
    "df_pf_01.set_index('index', inplace=True)\n",
    "df_pf_02.set_index('index', inplace=True)\n",
    "\n",
    "df_pf_01.rename(columns={'NG':'KOD_NEGERI',1.0:'BIL_ISI_RUMAH_RESPON_SELESAI'}, inplace=True)\n",
    "df_pf_02.rename(columns={'NG':'KOD_NEGERI',2.0:'BIL_ISI_RUMAH_RESPON_SELESAI'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bf3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##  MODULE 4 : BMP ADJUSTED WEIGHT DATA VALIDATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c75a96d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read T01\n",
    "df_jr5 = df_jr4\n",
    "df_t01 = read_anything(t01_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe4def2c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_t01.drop(columns=['BIL_ISI_RUMAH_RESPON_SELESAI','ADJUSTED_WEIGHT'], inplace=True)\n",
    "df_t01.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Splitting template table into 2 dataframe with 01 dalam bandar and luar bandar 02 \n",
    "df_t01_01 = df_t01[df_t01['RIN_STRATA']==1]\n",
    "df_t01_02 = df_t01[df_t01['RIN_STRATA']==2]\n",
    "\n",
    "df_pf_01['KOD_NEGERI'] = df_pf_01['KOD_NEGERI'].str.replace('.', '').astype(int)\n",
    "df_pf_02['KOD_NEGERI'] = df_pf_02['KOD_NEGERI'].str.replace('.', '').astype(int)\n",
    "df_t01_01['KOD_NEGERI'] = pd.to_numeric(df_t01_01['KOD_NEGERI'])\n",
    "df_t01_02['KOD_NEGERI'] = pd.to_numeric(df_t01_02['KOD_NEGERI'])\n",
    "\n",
    "t01_merged = pd.merge(df_t01_01,df_pf_01, on='KOD_NEGERI')\n",
    "t02_merged = pd.merge(df_t01_02,df_pf_02, on='KOD_NEGERI')\n",
    "aw_df = pd.concat([t01_merged,t02_merged], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "994082d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update df_t01 with HAFLYEAR VALUE \n",
    "df_t01['HALFYEAR'] = df_t01['QUARTER'].apply(lambda x: 1 if x in (1,2) else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98124006",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "months_t01 = df_t01['BULAN'].iloc[0]\n",
    "years_t01 = df_t01['TAHUN'].iloc[0]\n",
    "quarter_t01 = df_t01['QUARTER'].iloc[0]\n",
    "halfyear_t01 = df_t01['HALFYEAR'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec21f5d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fillna_and_convert_to_float(df, column_list):\n",
    "    \"\"\"\n",
    "    Fill NaN values with 0 and convert specified columns to the float data type.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column_list (list): A list of column names to fill NaN and convert to float.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()  # Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
    "\n",
    "    # Fill NaN with 0 for specified columns\n",
    "    for col in column_list:\n",
    "        df_copy[col].fillna(0, inplace=True)\n",
    "\n",
    "    # Convert specified columns to float data type\n",
    "    df_copy[column_list] = df_copy[column_list].astype(float)\n",
    "\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf87755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7425896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure no Nan in here dfue to arimethic requirement \n",
    "\n",
    "column_list = ['ADJUSTED_WEIGHT_BMP','BIL_ISI_RUMAH','BIL_ISI_RUMAH_RESPON_SELESAI']\n",
    "aw_df = fillna_and_convert_to_float(aw_df, column_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd2b648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# loop through the 'BIL_ISI_RUMAH_RESPON_SELESAI' column and replace NaN values with 0\n",
    "for i in range(len(aw_df)):\n",
    "    if np.isnan(aw_df.loc[i, 'BIL_ISI_RUMAH_RESPON_SELESAI']):\n",
    "        aw_df.loc[i, 'BIL_ISI_RUMAH_RESPON_SELESAI'] = 0\n",
    "        \n",
    "        \n",
    "aw_df['BIL_ISI_RUMAH_RESPON_SELESAI'] = aw_df['BIL_ISI_RUMAH_RESPON_SELESAI'].astype(int)\n",
    "aw_df['ADJUSTED_WEIGHT'] =aw_df['BIL_ISI_RUMAH'] / aw_df['BIL_ISI_RUMAH_RESPON_SELESAI'] \n",
    "aw_df['ADJUSTED_WEIGHT'] = round(aw_df['ADJUSTED_WEIGHT'], 2)\n",
    "aw_df['SEMAKAN_AW_BMP'] = (aw_df['ADJUSTED_WEIGHT'] - aw_df['ADJUSTED_WEIGHT_BMP'])\n",
    "aw_df['SEMAKAN_AW_BMP'] = round(aw_df['SEMAKAN_AW_BMP'], 2)\n",
    "aw_df.rename(columns={'KOD_NEGERI':'NG'},inplace=True )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ec8ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def nan_clear(col_name): \n",
    "    for i in range(len(aw_df)):\n",
    "        if np.isnan(aw_df.loc[i,col_name]):\n",
    "            aw_df.loc[i, 'ADJUSTED_WEIGHT_BMP'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c19b486",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nan_clear('ADJUSTED_WEIGHT_BMP')\n",
    "nan_clear('ADJUSTED_WEIGHT')\n",
    "nan_clear('SEMAKAN_AW_BMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8a56244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_temp = read_anything(temp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31f89d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aw2 = aw_df.merge(df_temp, how='left', on=('NG','RIN_STRATA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85c93ebe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_aw3 = df_aw2[['NG','RIN_STRATA','ST','ADJUSTED_WEIGHT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1256c209",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# aw_df.to_csv(bmp_awdf_compare_check+'/'+'awdf_bmp_compare_check.csv')\n",
    "\n",
    "if any(aw_df['SEMAKAN_AW_BMP'] != 0):\n",
    "    print(f'Error: SEMAKAN_AW_BMP failed because has non-zero value(s), please recheck BPPD value and jr4 new adjusted weight')\n",
    "    \n",
    "\n",
    "else: \n",
    "    print('Success: SEMAKAN_AW_BMP success since there are no differences between BPPD and DOSM adjusted weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf44030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Remap aw_df RIN STRATA to ST too ADW VALUE \n",
    "\n",
    "# MERGE AW RIN_STRATA + KOD_NEGERI + ADJUSTED_WEIGHT into a new DF = df_jr4_aw \n",
    "\n",
    "#Change KOD_NEGERI to NG \n",
    "try: \n",
    "    aw_df['NG'] = aw_df['KOD_NEGERI']\n",
    "    aw_df.drop('KOD_NEGERI', axis=1,inplace=True)\n",
    "    #Change df_jr5(RIN_STRATA) to remove floats if any to int \n",
    "\n",
    "except: \n",
    "    print(f'Light Warning !: Column KOD_NEGERI has been removed, ignore renaming column from KOD_NEGERI to NG')\n",
    "\n",
    "df_jr5['RIN_STRATA'] = df_jr5['RIN_STRATA'].astype(int)\n",
    "df_jr5['NG'] = df_jr5['NG'].str.replace('.', '').astype(int)\n",
    "df_jr5['ST']= df_jr5['ST'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae5faf95",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_aw = df_jr5.merge(df_aw3, how='left', on=('NG', 'RIN_STRATA','ST'))\n",
    "print(f'Success : JR4 has been merged with latest checked adjusted weight from BPPD & DOSM values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82c15555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_aw3_a = df_aw3\n",
    "\n",
    "\n",
    "# ##  MODULE 5 : JADUAL A_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2291f980",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "baad184d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pivoted_pf = pd.pivot_table(df_jr4_aw, values='ADJUSTED_WEIGHT',index=['CIT_NONCIT','NG','KU_5'], columns=['J','G3'], aggfunc='sum')\n",
    "# pivoted_pf.index.get_level_values(0).value_counts()\n",
    "# , removed due to unclear files provided "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "767b6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function Library \n",
    "def pivot_table(group, ng_code):\n",
    "    pivoted_pf = pd.pivot_table(df_jr4_aw, values='ADJUSTED_WEIGHT', index=['CIT_NONCIT', 'NG', 'KU_5'], columns=['J',group], aggfunc='sum')\n",
    "    filtered_pivoted_pf_sem = pivoted_pf.loc[pivoted_pf.index.get_level_values('CIT_NONCIT')==1]\n",
    "    filtered_pivoted_pf = filtered_pivoted_pf_sem.loc[filtered_pivoted_pf_sem.index.get_level_values('NG').isin(ng_code)]\n",
    "\n",
    "    # Return the output dataframe\n",
    "    return filtered_pivoted_pf\n",
    "\n",
    "def reset_frame(df):\n",
    "    df = df.reset_index()\n",
    "    df.columns = df.columns.map(lambda x: '_'.join(map(str, x)))\n",
    "    df = df.rename(columns={'CIT_NONCIT_':'CIT_NONCIT','NG_':'NG','KU_5_':'KU_5'})\n",
    "    return df\n",
    "\n",
    "def pivot_table_bw():\n",
    "    pivoted_pf = pd.pivot_table(df_jr4_aw, values='ADJUSTED_WEIGHT', index=['CIT_NONCIT','KU_5','NG'], columns=['J'], aggfunc='sum')\n",
    "    filtered_pivoted_pf = pivoted_pf.loc[pivoted_pf.index.get_level_values('CIT_NONCIT')==2]\n",
    "    # Return the output dataframe\n",
    "    return filtered_pivoted_pf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95045d7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ng_code= [1,2,3,4,5,6,7,8,9,10,11,14,16]\n",
    "group='G3'\n",
    "df_sem = pivot_table(group,ng_code)\n",
    "\n",
    "ng_code= [12,15]\n",
    "group='G4'\n",
    "df_sab = pivot_table(group,ng_code)\n",
    "\n",
    "ng_code= [13]\n",
    "group='G5'\n",
    "df_sar = pivot_table(group,ng_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd1aced7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#df_sem_1 ==  all column reset into 1 level and column name changed \n",
    "df_sem_1 = reset_frame(df_sem)\n",
    "df_sab_1 = reset_frame(df_sab)\n",
    "df_sar_1 = reset_frame(df_sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ecedbac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sar_1.columns = df_sar_1.columns.str.replace('.0','')\n",
    "df_sab_1.columns = df_sab_1.columns.str.replace('.0','')\n",
    "df_sem_1.columns = df_sem_1.columns.str.replace('.0','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1e137c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_bw = pivot_table_bw()\n",
    "df_bw = df_bw.reset_index()\n",
    "df_bw = df_bw.rename(columns={1:'1_BW',2:'2_BW'})\n",
    "df_bw = df_bw.rename(columns={'1': '1_BW', '2': '2_BW'})\n",
    "# add clause to check whether column successfully renamed from 1 to 1_BW or not\n",
    "df_bw = df_bw.drop('CIT_NONCIT',axis=1)\n",
    "if '1_BW' in df_bw.columns and '2_BW' in df_bw.columns:\n",
    "    print(\"Columns renamed successfully.\")\n",
    "else:\n",
    "    print(\"Columns renaming failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "287b53dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sem_bw_merged = df_sem_1.merge(df_bw,how='left',on=['NG','KU_5'])\n",
    "df_sab_bw_merged = df_sab_1.merge(df_bw,how='left',on=['NG','KU_5'])\n",
    "df_sar_bw_merged = df_sar_1.merge(df_bw,how='left',on=['NG','KU_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d6761c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Success: df_semenanjung, df_sabah & df_sarawak have been generated to produce trend documents')\n",
    "\n",
    "\n",
    "# ### MODULE 6 : BPPD + JADUAL A1 PRODUCE TREND DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a18dd020",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read file t02 ()\n",
    "\n",
    "df_to2 = read_anything(path_t02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f89c0bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#convert male to 1 & female to 2 \n",
    "df_to2['J'] = np.nan\n",
    "\n",
    "conditions = [\n",
    "    ((df_to2['GENDER'] == 'MALE')),\n",
    "    ((df_to2['GENDER'] == 'FEMALE')),\n",
    "]\n",
    "\n",
    "values = [1,2]\n",
    "\n",
    "df_to2['J'] = np.select(conditions,values,default=df_to2['J'])\n",
    "\n",
    "df_to2.reset_index\n",
    "df_to2['J'] = df_to2['J'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b8bd780",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Rename columns JADUAL A1\n",
    "df_to2 = df_to2.rename(columns={\n",
    "    'BPPD_MALAY': '1_SEM',\n",
    "    'BPPD_OTHER_BUMI': '2_SEM',\n",
    "    'BPPD_CHINESE': '3_SEM',\n",
    "    'BPPD_INDIAN': '4_SEM',\n",
    "    'BPPD_OTHERS': '5_SEM',\n",
    "    'BPPD_BUKAN_WARGA': 'BW_SEM',\n",
    "    \n",
    "    'BPPD_SABAH_MELAYU': '1_SAB',\n",
    "    'BPPD_SABAH_KADAZAN': '2_SAB',\n",
    "    'BPPD_SABAH_BAJAU': '3_SAB',\n",
    "    'BPPD_SABAH_MURUT': '4_SAB',\n",
    "    'BPPD_SABAH_BUMIPUTERA LAIN': '5_SAB',\n",
    "    'BPPD_SABAH_CINA': '6_SAB',\n",
    "    'BPPD_SABAH_LAIN LAIN': '7_SAB',\n",
    "    'BPPD_SABAH_BUKAN_WARGA': 'BW_SAB',\n",
    "    \n",
    "    'BPPD_SARAWAK_MELAYU': '1_SAR',\n",
    "    'BPPD_SARAWAK_IBAN': '2_SAR',\n",
    "    'BPPD_SARAWAK_BIDAYUH': '3_SAR',\n",
    "    'BPPD_SARAWAK_MELANAU': '4_SAR',\n",
    "    'BPPD_SARAWAK_BUMIPUTERA': '5_SAR',\n",
    "    'BPPD_SARAWAK_CINA': '6_SAR',\n",
    "    'BPPD_SARAWAK_LAIN_LAIN': '7_SAR',\n",
    "    'BPPD_SARAWAK_BUKAN_WARGA': 'BW_SAR',\n",
    "    'KOD_NEGERI': 'NG'\n",
    "})\n",
    "\n",
    "\n",
    "# #rename column in this format 1_X_B for Male & 2_X_B for Female \n",
    "# #make it as format of TO # GENDERNUMBER{1 OR 2} _#{NAMING FORMAT BPPD }}}\n",
    "# #AFTER COLUMN GENDER, START TO FILL IN 1_\"\"\"\"\"\" OR 2_'''''' FRONT OF EACH COLUMNS \n",
    "\n",
    "# #DROP COLUMNS \n",
    "df_to3 = df_to2.drop(columns={'NEGERI', 'AGE_GROUP', 'GENDER'})\n",
    "\n",
    "\n",
    "# # df_female = df.loc[df.index.get_level_values('GENDER') == 'MALE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b1e197f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Split male female then rename column \n",
    "def split_gender(q,df):\n",
    "    df = df_to3.loc[df_to2['J'].isin([q])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a7541baf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#FUNCTION TO 1. RENAME COLUMN NAME & 2.  THEN SPLIT IT INTO 3 UNIQUE DATAFRAME SEM , SAB & SAR \n",
    "#get the column index of KU_5 to identify column to fill in \n",
    "def rename_col(df,index_to_fill_after,fillwith):\n",
    "    cols = list(df.columns)\n",
    "    ku5_index = cols.index(index_to_fill_after)+1\n",
    "\n",
    "    #insert new column after certain column name \n",
    "    index_to_edit = cols[ku5_index:]\n",
    "    new_cols = cols[:ku5_index] + [f'{fillwith}_' + x for x in index_to_edit]\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "    #Alternatives\n",
    "    # new_cols = cols[:ku5_index+1]\n",
    "    # for x in index_to_edit:\n",
    "    #     new_cols.append('1_'+x)\n",
    "\n",
    "    # new_cols = ['1_'+ col for col in base_index]\n",
    "    # new_cols = base_index + new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1607afb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "q = 1\n",
    "df_male = pd.DataFrame()\n",
    "df = df_male\n",
    "df_male = split_gender(q,df)\n",
    "\n",
    "q = 2\n",
    "df_female = pd.DataFrame()\n",
    "df = df_female\n",
    "df_female = split_gender(q,df)\n",
    "\n",
    "df = df_male\n",
    "index_to_fill_after = 'KU_5'\n",
    "fillwith = 1\n",
    "\n",
    "df_male = rename_col(df,index_to_fill_after,fillwith)\n",
    "\n",
    "df = df_female\n",
    "index_to_fill_after = 'KU_5'\n",
    "fillwith = 2\n",
    "\n",
    "df_female = rename_col(df,index_to_fill_after,fillwith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e03c584",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#split female to 3 category sem, sab & sar \n",
    "#then convert to csv for calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dda75ba9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_col_list(df,col_start_name,col_end_name):\n",
    "    #get all column name in df \n",
    "    cols = list(df.columns)\n",
    "    #get index of the desired column name start & end \n",
    "    starting_index = cols.index(col_start_name)\n",
    "    last_col = cols.index(col_end_name)+1\n",
    "    \n",
    "    #front 2 column must maintain in each filter \n",
    "    base_index = cols[:2]\n",
    "    \n",
    "    #get the range of desired columns\n",
    "    desired_column = cols[starting_index:last_col]\n",
    "    new_col = base_index + desired_column\n",
    "    return new_col\n",
    "\n",
    "def separator(df,ng_list, col_list):\n",
    "    df = df.loc[df['NG'].isin(ng_list), col_list]\n",
    "    print('Success: Trend separated')\n",
    "    return df\n",
    "\n",
    "def merge_m_f(df1,df2):\n",
    "    df = df1.merge(df2,how='left',on=('NG','KU_5'))\n",
    "    print('Success: Trend merged')\n",
    "    return df\n",
    "\n",
    "def save(df,path,name):\n",
    "    df.to_excel(path+'/'+name+'.xlsx',index=False)\n",
    "    \n",
    "    print('Success : Files were being saved in xlsx format for quality checking. It will be moved to other container once this process finished ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "795ea92a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sem master\n",
    "df = df_male\n",
    "col_start_name = '1_1_SEM'\n",
    "col_end_name = '1_BW_SEM'\n",
    "df_sem_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sem_col_list\n",
    "ng_list = 1,2,3,4,5,6,7,8,9,10,11,14,16\n",
    "df =df_male\n",
    "df_sem_bppd_male = separator(df,ng_list, col_list)\n",
    "df_sem_bppd_male\n",
    "\n",
    "#female & sem\n",
    "df = df_female\n",
    "col_start_name = '2_1_SEM'\n",
    "col_end_name = '2_BW_SEM'\n",
    "df_sem_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sem_col_list\n",
    "ng_list = 1,2,3,4,5,6,7,8,9,10,11,14,16\n",
    "df =df_female\n",
    "df_sem_bppd_female = separator(df,ng_list, col_list)\n",
    "df_sem_bppd_female\n",
    "\n",
    "df1= df_sem_bppd_male\n",
    "df2= df_sem_bppd_female\n",
    "df_sem_bppd_master = merge_m_f(df1,df2)\n",
    "\n",
    "df = df_sem_bppd_master\n",
    "path = bppd_database #temp_path\n",
    "name = 'df_sem_bppd_master'\n",
    "save(df,path,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f64107f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sab master\n",
    "df = df_male\n",
    "col_start_name = '1_1_SAB'\n",
    "col_end_name = '1_BW_SAB'\n",
    "df_sab_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sab_col_list\n",
    "ng_list = 12,15\n",
    "df =df_male\n",
    "df_sab_bppd_male = separator(df,ng_list, col_list)\n",
    "df_sab_bppd_male\n",
    "\n",
    "#female & sem\n",
    "df = df_female\n",
    "col_start_name = '2_1_SAB'\n",
    "col_end_name = '2_BW_SAB'\n",
    "df_sab_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sab_col_list\n",
    "ng_list = 12,15\n",
    "df =df_female\n",
    "df_sab_bppd_female = separator(df,ng_list, col_list)\n",
    "df_sab_bppd_female\n",
    "\n",
    "df1= df_sab_bppd_male\n",
    "df2= df_sab_bppd_female\n",
    "df_sab_bppd_master = merge_m_f(df1,df2)\n",
    "\n",
    "df = df_sab_bppd_master\n",
    "path = bppd_database #temp_path\n",
    "name = 'df_sab_bppd_master'\n",
    "save(df,path,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "998632f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sar master\n",
    "df = df_male\n",
    "col_start_name = '1_1_SAR'\n",
    "col_end_name = '1_BW_SAR'\n",
    "df_sar_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sar_col_list\n",
    "ng_list =13,13\n",
    "df =df_male\n",
    "df_sar_bppd_male = separator(df,ng_list, col_list)\n",
    "df_sar_bppd_male\n",
    "\n",
    "#female & sar\n",
    "df = df_female\n",
    "col_start_name = '2_1_SAR'\n",
    "col_end_name = '2_BW_SAR'\n",
    "df_sar_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sar_col_list\n",
    "ng_list = 13,13\n",
    "df =df_female\n",
    "df_sar_bppd_female = separator(df,ng_list, col_list)\n",
    "df_sar_bppd_female\n",
    "\n",
    "df1= df_sar_bppd_male\n",
    "df2= df_sar_bppd_female\n",
    "df_sar_bppd_master = merge_m_f(df1,df2)\n",
    "\n",
    "df = df_sar_bppd_master\n",
    "path = bppd_database #temp_path\n",
    "name = 'df_sar_bppd_master'\n",
    "save(df,path,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aee65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "972b27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###AFTER SAVINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44ecf655",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#jaduala1 dataframe \n",
    "df_sem_a1_master = df_sem_bw_merged\n",
    "df_sab_a1_master = df_sab_bw_merged\n",
    "df_sar_a1_master = df_sar_bw_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49bce7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merging dataframe \n",
    "df1 = df_sem_bppd_master\n",
    "df2 = df_sem_a1_master\n",
    "\n",
    "df_sem_master = merge_m_f(df1,df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c557bad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merging dataframe \n",
    "df1 = df_sab_bppd_master\n",
    "df2 = df_sab_a1_master\n",
    "\n",
    "df_sab_master = merge_m_f(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82db02bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merging dataframe \n",
    "df1 = df_sar_bppd_master\n",
    "df2 = df_sar_a1_master\n",
    "\n",
    "df_sar_master = merge_m_f(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1f3aeb2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rearrange_columns(df,structure):\n",
    "    # Define a regular expression pattern to match the integer in the column names\n",
    "    pattern = r'(\\d+)'\n",
    "\n",
    "    # Extract the integer from each column name using the regular expression\n",
    "    int_cols = [(int(re.findall(pattern, col)[0]), col) for col in df.columns if re.findall(pattern, col)]\n",
    "\n",
    "    # Sort the column names based on the extracted integer\n",
    "    int_cols.sort()\n",
    "\n",
    "    # Rearrange the column names by pairing the '_SEM' columns with their corresponding non-SEM columns\n",
    "    new_cols = []\n",
    "    for i in range(0, len(int_cols), 2):\n",
    "        sem_col = f'{int_cols[i][1]}_{structure}'\n",
    "        new_cols.append(int_cols[i][1])\n",
    "        new_cols.append(sem_col)\n",
    "    new_cols.remove('KU_5_'+structure) if 'KU_5_'+structure in new_cols else print(\"'KU_5_'+structure' not found in new_cols\")\n",
    "\n",
    "\n",
    "    # Add any remaining columns that were not paired to the end of the list\n",
    "    if len(new_cols) < len(df.columns):\n",
    "        for col in df.columns:\n",
    "            if col not in new_cols:\n",
    "                new_cols.append(col)\n",
    "    if 'CIT_NONCIT' in new_cols:\n",
    "        new_cols.remove('CIT_NONCIT')\n",
    "    else:\n",
    "        print(\"'CIT_NONCIT' not found in new_cols\")\n",
    "\n",
    "    # Reorder the columns in the dataframe\n",
    "#     df = df[new_cols]\n",
    "# ** changed due to structure non sense\n",
    "    filter_nonsense = [x for x in new_cols if x in df.columns]\n",
    "    df = df[filter_nonsense]\n",
    "    print(f'{structure} re-arranged as per client requirements')\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7243d236",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df_sem_master\n",
    "structure = 'SEM'\n",
    "df_sem_master_trend = rearrange_columns(df,structure)\n",
    "\n",
    "df = df_sab_master\n",
    "structure = 'SAB'\n",
    "df_sab_master_trend = rearrange_columns(df,structure)\n",
    "\n",
    "df = df_sar_master\n",
    "structure = 'SAR'\n",
    "df_sar_master_trend = rearrange_columns(df,structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95a79fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c73b3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sem_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "335b0415",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def separator2(df,ng_list):\n",
    "    df = df.loc[df['NG'].isin(ng_list)]\n",
    "    #remove nan , to allow arimethic operation and change to int or float \n",
    "    for x in df.columns:\n",
    "        df[x] = df[x].astype(float)\n",
    "    \n",
    "    #run function to change \n",
    "    print(f'NG:{ng_list} separated into other dataframe')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7615c99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sem ng==1\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 1,1\n",
    "df_sem_master_trend_1 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==2\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 2,2\n",
    "df_sem_master_trend_2 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==3\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 3,3\n",
    "df_sem_master_trend_3 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==4\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 4,4\n",
    "df_sem_master_trend_4 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==5\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 5,5\n",
    "df_sem_master_trend_5 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==6\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 6,6\n",
    "df_sem_master_trend_6 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==7\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 7,7\n",
    "df_sem_master_trend_7 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==8\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 8,8\n",
    "df_sem_master_trend_8 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==9\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 9,9\n",
    "df_sem_master_trend_9 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==10\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 10,10\n",
    "df_sem_master_trend_10 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==11\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 11,11\n",
    "df_sem_master_trend_11 = separator2(df,ng_list)\n",
    "\n",
    "#sabah ng==12\n",
    "df = df_sab_master_trend\n",
    "ng_list = 12,12\n",
    "df_sab_master_trend_12 = separator2(df,ng_list)\n",
    "\n",
    "#sarawak ng==13\n",
    "df = df_sar_master_trend\n",
    "ng_list = 13,13\n",
    "df_sar_master_trend_13 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==14\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 14,14\n",
    "df_sem_master_trend_14 = separator2(df,ng_list)\n",
    "\n",
    "#sabah ng==15\n",
    "\n",
    "df = df_sab_master_trend\n",
    "ng_list = 15,15\n",
    "df_sab_master_trend_15 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==16\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 16,16\n",
    "df_sem_master_trend_16 = separator2(df,ng_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15e170f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to generate pop fac , we need to change nan to 0 to allow arimethic operation and change to float at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57ca0284",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#function to generate DIV column for all columns \n",
    "def generate_div(cols,region):\n",
    "    a = [col for col in cols if col.endswith(region)]\n",
    "    b = [re.findall(r'\\d{1,3}', col)[0] for col in a]\n",
    "    c = cols.filter(regex='^(' + '|'.join(b) + ')')\n",
    "    for i, row in c.iterrows():\n",
    "        for col1, value1 in row.items():\n",
    "            if col1.endswith(region):\n",
    "                col2 = col1[:-4]\n",
    "                value2 = row[col2]\n",
    "                new_col_name = col2 + '_DIV'\n",
    "                if value2 != 0:\n",
    "                    c.loc[i, new_col_name] = value1 / value2\n",
    "      \n",
    "    \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30967368",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cols = df_sem_master_trend_1\n",
    "region = 'SEM'\n",
    "df_sem_div_1 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_2\n",
    "region = 'SEM'\n",
    "df_sem_div_2 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_3\n",
    "region = 'SEM'\n",
    "df_sem_div_3 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_4\n",
    "region = 'SEM'\n",
    "df_sem_div_4 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_5\n",
    "region = 'SEM'\n",
    "df_sem_div_5 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_6\n",
    "region = 'SEM'\n",
    "df_sem_div_6 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_7\n",
    "region = 'SEM'\n",
    "df_sem_div_7 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_8\n",
    "region = 'SEM'\n",
    "df_sem_div_8 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_9\n",
    "region = 'SEM'\n",
    "df_sem_div_9 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_10\n",
    "region = 'SEM'\n",
    "df_sem_div_10 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_11\n",
    "region = 'SEM'\n",
    "df_sem_div_11 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sab_master_trend_12\n",
    "region = 'SAB'\n",
    "df_sab_div_12 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sar_master_trend_13\n",
    "region = 'SAR'\n",
    "df_sar_div_13 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_14\n",
    "region = 'SEM'\n",
    "df_sem_div_14 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sab_master_trend_15\n",
    "region = 'SAB'\n",
    "df_sab_div_15 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_16\n",
    "region = 'SEM'\n",
    "df_sem_div_16 = generate_div(cols, region)\n",
    "\n",
    "print(f'Phase Division Completed Successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6357cf26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def concatenate_div_columns(df):\n",
    "    d = [col for col in df if col.endswith('DIV')]\n",
    "    f = df.loc[:,d]\n",
    "    g = f.round(2)\n",
    "\n",
    "    col1 = pd.DataFrame([])\n",
    "    col2 = pd.DataFrame([])\n",
    "    col3 = pd.DataFrame([])\n",
    "    for column in g.columns:\n",
    "        if column.startswith('1') and not re.match(r'.*BW_DIV.*', column):\n",
    "            col1 = pd.concat([col1, g[column].reset_index(drop=True)], axis=0)\n",
    "            col1 = col1.fillna(0)\n",
    "        elif column.startswith('2') and not re.match(r'.*BW_DIV.*', column):\n",
    "            col2 = pd.concat([col2, g[column].reset_index(drop=True)], axis=0)\n",
    "            col2 = col2.fillna(0)\n",
    "        elif re.match(r'.*BW_DIV.*', column):\n",
    "            col3 = pd.concat([col3, g[column].reset_index(drop=True)], axis=0)\n",
    "            col3 = col3.fillna(0)\n",
    "\n",
    "    concatenated_df = pd.concat([col1.reset_index(drop=True), col2.reset_index(drop=True), col3.reset_index(drop=True)], axis=1)\n",
    "    concatenated_df.columns = ['Male', 'Female', 'Non Citizen']\n",
    "\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0201c65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df_sem_div_1\n",
    "df_popfac_1 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_2\n",
    "df_popfac_2 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_3\n",
    "df_popfac_3 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_4\n",
    "df_popfac_4 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_5\n",
    "df_popfac_5 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_6\n",
    "df_popfac_6 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_7\n",
    "df_popfac_7 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_8\n",
    "df_popfac_8 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_9\n",
    "df_popfac_9 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_10\n",
    "df_popfac_10 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_11\n",
    "df_popfac_11 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sab_div_12\n",
    "df_popfac_12 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sar_div_13\n",
    "df_popfac_13 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_14\n",
    "df_popfac_14 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sab_div_15\n",
    "df_popfac_15 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_16\n",
    "df_popfac_16 = concatenate_div_columns(df)\n",
    "\n",
    "print(f'Popfac calculated successfully for all negeri ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1cdc859",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(1,17):\n",
    "    df_name = f'df_popfac_{i}'\n",
    "    df = globals()[df_name]\n",
    "    df.to_excel(pop_fac_check+f'/{df_name}.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01024654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # aw_df.to_csv(aw_df_check+'/'+'ADW_CHECK.csv')\n",
    "# aw_df \n",
    "# for i in range(1,17):\n",
    "#     df_name = f'df_popfac_{i}'\n",
    "#     df = globals()[df_name]\n",
    "#     df.to_csv(qualitycheck_merged_path+'/'+f'{df_name}.csv',index=False)\n",
    "\n",
    "\n",
    "# ### MODULE POPFAC VALUE CONVERTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4739c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Store temporary popfac value in dictionary \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for x in range (1,17):\n",
    "    file_name = os.path.join(popfac_path,f'df_popfac_{x}*')\n",
    "    #make this as an array of path , to make it loopable \n",
    "    files = glob.glob(file_name)\n",
    "    if files:\n",
    "        latest_files = max(files, key=os.path.getctime)\n",
    "# os.path.getctime returns the time of the last metadata change to a file, which includes changes to the file's permissions, ownership, or timestamps.\n",
    "# os.path.getmtime returns the time of the last modification to the file's content, which includes any changes to the actual data in the file.\n",
    "# So which one you should use depends on what you consider to be the relevant change to the file.\n",
    "# In most cases, os.path.getmtime is the more appropriate choice, as it reflects changes to the actual data in the file. However, if you are interested in changes to the file's metadata, such as changes to its permissions or ownership, then os.path.getctime would be more appropriate.\n",
    "# In the context of reading the latest file in a directory, you would generally want to use os.path.getmtime, as you are likely interested in the latest version of the file's content.\n",
    "        file_list.append(latest_files)\n",
    "    \n",
    "file_dict = {}\n",
    "\n",
    "for i, file_path in enumerate(file_list):\n",
    "    df_name = f'df_{i+1}'\n",
    "    file_dict[df_name] = pd.read_excel(file_path) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b4c0ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#12 ,13 & 15 have 7 etnik number \n",
    "#the rest have 5 etnik numbers \n",
    "#ETNIK SABAH 12 & 15 \n",
    "#ETNIK SARAWAK 13 \n",
    "#ETINIK SEMENANJUNG 1 TO 11\n",
    "\n",
    "\n",
    "def popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name): \n",
    "    df = file_dict[df_num]\n",
    "    df['NG'] = ng_val\n",
    "    df = df[[columnd,'NG']]\n",
    "    df['J'] = gender_num\n",
    "    df['KU_5'] = 0\n",
    "    df['CIT_NONCIT'] = 1\n",
    "    df[etnik_label] = 0\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(entik_number):\n",
    "        start_idx = i * 18\n",
    "        end_idx = start_idx + 18\n",
    "        df.loc[start_idx:end_idx-1, etnik_label] = i+1\n",
    "\n",
    "    num_rows = df.shape[0]\n",
    "\n",
    "    seq = np.tile(np.arange(1, 19), (num_rows//18 + 1))[:num_rows]\n",
    "    df['KU_5'] = seq\n",
    "\n",
    "    df[popfac_name] = df[columnd]\n",
    "    df = df.drop(columnd,axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name): \n",
    "    df = file_dict[df_num]\n",
    "    df['NG'] = ng_val\n",
    "\n",
    "    df = df[[columnd,'NG']]\n",
    "    df['J'] = 0\n",
    "    df['KU_5'] = 0\n",
    "    df['CIT_NONCIT'] = 2\n",
    "    df[etnik_label] = 0\n",
    "    \n",
    "    for i in range(3):\n",
    "        start_idx = i*18\n",
    "        end_idx =start_idx + 18\n",
    "        df.loc[start_idx:end_idx-1, 'J'] = i+1\n",
    "\n",
    "    for i in range(5):\n",
    "        start_idx = i * 18\n",
    "        end_idx = start_idx + 18\n",
    "        df.loc[start_idx:end_idx, etnik_label] = i+1\n",
    "\n",
    "    num_rows = df.shape[0]\n",
    "\n",
    "    seq = np.tile(np.arange(1, 19), (num_rows//18 + 1))[:num_rows]\n",
    "    df['KU_5'] = seq\n",
    "\n",
    "    df[popfac_name] = df[columnd]\n",
    "    df = df.drop(columnd,axis=1)\n",
    "    df = df.dropna(subset=[popfac_name])\n",
    "    return df\n",
    "\n",
    "def combiner(df1,df2,df3):\n",
    "    df = pd.concat([df1,df2,df3], axis=0, ignore_index=True)\n",
    "    df\n",
    "    print(f'All dataframe combined successfully')\n",
    "    return df     \n",
    "#     duplicates = df_concat.columns[df_concat.columns.duplicated()]\n",
    "# The axis parameter in pd.concat() specifies the axis along which the data frames will be concatenated.\n",
    "# When axis=0, the data frames will be concatenated vertically, i.e., rows will be appended one after another, which means that the resulting data frame will have more rows.\n",
    "# When axis=1, the data frames will be concatenated horizontally, i.e., columns will be appended side by side, which means that the resulting data frame will have more columns.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4af666cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_popfac_sem = pd.DataFrame()\n",
    "df_popfac_sar = pd.DataFrame()\n",
    "df_popfac_sab = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e9c15a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ng_val = 16\n",
    "df_num = 'df_16'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 5\n",
    "etnik_label = 'G3'\n",
    "popfac_name = 'POPFAC_SEM' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sem = pd.concat([df,df_popfac_sem],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bcd14df5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Etnik SABAH 15 \n",
    "\n",
    "ng_val = 15\n",
    "df_num = 'df_15'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 7\n",
    "etnik_label = 'G4'\n",
    "popfac_name = 'POPFAC_SAB' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sab = pd.concat([df,df_popfac_sab],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d2161764",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Etnik SABAH  12 \n",
    "\n",
    "ng_val = 12\n",
    "df_num = 'df_12'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 7\n",
    "etnik_label = 'G4'\n",
    "popfac_name = 'POPFAC_SAB' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "df= combiner(df1,df2,df3)\n",
    "df_popfac_sab = pd.concat([df,df_popfac_sab],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e36f57d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Etnik SARAWAK 13\n",
    "\n",
    "ng_val = 13\n",
    "df_num = 'df_13'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 7\n",
    "etnik_label = 'G5'\n",
    "popfac_name = 'POPFAC_SAR' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sar = pd.concat([df,df_popfac_sar],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "23d85cab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ng_val = 14\n",
    "df_num = 'df_14'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 5\n",
    "etnik_label = 'G3'\n",
    "popfac_name = 'POPFAC_SEM' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sem = pd.concat([df,df_popfac_sem],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "70d1587f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for x in range(1,12):\n",
    "    ng_val = x\n",
    "    df_num = f'df_{x}'\n",
    "    columnd = 'Female'\n",
    "    gender_num = 2\n",
    "    entik_number = 5\n",
    "    etnik_label = 'G3'\n",
    "    popfac_name = 'POPFAC_SEM' \n",
    "    df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "    columnd = 'Male'\n",
    "    gender_num = 1\n",
    "    df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "    columnd = 'Non Citizen'\n",
    "    df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "    df = combiner(df1,df2,df3)\n",
    "    df_popfac_sem = pd.concat([df,df_popfac_sem],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "615f2caf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1 = df_jr4_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bf80dc31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['G3','NG','CIT_NONCIT','KU_5','J']\n",
    "df_popfac_sem[x]= df_popfac_sem[x].astype(int)\n",
    "df_popfac_sem[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d9d54a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['G4','NG','CIT_NONCIT','KU_5','J']\n",
    "df_popfac_sab[x] = df_popfac_sab[x].astype(int)\n",
    "df_popfac_sab[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0554b5cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['G5','NG','CIT_NONCIT','KU_5','J']\n",
    "df_popfac_sar[x] = df_popfac_sar[x].astype(int)\n",
    "df_popfac_sar[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "79bc7cea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['NG','CIT_NONCIT','KU_5','J']\n",
    "for col in x:\n",
    "    df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "df1[x] = df1[x].astype(int)\n",
    "df1[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d21f5fd6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df1 = df_jr4_a\n",
    "df_final_1 = df1.merge(df_popfac_sem, how ='outer' , on=['G3','NG','CIT_NONCIT','KU_5','J'])\n",
    "df_final_2 = df_final_1.merge(df_popfac_sab, how ='outer', on=['G4','NG','CIT_NONCIT','KU_5','J'])\n",
    "df_final_3 = df_final_2.merge(df_popfac_sar, how='outer' ,on=['G5','NG','CIT_NONCIT','KU_5','J'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1a52b154",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_final_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4ac4db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#delete row yang NOID == nan \n",
    "noid = df_final_3.columns[0]\n",
    "df_final_3.dropna(subset=[noid], inplace=True)\n",
    "\n",
    "#merge popfac sem sab & sar if nan value \n",
    "df_final_3['POPFAC'] = df_final_3[['POPFAC_SEM', 'POPFAC_SAB', 'POPFAC_SAR']].apply(lambda x: '|'.join(x.dropna().astype(str)), axis=1)\n",
    "df_final_3.drop(['POPFAC_SEM', 'POPFAC_SAB', 'POPFAC_SAR'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aac2bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dede7d2a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read files \n",
    "df_adw = df_aw3_a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "51f9c18f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw = df_final_3.merge(df_adw, how='outer', on=['ST','NG','RIN_STRATA'])\n",
    "df_merged_popfac_adw.dropna(subset=[noid],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "037a902e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw['POPFAC'] = df_merged_popfac_adw['POPFAC'].replace('',np.nan).fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb50b919",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw['POPFAC']= df_merged_popfac_adw['POPFAC'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5295f9d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw['PEMBERAT_1'] = df_merged_popfac_adw['ADJUSTED_WEIGHT']*df_merged_popfac_adw['POPFAC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83276300",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_new = df_merged_popfac_adw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a5f6081",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_pivotted = df_jr4_new.pivot_table(index='NG',columns='CIT_NONCIT',values='PEMBERAT_1', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5105fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "af2c8b5d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_pivotted_temp = df_jr4_pivotted.reset_index(drop=False)\n",
    "df_jr4_pivotted_temp_2 = df_jr4_pivotted_temp.reset_index(drop=True)\n",
    "df1x = df_jr4_pivotted_temp_2.loc[:,('NG',1.0)]\n",
    "df1y = df_jr4_pivotted_temp_2.loc[:,('NG',2.0)]\n",
    "df1x['CIT_NONCIT'] = 1\n",
    "df1y['CIT_NONCIT'] = 2\n",
    "df1x = df1x.rename(columns={1.0:'ANGGARAN_SAMPEL_STB'})\n",
    "df1y = df1y.rename(columns={2.0:'ANGGARAN_SAMPEL_STB'})\n",
    "\n",
    "df1x= df1x.reset_index().rename_axis(None,axis=1)\n",
    "df1y = df1y.reset_index().rename_axis(None,axis=1)\n",
    "df11 = df1x.loc[:,('NG','ANGGARAN_SAMPEL_STB','CIT_NONCIT')]\n",
    "df22 = df1y.loc[:,('NG','ANGGARAN_SAMPEL_STB','CIT_NONCIT')]\n",
    "df_jr4_pivoted = pd.concat([df11,df22])\n",
    "# df1x = df1x.reset_index(drop=True).rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f4e63da0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read excel T03 (Bppd) \n",
    "latest_bppd_files = read_anything(bppd_storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "30af3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_bppd_files_2 = df_jr4_pivoted.merge(latest_bppd_files,how='outer',on=['NG','CIT_NONCIT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "af45d763",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merge into df_bppd_user filled \n",
    "\n",
    "\n",
    "final_bppd = latest_bppd_files_2.loc[:,('NG','CIT_NONCIT','ANGGARAN_SAMPEL_STB','ANGGARAN_PENDUDUK(BPPD)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "54544c12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merge with \n",
    "df_wpop = df_jr4_new.merge(final_bppd,how='outer',on=['NG','CIT_NONCIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "35765492",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['ID_POP'] = df_wpop['NG'].astype(str) + df_wpop['CIT_NONCIT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bf0eeda9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['ID_POP'] = df_wpop['ID_POP'].astype(str).str.replace(r'\\.0$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed946bc5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the columns to string type\n",
    "df_wpop['ANGGARAN_PENDUDUK(BPPD)'] = df_wpop['ANGGARAN_PENDUDUK(BPPD)'].astype(str)\n",
    "df_wpop['ANGGARAN_SAMPEL_STB'] = df_wpop['ANGGARAN_SAMPEL_STB'].astype(str)\n",
    "\n",
    "# Removing commas from 'ANGGARAN_PENDUDUK(BPPD)' column\n",
    "df_wpop['ANGGARAN_PENDUDUK(BPPD)'] = df_wpop['ANGGARAN_PENDUDUK(BPPD)'].str.replace(',', '')\n",
    "\n",
    "# Removing commas from 'ANGGARAN_SAMPEL_STB' column\n",
    "df_wpop['ANGGARAN_SAMPEL_STB'] = df_wpop['ANGGARAN_SAMPEL_STB'].str.replace(',', '')\n",
    "\n",
    "# Converting the columns to integers\n",
    "df_wpop['ANGGARAN_PENDUDUK(BPPD)'] = df_wpop['ANGGARAN_PENDUDUK(BPPD)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "64f09824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['ANGGARAN_SAMPEL_STB'] = df_wpop['ANGGARAN_SAMPEL_STB'].astype(float)\n",
    "\n",
    "\n",
    "# Jana Pemberat Final \n",
    "# \n",
    "# \n",
    "# 1. from pop_temp_files , merge with df_jr4_new  based on idpop , to bring weight pop inside \n",
    "# 2. generate column pemberat final (weight pop) x pemberat first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd441490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "43ab6a69",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['WEIGHT_POP'] = (df_wpop['ANGGARAN_PENDUDUK(BPPD)'])/(df_wpop['ANGGARAN_SAMPEL_STB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7f562675",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#weight pop = anggaran penduduk bppd / anggaransampel(STB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9a2f36f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['PEMBERAT_FINAL'] = (df_wpop['WEIGHT_POP'] * df_wpop['PEMBERAT_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "14624fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "years_t01 = years_t01.astype(str)\n",
    "quarter = quarter_t01.astype(str)\n",
    "halfyear = halfyear_t01.astype(str)\n",
    "\n",
    "# ### Ingest to DATABASE DIRECTLY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "60e52571",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "schema='production_micro_fc_stb_halfyear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fae46aba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql+psycopg2://admin:admin@10.251.49.51:5432/postgres')\n",
    "connection = engine.connect()\n",
    "print(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b98007fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "end_time = time.time() # get the end time \n",
    "time_running = end_time - start_time  # Calculate the time difference\n",
    "minutes = time_running / 60  # Convert time_running to minutes\n",
    "print(f'it took {minutes} minutes to run the whole process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a4f80ff3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #Handle duplicates row if run twice \n",
    "\n",
    "# WITH duplicates_cte AS (\n",
    "#     SELECT column1, column2, ..., \n",
    "#            ROW_NUMBER() OVER(PARTITION BY column1, column2, ... ORDER BY column1) AS row_num\n",
    "#     FROM your_table\n",
    "# )\n",
    "# DELETE FROM your_table\n",
    "# WHERE (column1, column2, ...) IN (\n",
    "#     SELECT column1, column2, ...\n",
    "#     FROM duplicates_cte\n",
    "#     WHERE row_num > 1\n",
    "# );\n",
    "\n",
    "#Select distinct \n",
    "#drop duplicate once query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1d84da49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sanitize_column_name(name):\n",
    "    # Remove all non-alphanumeric characters except underscores\n",
    "    name = re.sub(r'\\W+', '', name)\n",
    "    \n",
    "    # Remove leading digits if present\n",
    "    name = re.sub(r'^\\d+', '', name)\n",
    "    \n",
    "    # Ensure the name doesn't start with an underscore\n",
    "    if name.startswith('_'):\n",
    "        name = name[1:]\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "692d2ed3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop.columns = df_wpop.columns.map(sanitize_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0c7035c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop.to_sql('JR4HY'+str(halfyear_t01)+'Y'+years_t01+'_FC',con=engine,schema=schema,index=False,if_exists='replace')\n",
    "\n",
    "\n",
    "# ### Remove files in bppd_database path to avoid clash process in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "70d88798",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#to remove data directly\n",
    "def clear_garbage(path):\n",
    "    file_avaialble = [x for x in os.listdir(path)]\n",
    "\n",
    "    try:\n",
    "        for x in file_avaialble:\n",
    "            os.remove(path+'/'+x)\n",
    "            y = str(x).upper()\n",
    "            print(f'{y} excess files from processing has been relocated, contact vendor if you require the files for quality check ')\n",
    "    except Exception as e:\n",
    "        print(f'Error relocating the files: {path} - {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "032b8c26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#move data to clear \n",
    "def mover(path, destination_folder):\n",
    "    files_available = [x for x in os.listdir(path)]\n",
    "\n",
    "    try:\n",
    "        for file_name in files_available:\n",
    "            source_file = os.path.join(path, file_name)\n",
    "            destination_file = os.path.join(destination_folder, file_name)\n",
    "            shutil.move(source_file, destination_file)\n",
    "            y = str(file_name).upper()\n",
    "            print(f'{y} excess files from processing has been relocated to {destination_folder}. Contact the vendor if you require the files for quality check.')\n",
    "    except Exception as e:\n",
    "        print(f'Error relocating the files: {path} - {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e7968275",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = bppd_database\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = popfac_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = t01_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "\n",
    "\n",
    "path = bppd_storage_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = temp_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "\n",
    "\n",
    "path = path_t02\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
