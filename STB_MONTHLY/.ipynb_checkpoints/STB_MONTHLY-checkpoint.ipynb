{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e8809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## MODULE 1 : BASIC PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea337d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are cleaning STB_MONTHLY Survey Data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time \n",
    "import traceback \n",
    "import sys \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import shutil\n",
    "from sqlalchemy import create_engine, text\n",
    "print(\"We are cleaning STB_MONTHLY Survey Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2914ba2e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read whatever files types being ingested \n",
    "def read_anything(path):\n",
    "    #Get all files avaialble in the path (path shall be only 1 file at a time to manage this )\n",
    "    get_working_files = [x for x in os.listdir(path)]\n",
    "#     get_working_files = x\n",
    "    #make this as a function later \n",
    "    if len(get_working_files) == 0:\n",
    "        print('My goodman, no files either csv nor excel found, please recheck in path the existence')\n",
    "        return None\n",
    "    \n",
    "    #Excel found \n",
    "    time_start = time.time()\n",
    "    get_types_available = os.path.splitext(get_working_files[0])[1]\n",
    "    file_name = os.path.splitext(get_working_files[0])[0]\n",
    "    if get_types_available.endswith('.xlsx'):\n",
    "        \n",
    "        time_start = time.time()\n",
    "        print('We found excel files, hence we will read it and save to df_master, hold a moment ....')\n",
    "        df_master = pd.read_excel(path+'/'+get_working_files[0]).dropna(how='all')\n",
    "        time_end = time.time()\n",
    "        \n",
    "        diff_time = time_end - time_start\n",
    "        print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "        return df_master\n",
    "    \n",
    "    elif get_types_available.endswith('.csv'):\n",
    "        print('We found csv files, hence we will read it and save to df_master, hold a moment ....')\n",
    "        time_start = time.time()  \n",
    "        try:\n",
    "            df_master = pd.read_csv(os.path.join(path, get_working_files[0]))\n",
    "        except UnicodeDecodeError:\n",
    "            # If 'utf-8' fails, try 'ISO-8859-1' encoding\n",
    "            df_master = pd.read_csv(os.path.join(path, get_working_files[0]), encoding='ISO-8859-1').dropna(how='all')\n",
    "#         df_master = pd.read_csv(path+'/'+get_working_files[0],encoding='ISO-8859-1')\n",
    "        time_end = time.time()\n",
    "        diff_time = time_end - time_start\n",
    "        print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "        return df_master\n",
    "\n",
    "#     elif file_extension.lower() == '.csv':\n",
    "#         print('We found csv files, hence we will read it and save to df_master, hold a moment ....')\n",
    "#         time_start = time.time()\n",
    "#         try:\n",
    "#             df_master = pd.read_csv(os.path.join(path, get_working_files[0]))\n",
    "#         except UnicodeDecodeError:\n",
    "#             # If 'utf-8' fails, try 'ISO-8859-1' encoding\n",
    "#             df_master = pd.read_csv(os.path.join(path, get_working_files[0]), encoding='ISO-8859-1')\n",
    "#         time_end = time.time()\n",
    "#         diff_time = time_end - time_start\n",
    "#         print(f'My performance reading {file_name} file took: {diff_time} seconds')\n",
    "#         return df_master\n",
    "    \n",
    "#     diff_time = time_end - time_start\n",
    "#     print(f'my performance reading this file took : {diff_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ba7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "current_workingpath = os.getcwd()\n",
    "\n",
    "\n",
    "# #### PATH DECLARATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c90a01",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### INPUT FOR HDFS\n",
    "\n",
    "t01_path = os.path.join(current_workingpath,'INPUT_T01')\n",
    "path_t02 = os.path.join(current_workingpath,'INPUT_T02')\n",
    "temp_path = os.path.join(current_workingpath,'INPUT_MAP_RINSTRATA')\n",
    "jr4_raw_path = os.path.join(current_workingpath,'INPUT_RAWDATA_STB_JR4')\n",
    "jr2_raw_path = os.path.join(current_workingpath,'INPUT_RAWDATA_STB_JR2')\n",
    "bppd_storage_path = os.path.join(current_workingpath,'INPUT_T03')\n",
    "bppd_database = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_8')\n",
    "pop_fac_check = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_7')\n",
    "popfac_path = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_7')\n",
    "bin_path = os.path.join(current_workingpath,'BIN')\n",
    "\n",
    "#### INPUT FOR NIFI SERVER ONLY \n",
    "\n",
    "# t01_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_T01'\n",
    "# path_t02 = '/home/hadoop/codes/prod_stb_monthly/INPUT_T02'\n",
    "# temp_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_RAWDATA_STB_JR4'\n",
    "# jr4_raw_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_RAWDATA_STB_JR2'\n",
    "# jr2_raw_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_T02'\n",
    "# bppd_storage_path = '/home/hadoop/codes/prod_stb_monthly/INPUT_T03'\n",
    "# bppd_database = '/home/hadoop/codes/prod_stb_monthly/OUTPUT_QUALITYCHECK_8'\n",
    "# pop_fac_check = '/home/hadoop/codes/prod_stb_monthly/OUTPUT_QUALITYCHECK_7'\n",
    "# popfac_path ='/home/hadoop/codes/prod_stb_monthly/OUTPUT_QUALITYCHECK_7'\n",
    "# bin_path = '/home/hadoop/codes/prod_stb_monthly/BIN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22d6a21",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #### OUTPUT\n",
    "\n",
    "# new_jr4_path = os.path.join(current_workingpath,'OUTPUT_NEW_DFJR4')\n",
    "# path_popfac_new = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_2')\n",
    "# path_dfjr4_check = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_3') \n",
    "# jr4_final_output = os.path.join(current_workingpath,'OUTPUT_DFJR4_OUTPUT') \n",
    "# bmp_awdf_compare_check = os.path.join(current_workingpath,'OUTPUT_AW_BMP_COMPARE')\n",
    "# aw_df_check = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_5')\n",
    "# qualitycheck_merged_path = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_6')\n",
    "# path = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_9')\n",
    "# popfac_path = os.path.join(current_workingpath,'OUTPUT_QUALITYCHECK_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853b26ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ST_RINSTRATA_MAP'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "get_working_files = [x for x in os.listdir(temp_path)]\n",
    "a =os.path.splitext(str(get_working_files[0]))\n",
    "a[0]\n",
    "\n",
    "\n",
    "# #### FUNCTION TO AUTO DETECT FILES TYPES AND READ IT USING CORRECT LIBRARY\n",
    "\n",
    "# #### READING RAW JR4 & JR2 AND STORE IN DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e204985",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found csv files, hence we will read it and save to df_master, hold a moment ....\n",
      "My performance reading JR4M11Y2021_DATA RAW file took : 0.24610185623168945 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_jr4 = read_anything(jr4_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ee976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found csv files, hence we will read it and save to df_master, hold a moment ....\n",
      "My performance reading JR4M11Y2021_DATA RAW file took : 0.1928572654724121 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_jr2 = read_anything(jr2_raw_path)\n",
    "\n",
    "\n",
    "# #### STANDARDIZE COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a91c072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4.columns = df_jr4.columns.str.upper()\n",
    "df_jr2.columns = df_jr2.columns.str.upper()\n",
    "\n",
    "\n",
    "# #### CREATING 38 NOID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd914f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Check if digit required is not enough, to add leading 0 to ensure no id 38 is complete\n",
    "def check_column_lengths(df01, columns, required_lengths):\n",
    "    for col, req_len in zip(columns, required_lengths):\n",
    "        df01[col] = df01[col].astype(str).str.zfill(req_len)\n",
    "        df01[col] = df01[col].str[:req_len]\n",
    "    print('Value in columns specified has been added with leading 0 to ensure 38 digit')\n",
    "    return df01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5bc5d32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Concatting value from selected column to generate NOID 38 \n",
    "\n",
    "def generate_new_noid(df,columns,noid_col_name):\n",
    "    df[noid_col_name] = df.loc[:,columns].astype(str).apply(''.join, axis=1)\n",
    "    print(f'{noid_col_name} has been generated by merging values from specified columns')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#check all value in columns shall be 38 (or specified by user)\n",
    "\n",
    "def check_digit_match(df,noid_col_name):\n",
    "    digit_generated = df[noid_col_name].apply(lambda x: len(str(x)))\n",
    "    counts_digit_unique = digit_generated.value_counts()\n",
    "    if len(counts_digit_unique) > 1: \n",
    "        print(f\"Recheck due to inconsistent digit in NOID {counts_digit_unique}\")\n",
    "    else: \n",
    "        counts_digit = digit_generated.unique()[0]\n",
    "        print(f'{counts_digit} consistent digits has been generated , does this tally with client requirement? ')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b93abe1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4.columns = df_jr4.columns.str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "666800c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value in columns specified has been added with leading 0 to ensure 38 digit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Manipulate Here \n",
    "columns_sel = ['NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']\n",
    "required_lengths = [2, 2, 3, 3, 3, 3, 1, 4, 2, 1, 3, 2, 2, 1, 4, 2]\n",
    "df_jr4_new = check_column_lengths(df_jr4, columns_sel, required_lengths)\n",
    "noid_col_name = 'NOID_38'\n",
    "required_digits = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "031b96e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOID_38 has been generated by merging values from specified columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_jr4_new_2 = generate_new_noid(df_jr4_new,columns_sel,noid_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4731d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 consistent digits has been generated , does this tally with client requirement? \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "check_digit_match(df_jr4_new_2,noid_col_name)\n",
    "\n",
    "\n",
    "# ## MODULE 2 : GROUPING ACCORDING TO CLIENT REQUIREMENT\n",
    "\n",
    "# \t- GROUP BY DALAM UMUR 5 TAHUN \n",
    "# \t- GROUP BY DALAM UMUR 10 TAHUN \n",
    "# \t- GROUP BY DALAM ETNIK SEMENANJUNG \n",
    "# \t- GROUP BY ETNIK SABAH (BUMIPUTRA SABAH)\n",
    "# \t- GROUP BY ETNIK SARAWAK \n",
    "# \t- GROUP BY CIT_NONCIT \n",
    "# \t- GROUP BY RIN_STRATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e38ce439",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4 = df_jr4_new_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44859f6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nan_indices = df_jr4.index[df_jr4['U'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c91aeb38",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([6704, 13558, 20270, 22927], dtype='int64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "nan_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b38bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #make columns upfronts detect dataypes and fill first with nan to ensure complete fillup\n",
    "# df_jr4['G1'] = np.nan\n",
    "# df_jr4['KU_5'] = np.nan\n",
    "# df_jr4['G2'] = np.nan\n",
    "# df_jr4['G3'] = np.nan\n",
    "# df_jr4['G4'] = np.nan\n",
    "# df_jr4['G5'] = np.nan\n",
    "# df_jr4['CIT_NONCIT'] = np.nan\n",
    "# df_jr4['RIN_STRATA'] = np.nan\n",
    "\n",
    "# # df_jr4['U'] = pd.to_numeric(df_jr4['U'], errors='coerce')\n",
    "# # df_jr4['KET'] = pd.to_numeric(df_jr4['KET'], errors='coerce')\n",
    "# # df_jr4['KW'] = pd.to_numeric(df_jr4['KW'], errors='coerce')\n",
    "# # df_jr4['ST'] = pd.to_numeric(df_jr4['ST'], errors='coerce')\n",
    "\n",
    "\n",
    "# df_jr4['U'] =df_jr4['U'].astype(int)\n",
    "# df_jr4['KET'] =df_jr4['KET'].astype(int)\n",
    "# df_jr4['KW'] =df_jr4['KW'].astype(int)\n",
    "# df_jr4['ST'] = df_jr4['ST'].astype(int) \n",
    "\n",
    "#make columns upfronts detect dataypes and fill first with nan to ensure complete fillup\n",
    "df_jr4['G1'] = np.nan\n",
    "df_jr4['KU_5'] = np.nan\n",
    "df_jr4['G2'] = np.nan\n",
    "df_jr4['G3'] = np.nan\n",
    "df_jr4['G4'] = np.nan\n",
    "df_jr4['G5'] = np.nan\n",
    "df_jr4['CIT_NONCIT'] = np.nan\n",
    "df_jr4['RIN_STRATA'] = np.nan\n",
    "\n",
    "# df_jr4['U'] = pd.to_numeric(df_jr4['U'], errors='coerce')\n",
    "# df_jr4['KET'] = pd.to_numeric(df_jr4['KET'], errors='coerce')\n",
    "# df_jr4['KW'] = pd.to_numeric(df_jr4['KW'], errors='coerce')\n",
    "# df_jr4['ST'] = pd.to_numeric(df_jr4['ST'], errors='coerce')\n",
    "\n",
    "# Convert 'U' column to numeric, replacing non-finite values with NaN\n",
    "df_jr4['U'] = pd.to_numeric(df_jr4['U'], errors='coerce')\n",
    "\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['U'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_jr4['KET'] = pd.to_numeric(df_jr4['KET'], errors='coerce')\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['KET'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_jr4['KW'] = pd.to_numeric(df_jr4['KW'], errors='coerce')\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['KW'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_jr4['ST'] = pd.to_numeric(df_jr4['ST'], errors='coerce')\n",
    "# Replace NaN values with a default value (e.g., 0)\n",
    "df_jr4['ST'].fillna(0, inplace=True)\n",
    "\n",
    "# Now convert the 'U' column to integers\n",
    "\n",
    "df_jr4['U'] =df_jr4['U'].astype(int)\n",
    "\n",
    "df_jr4['KET'] =df_jr4['KET'].astype(int)\n",
    "df_jr4['KW'] =df_jr4['KW'].astype(int)\n",
    "df_jr4['ST'] = df_jr4['ST'].astype(int) \n",
    "\n",
    "# #### G1 Checks KU_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ce42a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column KU_5 does not contain null values, proceed with next group\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "conditions = [\n",
    "    df_jr4['U'].between(0, 4, inclusive='both'),\n",
    "    df_jr4['U'].between(5, 9, inclusive='both'),\n",
    "    df_jr4['U'].between(10, 14, inclusive='both'),\n",
    "    df_jr4['U'].between(15, 19, inclusive='both'),\n",
    "    df_jr4['U'].between(20, 24, inclusive='both'),\n",
    "    df_jr4['U'].between(25, 29, inclusive='both'),\n",
    "    df_jr4['U'].between(30, 34, inclusive='both'),\n",
    "    df_jr4['U'].between(35, 39, inclusive='both'),\n",
    "    df_jr4['U'].between(40, 44, inclusive='both'),\n",
    "    df_jr4['U'].between(45, 49, inclusive='both'),\n",
    "    df_jr4['U'].between(50, 54, inclusive='both'),\n",
    "    df_jr4['U'].between(55, 59, inclusive='both'),\n",
    "    df_jr4['U'].between(60, 64, inclusive='both'),\n",
    "    df_jr4['U'].between(65, 69, inclusive='both'),\n",
    "    df_jr4['U'].between(70, 74, inclusive='both'),\n",
    "    df_jr4['U'].between(75, 79, inclusive='both'),\n",
    "    df_jr4['U'].between(80, 84, inclusive='both'),\n",
    "    (df_jr4['U'] >= 85)\n",
    "]\n",
    "\n",
    "\n",
    "# Define values to fill in based on conditions\n",
    "values = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "\n",
    "# Use np.select to fill in values based on conditions\n",
    "df_jr4['KU_5'] = np.select(conditions, values, default=df_jr4['KU_5'])\n",
    "\n",
    "#Check for null values in new column created \n",
    "\n",
    "# df[['U','G1']]\n",
    "df_jr4[['KU_5']].notnull().value_counts()\n",
    "\n",
    "# add condition when theres null values, trigger alert \n",
    "if df_jr4['KU_5'].isnull().any().any():\n",
    "    print(\"Column KU_5 contains null values,please recheck data \")\n",
    "else:\n",
    "    print(\"Column KU_5 does not contain null values, proceed with next group\")\n",
    "##****** TBC \n",
    "\n",
    "\n",
    "# #### G2 Checks KU_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20ced6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column G2 does not contain null values, proceed with next group\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    \n",
    "    df_jr4['U'].between(0, 9,inclusive = 'both'),\n",
    "    df_jr4['U'].between(10, 19,inclusive = 'both'),\n",
    "    df_jr4['U'].between(20, 29,inclusive = 'both'),\n",
    "    df_jr4['U'].between(30, 39,inclusive = 'both'),\n",
    "    df_jr4['U'].between(40, 49,inclusive = 'both'),\n",
    "    df_jr4['U'].between(50, 59,inclusive = 'both'),\n",
    "    df_jr4['U'].between(60, 69,inclusive = 'both'),\n",
    "    df_jr4['U'].between(70, 79,inclusive = 'both'),\n",
    "    df_jr4['U'].between(80, 89,inclusive = 'both'),\n",
    "    df_jr4['U'].between(90, 99,inclusive = 'both'),\n",
    "    df_jr4['U'].between(100, 109,inclusive = 'both'),\n",
    "    df_jr4['U'].between(110, 119,inclusive = 'both'),\n",
    "    df_jr4['U'].between(120, 129,inclusive = 'both')\n",
    "]\n",
    "\n",
    "\n",
    "values = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "df_jr4['G2'] = np.select(condition,values, default=df_jr4['G2'])\n",
    "\n",
    "if df_jr4['G2'].isnull().any().any():\n",
    "    print(\"Column G2 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G2 does not contain null values, proceed with next group\")\n",
    "\n",
    "\n",
    "# #### G3 Checks KET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e04331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column G3 contains null values, recheck \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['KET'] == 1100) | (df_jr4['KET'] == 3210)),\n",
    "    df_jr4['KET'].isin([2111, 2112, 2113, 2114, 2115, 2116, 2121, 2122, 2123, 2124, 2125, 2126, 2131, 2132, 2133, 2134, 2135, 2136, 3110, 3120, 3130, 3140, 3150, 3160, 3170, 3180, 3190, 3200, 3220, 3230, 3240, 3250, 3260, 3998, 4110, 4120, 4130, 4140, 4150, 4160, 4170, 4180, 4190, 4200, 4210, 4220, 4230, 4240, 4250, 4260, 4270, 4280, 4290, 4300, 4310, 4320, 4330, 4340, 4350, 4360, 4998]),\n",
    "    df_jr4['KET'].isin([5110, 5120, 5130, 5140, 5150, 5160, 5170, 5180, 5190, 5200, 5998]),\n",
    "    df_jr4['KET'].isin([6110, 6120, 6130, 6140, 6150, 6160, 6170, 6180, 6998]),\n",
    "    df_jr4['KET'].isin([7110, 7120,7130, 7140, 7150, 7160, 7170, 7180, 7190, 7200, 7210, 7220, 7230, 7998, 8110, 8120, 8130, 8140, 8150, 8160, 8998, 9110, 9120, 9130, 9140, 9150, 9998])\n",
    "]\n",
    " \n",
    "values = [1,2,3,4,5]\n",
    "df_jr4['G3'] = np.select(condition, values, default=df_jr4['G3'])\n",
    "if df_jr4['G3'].isnull().any().any():\n",
    "    print(\"Column G3 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G3 does not contain null values, proceed with next group\")\n",
    "    \n",
    "\n",
    "\n",
    "# #### G4 Checks KET_SAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcc50aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column G4 contains null values, recheck \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['KET'] == 1100) | (df_jr4['KET'] == 3210)),\n",
    "    ((df_jr4['KET'] == 3150) | (df_jr4['KET'] == 3190)),\n",
    "    ((df_jr4['KET'] == 3110)),\n",
    "    ((df_jr4['KET'] == 3220)),\n",
    "    df_jr4['KET'].isin([2111,2112,2113,2114,2115,2116, 2121,2122,2123,2124,2125,2126, 2131,2132,2133,2134,2135,2136, 3120, 3130, 3140, 3160, 3170, 3180, 3200, 3230, 3240, 3250, 3260, 3998, 4110, 4120, 4130, 4140, 4150, 4160, 4170, 4180, 4190, 4200, 4210, 4220, 4230, 4240, 4250, 4260, 4270, 4280, 4290, 4300, 4310, 4320, 4330, 4340, 4350, 4360, 4998]),\n",
    "    df_jr4['KET'].isin([ 5110, 5120, 5130, 5140, 5150, 5160, 5170, 5180, 5190, 5200, 5998]),\n",
    "    df_jr4['KET'].isin([6110, 6120, 6130, 6140, 6150, 6160, 6170, 6180, 6998, 7110, 7120,7130, 7140, 7150, 7160, 7170, 7180, 7190, 7200, 7210, 7220, 7230, 7998, 8110, 8120, 8130, 8140, 8150, 8160, 8998, 9110, 9120, 9130, 9140, 9150, 9998]),\n",
    "]\n",
    " \n",
    "values = [1,2,3,4,5,6,7]\n",
    "df_jr4['G4'] = np.select(condition, values, default=df_jr4['G4'])\n",
    "if df_jr4['G4'].isnull().any().any():\n",
    "    print(\"Column G4 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G4 does not contain null values, proceed with next group\")\n",
    "    \n",
    "\n",
    "\n",
    "# #### G5 Checks : KUMPULAN ETNIK SARAWAK  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f1af3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column G5 contains null values, recheck \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['KET'] == 1100) | (df_jr4['KET'] == 3210)),\n",
    "    ((df_jr4['KET'] == 4140)),\n",
    "    ((df_jr4['KET'] == 4110)),\n",
    "    ((df_jr4['KET'] == 4260)),\n",
    "    df_jr4['KET'].isin([2111,2112,2113,2114,2115,2116, 2121,2122,2123,2124,2125,2126, 2131,2132,2133,2134,2135,2136, 3110, 3120, 3130, 3140, 3150, 3160, 3170, 3180, 3190, 3200, 3220, 3230, 3240, 3250, 3260, 3998, 4120, 4130, 4150, 4160, 4170, 4180, 4190, 4200, 4210, 4220, 4230, 4240, 4250, 4270, 4280, 4290, 4300, 4310, 4320, 4330, 4340, 4350, 4360, 4998]),\n",
    "    df_jr4['KET'].isin([5110, 5120, 5130, 5140, 5150, 5160, 5170, 5180, 5190, 5200, 5998]),\n",
    "    df_jr4['KET'].isin([6110, 6120, 6130, 6140, 6150, 6160, 6170, 6180, 6998, 7110, 7120,7130, 7140, 7150, 7160, 7170, 7180, 7190, 7200, 7210, 7220, 7230, 7998, 8110, 8120, 8130, 8140, 8150, 8160, 8998, 9110, 9120, 9130, 9140, 9150, 9998]),\n",
    "]\n",
    " \n",
    "values = [1,2,3,4,5,6,7]\n",
    "df_jr4['G5'] = np.select(condition, values, default=df_jr4['G5'])\n",
    "if df_jr4['G5'].isnull().any().any():\n",
    "    print(\"Column G5 contains null values, recheck \")\n",
    "else:\n",
    "    print(\"Column G5 does not contain null values, proceed with next group\")\n",
    "    \n",
    "\n",
    "\n",
    "# #### G6 Checks : 6 : KUMPULAN CIT_NONCIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a32fcdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column G6 does not contain null values, proceed with next group\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    df_jr4['KW'] == 458,\n",
    "    df_jr4['KW'] != 458\n",
    "\n",
    "]\n",
    "\n",
    "values = [1,2]\n",
    "\n",
    "df_jr4['CIT_NONCIT'] = np.select(condition,values, default=df_jr4['CIT_NONCIT'])\n",
    "mask = df_jr4[['KW','CIT_NONCIT']].isnull().any(axis=1)\n",
    "row_null = df_jr4.loc[mask]\n",
    "\n",
    "x=row_null[['KW','CIT_NONCIT']]\n",
    "\n",
    "if not x .empty:\n",
    "    print('Recheck this portion since it contain null values')\n",
    "else: \n",
    "    print(\"Column G6 does not contain null values, proceed with next group\")\n",
    "\n",
    "\n",
    "# #### G7 Checks :  KUMPULAN RIN_STRATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c283dc9f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column RIN_STRATA does not contain null values, proceed with next group\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "condition = [\n",
    "    ((df_jr4['ST'] ==2 ) | (df_jr4['ST'] ==1 )),\n",
    "    ((df_jr4['ST'].between(3,9,inclusive = 'both')) | (df_jr4['ST'] ==0 ))\n",
    "]\n",
    "values = [1,2]\n",
    "\n",
    "df_jr4['RIN_STRATA'] = np.select(condition,values, default=df_jr4['RIN_STRATA'])\n",
    "\n",
    "mask = df_jr4[['ST','RIN_STRATA']].isnull().any(axis=1)\n",
    "row_null = df_jr4.loc[mask]\n",
    "\n",
    "x=row_null[['ST','RIN_STRATA']]\n",
    "\n",
    "if not x .empty:\n",
    "    print('Recheck this portion since it contain null values')\n",
    "else: \n",
    "    print(\"Column RIN_STRATA does not contain null values, proceed with next group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86f22353",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code took -1.5439534187316895 seconds to finish phase 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "end_time = time.time()\n",
    "end_time_1 = start_time - end_time\n",
    "\n",
    "print(f'Code took {end_time_1} seconds to finish phase 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2983dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_a = df_jr4\n",
    "\n",
    "\n",
    "# ## MODULE 3 : MARKING PRIMARY_FIRST FOR VARIABLES XM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af473fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#remove column name with spaces to standardize \n",
    "# columns_with_spaces = [col for col in df.columns if ' ' in col]\n",
    "columns_with_spaces = [col for col in df_jr4.columns if ' ' in col]\n",
    "new_columns = [col.replace(' ', '') for col in df_jr4.columns]\n",
    "df_jr4.columns = new_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41d70a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "XM = ['B', 'NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP']\n",
    "\n",
    "for col in XM:\n",
    "    if not df_jr4[col].dtype == 'object':\n",
    "        df_jr4[col] = df_jr4[col].astype(str)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "807f0a4d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4['PF_INPUT'] = df_jr4.apply(lambda row: ''.join(row[XM]), axis=1)\n",
    "\n",
    "#Indentify first duplicate value in column PF_MAIN\n",
    "mask_duplicate = df_jr4.duplicated(subset=['PF_INPUT'], keep='first')\n",
    "\n",
    "#Mark all value in PF_MAIN with value 2 as a starting values \n",
    "df_jr4['PF_OUTPUT'] = 2\n",
    "\n",
    "# The line of code conditions = [~mask_duplicate, mask_duplicate] creates a list of two boolean arrays\n",
    "# : ~mask_duplicate and mask_duplicate. \n",
    "#     The ~ operator is the bitwise NOT operator, which in this context is used to invert \n",
    "#     the boolean values in the mask_duplicate array.\n",
    "\n",
    "# Set values based on duplicate mask\n",
    "values = [1, 2]\n",
    "conditions = [~mask_duplicate, mask_duplicate]\n",
    "df_jr4['PF_OUTPUT'] = np.select(conditions, values, default=df_jr4['PF_OUTPUT'])\n",
    "\n",
    "# df_jr4[['PF_INPUT','PF_OUTPUT']].value_counts().sort_values('PF_INPUT')\n",
    "# print(df_jr4.groupby(['PF_INPUT','PF_OUTPUT']).size().reset_index(name='count').sort_values('PF_OUTPUT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e4abcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code took -10.445857763290405 seconds to finish phase 2 generating primary first XM \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "end_time = time.time()\n",
    "end_time_2 = start_time - end_time\n",
    "\n",
    "print(f'Code took {end_time_2} seconds to finish phase 2 generating primary first XM ')\n",
    "\n",
    "\n",
    "# ##  MODULE 3 : ADJUSTED WEIGHT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78e616cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_bir = df_jr4[['NG','RIN_STRATA','PF_OUTPUT','PF_INPUT']]\n",
    "filtered_df = df_bir[df_bir['PF_OUTPUT'] == 1]\n",
    "df_bir1 = filtered_df.drop('PF_OUTPUT', axis=1)\n",
    "\n",
    "\n",
    "df_bir2 = df_bir[df_bir['PF_OUTPUT'] == 2]\n",
    "\n",
    "\n",
    "pivoted_pf = pd.pivot_table(df_bir1, values='PF_INPUT', index='NG', columns='RIN_STRATA', aggfunc='count')\n",
    "pivoted_dc = pd.pivot_table(df_bir2, values='PF_INPUT', index='NG', columns='RIN_STRATA', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fa650d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NG</th>\n",
       "      <th>RIN_STRATA</th>\n",
       "      <th>PF_OUTPUT</th>\n",
       "      <th>PF_INPUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.5.30.24.0000.0103.01.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.5.30.24.0000.0136.01.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.5.30.24.0000.01114.1.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.5.30.16.0000.0118.01.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.3.21.74.0000.0140.01.1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27169</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11124.7.095.00B95.145.01.1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27170</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11124.7.095.00B95.155.01.1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27171</th>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11126.55.64.00E64.7247.1.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27172</th>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11126.55.64.00E64.7292.1.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27173</th>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11126.55.64.00E64.7304.1.1107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27174 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NG  RIN_STRATA  PF_OUTPUT                       PF_INPUT\n",
       "0      5.         1.0          1  115.5.30.24.0000.0103.01.1107\n",
       "1      5.         1.0          1  115.5.30.24.0000.0136.01.1107\n",
       "2      5.         1.0          1  115.5.30.24.0000.01114.1.1107\n",
       "3      5.         1.0          1  115.5.30.16.0000.0118.01.1107\n",
       "4      5.         1.0          1  115.3.21.74.0000.0140.01.1106\n",
       "...    ..         ...        ...                            ...\n",
       "27169  12         1.0          2  11124.7.095.00B95.145.01.1105\n",
       "27170  12         1.0          2  11124.7.095.00B95.155.01.1105\n",
       "27171  12         2.0          2  11126.55.64.00E64.7247.1.1107\n",
       "27172  12         2.0          2  11126.55.64.00E64.7292.1.1107\n",
       "27173  12         2.0          2  11126.55.64.00E64.7304.1.1107\n",
       "\n",
       "[27174 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89ad8b68",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df2 = pivoted_df.reset_index()\n",
    "df_pf = pivoted_pf.reset_index()\n",
    "\n",
    "#Dalam Bandar \n",
    "df_pf_01 = df_pf[['NG',1.0]]\n",
    "#Luar Bandar \n",
    "df_pf_02 = df_pf[['NG',2.0]]\n",
    "\n",
    "df_pf_01 = df_pf_01.reset_index()\n",
    "df_pf_02 = df_pf_02.reset_index()\n",
    "\n",
    "df_pf_01.set_index('index', inplace=True)\n",
    "df_pf_02.set_index('index', inplace=True)\n",
    "\n",
    "df_pf_01.rename(columns={'NG':'KOD_NEGERI',1.0:'BIL_ISI_RUMAH_RESPON_SELESAI'}, inplace=True)\n",
    "df_pf_02.rename(columns={'NG':'KOD_NEGERI',2.0:'BIL_ISI_RUMAH_RESPON_SELESAI'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bf3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##  MODULE 4 : BMP ADJUSTED WEIGHT DATA VALIDATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c75a96d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found excel files, hence we will read it and save to df_master, hold a moment ....\n",
      "My performance reading 112021_T01_BMP_AW file took : 0.43895697593688965 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#read T01\n",
    "df_jr5 = df_jr4\n",
    "df_t01 = read_anything(t01_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73366cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>RIN_STRATA</th>\n",
       "      <th>KOD_NEGERI</th>\n",
       "      <th>BIL_ISI_RUMAH_RESPON_SELESAI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.</td>\n",
       "      <td>441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.</td>\n",
       "      <td>212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.</td>\n",
       "      <td>314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.</td>\n",
       "      <td>453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "RIN_STRATA KOD_NEGERI  BIL_ISI_RUMAH_RESPON_SELESAI\n",
       "index                                              \n",
       "0                  1.                         570.0\n",
       "1                  10                         608.0\n",
       "2                  11                         293.0\n",
       "3                  12                         475.0\n",
       "4                  13                         339.0\n",
       "5                  14                         276.0\n",
       "6                  15                          65.0\n",
       "7                  16                          88.0\n",
       "8                  2.                         441.0\n",
       "9                  3.                         212.0\n",
       "10                 4.                         259.0\n",
       "11                 5.                         314.0\n",
       "12                 6.                         340.0\n",
       "13                 7.                         380.0\n",
       "14                 8.                         453.0\n",
       "15                 9.                         119.0\n",
       "16                 na                           NaN"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pf_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe4def2c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'na'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m df_t01_01 \u001b[38;5;241m=\u001b[39m df_t01[df_t01[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIN_STRATA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m df_t01_02 \u001b[38;5;241m=\u001b[39m df_t01[df_t01[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIN_STRATA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m df_pf_01[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKOD_NEGERI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_pf_01\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKOD_NEGERI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df_pf_02[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKOD_NEGERI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_pf_02[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKOD_NEGERI\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     10\u001b[0m df_t01_01[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKOD_NEGERI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df_t01_01[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKOD_NEGERI\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6233\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m   6235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m   6236\u001b[0m     ]\n\u001b[0;32m   6238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6239\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6240\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6243\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:448\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, dtype, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    524\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 526\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    529\u001b[0m newb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(arr\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'na'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_t01.drop(columns=['BIL_ISI_RUMAH_RESPON_SELESAI','ADJUSTED_WEIGHT'], inplace=True)\n",
    "df_t01.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Splitting template table into 2 dataframe with 01 dalam bandar and luar bandar 02 \n",
    "df_t01_01 = df_t01[df_t01['RIN_STRATA']==1]\n",
    "df_t01_02 = df_t01[df_t01['RIN_STRATA']==2]\n",
    "\n",
    "df_pf_01['KOD_NEGERI'] = df_pf_01['KOD_NEGERI'].str.replace('.', '').astype(int)\n",
    "df_pf_02['KOD_NEGERI'] = df_pf_02['KOD_NEGERI'].str.replace('.', '').astype(int)\n",
    "df_t01_01['KOD_NEGERI'] = pd.to_numeric(df_t01_01['KOD_NEGERI'])\n",
    "df_t01_02['KOD_NEGERI'] = pd.to_numeric(df_t01_02['KOD_NEGERI'])\n",
    "\n",
    "t01_merged = pd.merge(df_t01_01,df_pf_01, on='KOD_NEGERI')\n",
    "t02_merged = pd.merge(df_t01_02,df_pf_02, on='KOD_NEGERI')\n",
    "aw_df = pd.concat([t01_merged,t02_merged], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98124006",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "months_t01 = df_t01['BULAN'].iloc[0]\n",
    "years_t01 = df_t01['TAHUN'].iloc[0]\n",
    "quarter_t01 = df_t01['QUARTER'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# loop through the 'BIL_ISI_RUMAH_RESPON_SELESAI' column and replace NaN values with 0\n",
    "for i in range(len(aw_df)):\n",
    "    if np.isnan(aw_df.loc[i, 'BIL_ISI_RUMAH_RESPON_SELESAI']):\n",
    "        aw_df.loc[i, 'BIL_ISI_RUMAH_RESPON_SELESAI'] = 0\n",
    "        \n",
    "        \n",
    "aw_df['BIL_ISI_RUMAH_RESPON_SELESAI'] = aw_df['BIL_ISI_RUMAH_RESPON_SELESAI'].astype(int)\n",
    "aw_df['ADJUSTED_WEIGHT'] =aw_df['BIL_ISI_RUMAH'] / aw_df['BIL_ISI_RUMAH_RESPON_SELESAI'] \n",
    "aw_df['ADJUSTED_WEIGHT'] = round(aw_df['ADJUSTED_WEIGHT'], 2)\n",
    "aw_df['SEMAKAN_AW_BMP'] = (aw_df['ADJUSTED_WEIGHT'] - aw_df['ADJUSTED_WEIGHT_BMP'])\n",
    "aw_df['SEMAKAN_AW_BMP'] = round(aw_df['SEMAKAN_AW_BMP'], 2)\n",
    "aw_df.rename(columns={'KOD_NEGERI':'NG'},inplace=True )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def nan_clear(col_name): \n",
    "    for i in range(len(aw_df)):\n",
    "        if np.isnan(aw_df.loc[i,col_name]):\n",
    "            aw_df.loc[i, 'ADJUSTED_WEIGHT_BMP'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19b486",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nan_clear('ADJUSTED_WEIGHT_BMP')\n",
    "nan_clear('ADJUSTED_WEIGHT')\n",
    "nan_clear('SEMAKAN_AW_BMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c93ebe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_temp = read_anything(temp_path)\n",
    "df_aw2 = aw_df.merge(df_temp, how='left', on=('NG','RIN_STRATA'))\n",
    "df_aw3 = df_aw2[['NG','RIN_STRATA','ST','ADJUSTED_WEIGHT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256c209",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# aw_df.to_csv(bmp_awdf_compare_check+'/'+'awdf_bmp_compare_check.csv')\n",
    "\n",
    "if any(aw_df['SEMAKAN_AW_BMP'] != 0):\n",
    "    print(f'Error: SEMAKAN_AW_BMP failed because has non-zero value(s), please recheck BPPD value and jr4 new adjusted weight')\n",
    "    \n",
    "\n",
    "else: \n",
    "    print('Success: SEMAKAN_AW_BMP success since there are no differences between BPPD and DOSM adjusted weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Remap aw_df RIN STRATA to ST too ADW VALUE \n",
    "\n",
    "# MERGE AW RIN_STRATA + KOD_NEGERI + ADJUSTED_WEIGHT into a new DF = df_jr4_aw \n",
    "\n",
    "#Change KOD_NEGERI to NG \n",
    "try: \n",
    "    aw_df['NG'] = aw_df['KOD_NEGERI']\n",
    "    aw_df.drop('KOD_NEGERI', axis=1,inplace=True)\n",
    "    #Change df_jr5(RIN_STRATA) to remove floats if any to int \n",
    "\n",
    "except: \n",
    "    print(f'Light Warning !: Column KOD_NEGERI has been removed, ignore renaming column from KOD_NEGERI to NG')\n",
    "\n",
    "df_jr5['RIN_STRATA'] = df_jr5['RIN_STRATA'].astype(int)\n",
    "df_jr5['NG'] = df_jr5['NG'].str.replace('.', '').astype(int)\n",
    "df_jr5['ST']= df_jr5['ST'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5faf95",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_aw = df_jr5.merge(df_aw3, how='left', on=('NG', 'RIN_STRATA','ST'))\n",
    "print(f'Success : JR4 has been merged with latest checked adjusted weight from BPPD & DOSM values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c15555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_aw3_a = df_aw3\n",
    "\n",
    "\n",
    "# ##  MODULE 5 : JADUAL A_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291f980",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad184d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pivoted_pf = pd.pivot_table(df_jr4_aw, values='ADJUSTED_WEIGHT',index=['CIT_NONCIT','NG','KU_5'], columns=['J','G3'], aggfunc='sum')\n",
    "# pivoted_pf.index.get_level_values(0).value_counts()\n",
    "# , removed due to unclear files provided "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function Library \n",
    "def pivot_table(group, ng_code):\n",
    "    pivoted_pf = pd.pivot_table(df_jr4_aw, values='ADJUSTED_WEIGHT', index=['CIT_NONCIT', 'NG', 'KU_5'], columns=['J',group], aggfunc='sum')\n",
    "    filtered_pivoted_pf_sem = pivoted_pf.loc[pivoted_pf.index.get_level_values('CIT_NONCIT')==1]\n",
    "    filtered_pivoted_pf = filtered_pivoted_pf_sem.loc[filtered_pivoted_pf_sem.index.get_level_values('NG').isin(ng_code)]\n",
    "\n",
    "    # Return the output dataframe\n",
    "    return filtered_pivoted_pf\n",
    "\n",
    "def reset_frame(df):\n",
    "    df = df.reset_index()\n",
    "    df.columns = df.columns.map(lambda x: '_'.join(map(str, x)))\n",
    "    df = df.rename(columns={'CIT_NONCIT_':'CIT_NONCIT','NG_':'NG','KU_5_':'KU_5'})\n",
    "    return df\n",
    "\n",
    "def pivot_table_bw():\n",
    "    pivoted_pf = pd.pivot_table(df_jr4_aw, values='ADJUSTED_WEIGHT', index=['CIT_NONCIT','KU_5','NG'], columns=['J'], aggfunc='sum')\n",
    "    filtered_pivoted_pf = pivoted_pf.loc[pivoted_pf.index.get_level_values('CIT_NONCIT')==2]\n",
    "    # Return the output dataframe\n",
    "    return filtered_pivoted_pf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95045d7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ng_code= [1,2,3,4,5,6,7,8,9,10,11,14,16]\n",
    "group='G3'\n",
    "df_sem = pivot_table(group,ng_code)\n",
    "\n",
    "ng_code= [12,15]\n",
    "group='G4'\n",
    "df_sab = pivot_table(group,ng_code)\n",
    "\n",
    "ng_code= [13]\n",
    "group='G5'\n",
    "df_sar = pivot_table(group,ng_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1aced7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#df_sem_1 ==  all column reset into 1 level and column name changed \n",
    "df_sem_1 = reset_frame(df_sem)\n",
    "df_sab_1 = reset_frame(df_sab)\n",
    "df_sar_1 = reset_frame(df_sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecedbac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sar_1.columns = df_sar_1.columns.str.replace('.0','')\n",
    "df_sab_1.columns = df_sab_1.columns.str.replace('.0','')\n",
    "df_sem_1.columns = df_sem_1.columns.str.replace('.0','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e137c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_bw = pivot_table_bw()\n",
    "df_bw = df_bw.reset_index()\n",
    "df_bw = df_bw.rename(columns={1:'1_BW',2:'2_BW'})\n",
    "df_bw = df_bw.rename(columns={'1': '1_BW', '2': '2_BW'})\n",
    "# add clause to check whether column successfully renamed from 1 to 1_BW or not\n",
    "df_bw = df_bw.drop('CIT_NONCIT',axis=1)\n",
    "if '1_BW' in df_bw.columns and '2_BW' in df_bw.columns:\n",
    "    print(\"Columns renamed successfully.\")\n",
    "else:\n",
    "    print(\"Columns renaming failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b53dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sem_bw_merged = df_sem_1.merge(df_bw,how='left',on=['NG','KU_5'])\n",
    "df_sab_bw_merged = df_sab_1.merge(df_bw,how='left',on=['NG','KU_5'])\n",
    "df_sar_bw_merged = df_sar_1.merge(df_bw,how='left',on=['NG','KU_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6761c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Success: df_semenanjung, df_sabah & df_sarawak have been generated to produce trend documents')\n",
    "\n",
    "\n",
    "# ### MODULE 6 : BPPD + JADUAL A1 PRODUCE TREND DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dd020",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read file t02 ()\n",
    "\n",
    "df_to2 = read_anything(path_t02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89c0bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#convert male to 1 & female to 2 \n",
    "df_to2['J'] = np.nan\n",
    "\n",
    "conditions = [\n",
    "    ((df_to2['GENDER'] == 'MALE')),\n",
    "    ((df_to2['GENDER'] == 'FEMALE')),\n",
    "]\n",
    "\n",
    "values = [1,2]\n",
    "\n",
    "df_to2['J'] = np.select(conditions,values,default=df_to2['J'])\n",
    "\n",
    "df_to2.reset_index\n",
    "df_to2['J'] = df_to2['J'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bd780",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Rename columns JADUAL A1\n",
    "df_to2 = df_to2.rename(columns={\n",
    "    'BPPD_MALAY': '1_SEM',\n",
    "    'BPPD_OTHER_BUMI': '2_SEM',\n",
    "    'BPPD_CHINESE': '3_SEM',\n",
    "    'BPPD_INDIAN': '4_SEM',\n",
    "    'BPPD_OTHERS': '5_SEM',\n",
    "    'BPPD_BUKAN_WARGA': 'BW_SEM',\n",
    "    \n",
    "    'BPPD_SABAH_MELAYU': '1_SAB',\n",
    "    'BPPD_SABAH_KADAZAN': '2_SAB',\n",
    "    'BPPD_SABAH_BAJAU': '3_SAB',\n",
    "    'BPPD_SABAH_MURUT': '4_SAB',\n",
    "    'BPPD_SABAH_BUMIPUTERA LAIN': '5_SAB',\n",
    "    'BPPD_SABAH_CINA': '6_SAB',\n",
    "    'BPPD_SABAH_LAIN LAIN': '7_SAB',\n",
    "    'BPPD_SABAH_BUKAN_WARGA': 'BW_SAB',\n",
    "    \n",
    "    'BPPD_SARAWAK_MELAYU': '1_SAR',\n",
    "    'BPPD_SARAWAK_IBAN': '2_SAR',\n",
    "    'BPPD_SARAWAK_BIDAYUH': '3_SAR',\n",
    "    'BPPD_SARAWAK_MELANAU': '4_SAR',\n",
    "    'BPPD_SARAWAK_BUMIPUTERA': '5_SAR',\n",
    "    'BPPD_SARAWAK_CINA': '6_SAR',\n",
    "    'BPPD_SARAWAK_LAIN_LAIN': '7_SAR',\n",
    "    'BPPD_SARAWAK_BUKAN_WARGA': 'BW_SAR',\n",
    "    'KOD_NEGERI': 'NG'\n",
    "})\n",
    "\n",
    "\n",
    "# #rename column in this format 1_X_B for Male & 2_X_B for Female \n",
    "# #make it as format of TO # GENDERNUMBER{1 OR 2} _#{NAMING FORMAT BPPD }}}\n",
    "# #AFTER COLUMN GENDER, START TO FILL IN 1_\"\"\"\"\"\" OR 2_'''''' FRONT OF EACH COLUMNS \n",
    "\n",
    "# #DROP COLUMNS \n",
    "df_to3 = df_to2.drop(columns={'NEGERI', 'AGE_GROUP', 'GENDER'})\n",
    "\n",
    "\n",
    "# # df_female = df.loc[df.index.get_level_values('GENDER') == 'MALE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e197f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Split male female then rename column \n",
    "def split_gender(q,df):\n",
    "    df = df_to3.loc[df_to2['J'].isin([q])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7541baf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#FUNCTION TO 1. RENAME COLUMN NAME & 2.  THEN SPLIT IT INTO 3 UNIQUE DATAFRAME SEM , SAB & SAR \n",
    "#get the column index of KU_5 to identify column to fill in \n",
    "def rename_col(df,index_to_fill_after,fillwith):\n",
    "    cols = list(df.columns)\n",
    "    ku5_index = cols.index(index_to_fill_after)+1\n",
    "\n",
    "    #insert new column after certain column name \n",
    "    index_to_edit = cols[ku5_index:]\n",
    "    new_cols = cols[:ku5_index] + [f'{fillwith}_' + x for x in index_to_edit]\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "    #Alternatives\n",
    "    # new_cols = cols[:ku5_index+1]\n",
    "    # for x in index_to_edit:\n",
    "    #     new_cols.append('1_'+x)\n",
    "\n",
    "    # new_cols = ['1_'+ col for col in base_index]\n",
    "    # new_cols = base_index + new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1607afb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "q = 1\n",
    "df_male = pd.DataFrame()\n",
    "df = df_male\n",
    "df_male = split_gender(q,df)\n",
    "\n",
    "q = 2\n",
    "df_female = pd.DataFrame()\n",
    "df = df_female\n",
    "df_female = split_gender(q,df)\n",
    "\n",
    "df = df_male\n",
    "index_to_fill_after = 'KU_5'\n",
    "fillwith = 1\n",
    "\n",
    "df_male = rename_col(df,index_to_fill_after,fillwith)\n",
    "\n",
    "df = df_female\n",
    "index_to_fill_after = 'KU_5'\n",
    "fillwith = 2\n",
    "\n",
    "df_female = rename_col(df,index_to_fill_after,fillwith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03c584",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#split female to 3 category sem, sab & sar \n",
    "#then convert to csv for calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda75ba9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_col_list(df,col_start_name,col_end_name):\n",
    "    #get all column name in df \n",
    "    cols = list(df.columns)\n",
    "    #get index of the desired column name start & end \n",
    "    starting_index = cols.index(col_start_name)\n",
    "    last_col = cols.index(col_end_name)+1\n",
    "    \n",
    "    #front 2 column must maintain in each filter \n",
    "    base_index = cols[:2]\n",
    "    \n",
    "    #get the range of desired columns\n",
    "    desired_column = cols[starting_index:last_col]\n",
    "    new_col = base_index + desired_column\n",
    "    return new_col\n",
    "\n",
    "def separator(df,ng_list, col_list):\n",
    "    df = df.loc[df['NG'].isin(ng_list), col_list]\n",
    "    print('Success: Trend separated')\n",
    "    return df\n",
    "\n",
    "def merge_m_f(df1,df2):\n",
    "    df = df1.merge(df2,how='left',on=('NG','KU_5'))\n",
    "    print('Success: Trend merged')\n",
    "    return df\n",
    "\n",
    "def save(df,path,name):\n",
    "    df.to_excel(path+'/'+name+'.xlsx',index=False)\n",
    "    \n",
    "    print('Success : Files were being saved in xlsx format for quality checking. It will be moved to other container once this process finished ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ea92a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sem master\n",
    "df = df_male\n",
    "col_start_name = '1_1_SEM'\n",
    "col_end_name = '1_BW_SEM'\n",
    "df_sem_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sem_col_list\n",
    "ng_list = 1,2,3,4,5,6,7,8,9,10,11,14,16\n",
    "df =df_male\n",
    "df_sem_bppd_male = separator(df,ng_list, col_list)\n",
    "df_sem_bppd_male\n",
    "\n",
    "#female & sem\n",
    "df = df_female\n",
    "col_start_name = '2_1_SEM'\n",
    "col_end_name = '2_BW_SEM'\n",
    "df_sem_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sem_col_list\n",
    "ng_list = 1,2,3,4,5,6,7,8,9,10,11,14,16\n",
    "df =df_female\n",
    "df_sem_bppd_female = separator(df,ng_list, col_list)\n",
    "df_sem_bppd_female\n",
    "\n",
    "df1= df_sem_bppd_male\n",
    "df2= df_sem_bppd_female\n",
    "df_sem_bppd_master = merge_m_f(df1,df2)\n",
    "\n",
    "df = df_sem_bppd_master\n",
    "path = bppd_database #temp_path\n",
    "name = 'df_sem_bppd_master'\n",
    "save(df,path,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64107f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sab master\n",
    "df = df_male\n",
    "col_start_name = '1_1_SAB'\n",
    "col_end_name = '1_BW_SAB'\n",
    "df_sab_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sab_col_list\n",
    "ng_list = 12,15\n",
    "df =df_male\n",
    "df_sab_bppd_male = separator(df,ng_list, col_list)\n",
    "df_sab_bppd_male\n",
    "\n",
    "#female & sem\n",
    "df = df_female\n",
    "col_start_name = '2_1_SAB'\n",
    "col_end_name = '2_BW_SAB'\n",
    "df_sab_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sab_col_list\n",
    "ng_list = 12,15\n",
    "df =df_female\n",
    "df_sab_bppd_female = separator(df,ng_list, col_list)\n",
    "df_sab_bppd_female\n",
    "\n",
    "df1= df_sab_bppd_male\n",
    "df2= df_sab_bppd_female\n",
    "df_sab_bppd_master = merge_m_f(df1,df2)\n",
    "\n",
    "df = df_sab_bppd_master\n",
    "path = bppd_database #temp_path\n",
    "name = 'df_sab_bppd_master'\n",
    "save(df,path,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998632f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sar master\n",
    "df = df_male\n",
    "col_start_name = '1_1_SAR'\n",
    "col_end_name = '1_BW_SAR'\n",
    "df_sar_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sar_col_list\n",
    "ng_list =13,13\n",
    "df =df_male\n",
    "df_sar_bppd_male = separator(df,ng_list, col_list)\n",
    "df_sar_bppd_male\n",
    "\n",
    "#female & sar\n",
    "df = df_female\n",
    "col_start_name = '2_1_SAR'\n",
    "col_end_name = '2_BW_SAR'\n",
    "df_sar_col_list = get_col_list(df,col_start_name,col_end_name)\n",
    "\n",
    "col_list = df_sar_col_list\n",
    "ng_list = 13,13\n",
    "df =df_female\n",
    "df_sar_bppd_female = separator(df,ng_list, col_list)\n",
    "df_sar_bppd_female\n",
    "\n",
    "df1= df_sar_bppd_male\n",
    "df2= df_sar_bppd_female\n",
    "df_sar_bppd_master = merge_m_f(df1,df2)\n",
    "\n",
    "df = df_sar_bppd_master\n",
    "path = bppd_database #temp_path\n",
    "name = 'df_sar_bppd_master'\n",
    "save(df,path,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecf655",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#jaduala1 dataframe \n",
    "df_sem_a1_master = df_sem_bw_merged\n",
    "df_sab_a1_master = df_sab_bw_merged\n",
    "df_sar_a1_master = df_sar_bw_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bce7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merging dataframe \n",
    "df1 = df_sem_bppd_master\n",
    "df2 = df_sem_a1_master\n",
    "\n",
    "df_sem_master = merge_m_f(df1,df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c557bad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merging dataframe \n",
    "df1 = df_sab_bppd_master\n",
    "df2 = df_sab_a1_master\n",
    "\n",
    "df_sab_master = merge_m_f(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db02bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merging dataframe \n",
    "df1 = df_sar_bppd_master\n",
    "df2 = df_sar_a1_master\n",
    "\n",
    "df_sar_master = merge_m_f(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3aeb2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rearrange_columns(df,structure):\n",
    "    # Define a regular expression pattern to match the integer in the column names\n",
    "    pattern = r'(\\d+)'\n",
    "\n",
    "    # Extract the integer from each column name using the regular expression\n",
    "    int_cols = [(int(re.findall(pattern, col)[0]), col) for col in df.columns if re.findall(pattern, col)]\n",
    "\n",
    "    # Sort the column names based on the extracted integer\n",
    "    int_cols.sort()\n",
    "\n",
    "    # Rearrange the column names by pairing the '_SEM' columns with their corresponding non-SEM columns\n",
    "    new_cols = []\n",
    "    for i in range(0, len(int_cols), 2):\n",
    "        sem_col = f'{int_cols[i][1]}_{structure}'\n",
    "        new_cols.append(int_cols[i][1])\n",
    "        new_cols.append(sem_col)\n",
    "    new_cols.remove('KU_5_'+structure)\n",
    "\n",
    "    # Add any remaining columns that were not paired to the end of the list\n",
    "    if len(new_cols) < len(df.columns):\n",
    "        for col in df.columns:\n",
    "            if col not in new_cols:\n",
    "                new_cols.append(col)\n",
    "    new_cols.remove('CIT_NONCIT')\n",
    "\n",
    "    # Reorder the columns in the dataframe\n",
    "#     df = df[new_cols]\n",
    "# ** changed due to structure non sense\n",
    "    filter_nonsense = [x for x in new_cols if x in df.columns]\n",
    "    df = df[filter_nonsense]\n",
    "    print(f'{structure} re-arranged as per client requirements')\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243d236",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df_sem_master\n",
    "structure = 'SEM'\n",
    "df_sem_master_trend = rearrange_columns(df,structure)\n",
    "\n",
    "df = df_sab_master\n",
    "structure = 'SAB'\n",
    "df_sab_master_trend = rearrange_columns(df,structure)\n",
    "\n",
    "df = df_sar_master\n",
    "structure = 'SAR'\n",
    "df_sar_master_trend = rearrange_columns(df,structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b0415",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def separator2(df,ng_list):\n",
    "    df = df.loc[df['NG'].isin(ng_list)]\n",
    "    print(f'NG:{ng_list} separated into other dataframe')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7615c99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sem ng==1\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 1,1\n",
    "df_sem_master_trend_1 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==2\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 2,2\n",
    "df_sem_master_trend_2 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==3\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 3,3\n",
    "df_sem_master_trend_3 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==4\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 4,4\n",
    "df_sem_master_trend_4 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==5\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 5,5\n",
    "df_sem_master_trend_5 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==6\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 6,6\n",
    "df_sem_master_trend_6 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==7\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 7,7\n",
    "df_sem_master_trend_7 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==8\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 8,8\n",
    "df_sem_master_trend_8 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==9\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 9,9\n",
    "df_sem_master_trend_9 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==10\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 10,10\n",
    "df_sem_master_trend_10 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==11\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 11,11\n",
    "df_sem_master_trend_11 = separator2(df,ng_list)\n",
    "\n",
    "#sabah ng==12\n",
    "df = df_sab_master_trend\n",
    "ng_list = 12,12\n",
    "df_sab_master_trend_12 = separator2(df,ng_list)\n",
    "\n",
    "#sarawak ng==13\n",
    "df = df_sar_master_trend\n",
    "ng_list = 13,13\n",
    "df_sar_master_trend_13 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==14\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 14,14\n",
    "df_sem_master_trend_14 = separator2(df,ng_list)\n",
    "\n",
    "#sabah ng==15\n",
    "\n",
    "df = df_sab_master_trend\n",
    "ng_list = 15,15\n",
    "df_sab_master_trend_15 = separator2(df,ng_list)\n",
    "\n",
    "#sem ng==16\n",
    "\n",
    "df = df_sem_master_trend\n",
    "ng_list = 16,16\n",
    "df_sem_master_trend_16 = separator2(df,ng_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca0284",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#function to generate DIV column for all columns \n",
    "def generate_div(cols,region):\n",
    "    a = [col for col in cols if col.endswith(region)]\n",
    "    b = [re.findall(r'\\d{1,3}', col)[0] for col in a]\n",
    "    c = cols.filter(regex='^(' + '|'.join(b) + ')')\n",
    "    for i, row in c.iterrows():\n",
    "        for col1, value1 in row.items():\n",
    "            if col1.endswith(region):\n",
    "                col2 = col1[:-4]\n",
    "                value2 = row[col2]\n",
    "                new_col_name = col2 + '_DIV'\n",
    "                if value2 != 0:\n",
    "                    c.loc[i, new_col_name] = value1 / value2\n",
    "        \n",
    "    \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30967368",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cols = df_sem_master_trend_1\n",
    "region = 'SEM'\n",
    "df_sem_div_1 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_2\n",
    "region = 'SEM'\n",
    "df_sem_div_2 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_3\n",
    "region = 'SEM'\n",
    "df_sem_div_3 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_4\n",
    "region = 'SEM'\n",
    "df_sem_div_4 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_5\n",
    "region = 'SEM'\n",
    "df_sem_div_5 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_6\n",
    "region = 'SEM'\n",
    "df_sem_div_6 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_7\n",
    "region = 'SEM'\n",
    "df_sem_div_7 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_8\n",
    "region = 'SEM'\n",
    "df_sem_div_8 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_9\n",
    "region = 'SEM'\n",
    "df_sem_div_9 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_10\n",
    "region = 'SEM'\n",
    "df_sem_div_10 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_11\n",
    "region = 'SEM'\n",
    "df_sem_div_11 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sab_master_trend_12\n",
    "region = 'SAB'\n",
    "df_sab_div_12 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sar_master_trend_13\n",
    "region = 'SAR'\n",
    "df_sar_div_13 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_14\n",
    "region = 'SEM'\n",
    "df_sem_div_14 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sab_master_trend_15\n",
    "region = 'SAB'\n",
    "df_sab_div_15 = generate_div(cols, region)\n",
    "\n",
    "cols = df_sem_master_trend_16\n",
    "region = 'SEM'\n",
    "df_sem_div_16 = generate_div(cols, region)\n",
    "\n",
    "print(f'Phase Division Completed Successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357cf26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def concatenate_div_columns(df):\n",
    "    d = [col for col in df if col.endswith('DIV')]\n",
    "    f = df.loc[:,d]\n",
    "    g = f.round(2)\n",
    "\n",
    "    col1 = pd.DataFrame([])\n",
    "    col2 = pd.DataFrame([])\n",
    "    col3 = pd.DataFrame([])\n",
    "    for column in g.columns:\n",
    "        if column.startswith('1') and not re.match(r'.*BW_DIV.*', column):\n",
    "            col1 = pd.concat([col1, g[column].reset_index(drop=True)], axis=0)\n",
    "            col1 = col1.fillna(0)\n",
    "        elif column.startswith('2') and not re.match(r'.*BW_DIV.*', column):\n",
    "            col2 = pd.concat([col2, g[column].reset_index(drop=True)], axis=0)\n",
    "            col2 = col2.fillna(0)\n",
    "        elif re.match(r'.*BW_DIV.*', column):\n",
    "            col3 = pd.concat([col3, g[column].reset_index(drop=True)], axis=0)\n",
    "            col3 = col3.fillna(0)\n",
    "\n",
    "    concatenated_df = pd.concat([col1.reset_index(drop=True), col2.reset_index(drop=True), col3.reset_index(drop=True)], axis=1)\n",
    "    concatenated_df.columns = ['Male', 'Female', 'Non Citizen']\n",
    "\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0201c65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df_sem_div_1\n",
    "df_popfac_1 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_2\n",
    "df_popfac_2 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_3\n",
    "df_popfac_3 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_4\n",
    "df_popfac_4 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_5\n",
    "df_popfac_5 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_6\n",
    "df_popfac_6 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_7\n",
    "df_popfac_7 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_8\n",
    "df_popfac_8 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_9\n",
    "df_popfac_9 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_10\n",
    "df_popfac_10 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_11\n",
    "df_popfac_11 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sab_div_12\n",
    "df_popfac_12 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sar_div_13\n",
    "df_popfac_13 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_14\n",
    "df_popfac_14 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sab_div_15\n",
    "df_popfac_15 = concatenate_div_columns(df)\n",
    "\n",
    "df = df_sem_div_16\n",
    "df_popfac_16 = concatenate_div_columns(df)\n",
    "\n",
    "print(f'Popfac calculated successfully for all negeri ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cdc859",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(1,17):\n",
    "    df_name = f'df_popfac_{i}'\n",
    "    df = globals()[df_name]\n",
    "    df.to_excel(pop_fac_check+f'/{df_name}.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01024654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # aw_df.to_csv(aw_df_check+'/'+'ADW_CHECK.csv')\n",
    "# aw_df \n",
    "# for i in range(1,17):\n",
    "#     df_name = f'df_popfac_{i}'\n",
    "#     df = globals()[df_name]\n",
    "#     df.to_csv(qualitycheck_merged_path+'/'+f'{df_name}.csv',index=False)\n",
    "\n",
    "\n",
    "# ### MODULE POPFAC VALUE CONVERTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Store temporary popfac value in dictionary \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for x in range (1,17):\n",
    "    file_name = os.path.join(popfac_path,f'df_popfac_{x}*')\n",
    "    #make this as an array of path , to make it loopable \n",
    "    files = glob.glob(file_name)\n",
    "    if files:\n",
    "        latest_files = max(files, key=os.path.getctime)\n",
    "# os.path.getctime returns the time of the last metadata change to a file, which includes changes to the file's permissions, ownership, or timestamps.\n",
    "# os.path.getmtime returns the time of the last modification to the file's content, which includes any changes to the actual data in the file.\n",
    "# So which one you should use depends on what you consider to be the relevant change to the file.\n",
    "# In most cases, os.path.getmtime is the more appropriate choice, as it reflects changes to the actual data in the file. However, if you are interested in changes to the file's metadata, such as changes to its permissions or ownership, then os.path.getctime would be more appropriate.\n",
    "# In the context of reading the latest file in a directory, you would generally want to use os.path.getmtime, as you are likely interested in the latest version of the file's content.\n",
    "        file_list.append(latest_files)\n",
    "    \n",
    "file_dict = {}\n",
    "\n",
    "for i, file_path in enumerate(file_list):\n",
    "    df_name = f'df_{i+1}'\n",
    "    file_dict[df_name] = pd.read_excel(file_path) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#12 ,13 & 15 have 7 etnik number \n",
    "#the rest have 5 etnik numbers \n",
    "#ETNIK SABAH 12 & 15 \n",
    "#ETNIK SARAWAK 13 \n",
    "#ETINIK SEMENANJUNG 1 TO 11\n",
    "\n",
    "\n",
    "def popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name): \n",
    "    df = file_dict[df_num]\n",
    "    df['NG'] = ng_val\n",
    "    df = df[[columnd,'NG']]\n",
    "    df['J'] = gender_num\n",
    "    df['KU_5'] = 0\n",
    "    df['CIT_NONCIT'] = 1\n",
    "    df[etnik_label] = 0\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(entik_number):\n",
    "        start_idx = i * 18\n",
    "        end_idx = start_idx + 18\n",
    "        df.loc[start_idx:end_idx-1, etnik_label] = i+1\n",
    "\n",
    "    num_rows = df.shape[0]\n",
    "\n",
    "    seq = np.tile(np.arange(1, 19), (num_rows//18 + 1))[:num_rows]\n",
    "    df['KU_5'] = seq\n",
    "\n",
    "    df[popfac_name] = df[columnd]\n",
    "    df = df.drop(columnd,axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name): \n",
    "    df = file_dict[df_num]\n",
    "    df['NG'] = ng_val\n",
    "\n",
    "    df = df[[columnd,'NG']]\n",
    "    df['J'] = 0\n",
    "    df['KU_5'] = 0\n",
    "    df['CIT_NONCIT'] = 2\n",
    "    df[etnik_label] = 0\n",
    "    \n",
    "    for i in range(3):\n",
    "        start_idx = i*18\n",
    "        end_idx =start_idx + 18\n",
    "        df.loc[start_idx:end_idx-1, 'J'] = i+1\n",
    "\n",
    "    for i in range(5):\n",
    "        start_idx = i * 18\n",
    "        end_idx = start_idx + 18\n",
    "        df.loc[start_idx:end_idx, etnik_label] = i+1\n",
    "\n",
    "    num_rows = df.shape[0]\n",
    "\n",
    "    seq = np.tile(np.arange(1, 19), (num_rows//18 + 1))[:num_rows]\n",
    "    df['KU_5'] = seq\n",
    "\n",
    "    df[popfac_name] = df[columnd]\n",
    "    df = df.drop(columnd,axis=1)\n",
    "    df = df.dropna(subset=[popfac_name])\n",
    "    return df\n",
    "\n",
    "def combiner(df1,df2,df3):\n",
    "    df = pd.concat([df1,df2,df3], axis=0, ignore_index=True)\n",
    "    df\n",
    "    print(f'All dataframe combined successfully')\n",
    "    return df     \n",
    "#     duplicates = df_concat.columns[df_concat.columns.duplicated()]\n",
    "# The axis parameter in pd.concat() specifies the axis along which the data frames will be concatenated.\n",
    "# When axis=0, the data frames will be concatenated vertically, i.e., rows will be appended one after another, which means that the resulting data frame will have more rows.\n",
    "# When axis=1, the data frames will be concatenated horizontally, i.e., columns will be appended side by side, which means that the resulting data frame will have more columns.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af666cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_popfac_sem = pd.DataFrame()\n",
    "df_popfac_sar = pd.DataFrame()\n",
    "df_popfac_sab = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c15a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ng_val = 16\n",
    "df_num = 'df_16'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 5\n",
    "etnik_label = 'G3'\n",
    "popfac_name = 'POPFAC_SEM' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sem = pd.concat([df,df_popfac_sem],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd14df5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Etnik SABAH 15 \n",
    "\n",
    "ng_val = 15\n",
    "df_num = 'df_15'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 7\n",
    "etnik_label = 'G4'\n",
    "popfac_name = 'POPFAC_SAB' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sab = pd.concat([df,df_popfac_sab],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2161764",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Etnik SABAH  12 \n",
    "\n",
    "ng_val = 12\n",
    "df_num = 'df_12'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 7\n",
    "etnik_label = 'G4'\n",
    "popfac_name = 'POPFAC_SAB' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "df= combiner(df1,df2,df3)\n",
    "df_popfac_sab = pd.concat([df,df_popfac_sab],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f57d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Etnik SARAWAK 13\n",
    "\n",
    "ng_val = 13\n",
    "df_num = 'df_13'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 7\n",
    "etnik_label = 'G5'\n",
    "popfac_name = 'POPFAC_SAR' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sar = pd.concat([df,df_popfac_sar],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d85cab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ng_val = 14\n",
    "df_num = 'df_14'\n",
    "columnd = 'Female'\n",
    "gender_num = 2\n",
    "entik_number = 5\n",
    "etnik_label = 'G3'\n",
    "popfac_name = 'POPFAC_SEM' \n",
    "df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Male'\n",
    "gender_num = 1\n",
    "df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "columnd = 'Non Citizen'\n",
    "df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "df = combiner(df1,df2,df3)\n",
    "df_popfac_sem = pd.concat([df,df_popfac_sem],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1587f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for x in range(1,12):\n",
    "    ng_val = x\n",
    "    df_num = f'df_{x}'\n",
    "    columnd = 'Female'\n",
    "    gender_num = 2\n",
    "    entik_number = 5\n",
    "    etnik_label = 'G3'\n",
    "    popfac_name = 'POPFAC_SEM' \n",
    "    df2 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "\n",
    "    columnd = 'Male'\n",
    "    gender_num = 1\n",
    "    df1 = popfac_converter(ng_val,df_num,gender_num,entik_number,etnik_label,popfac_name)\n",
    "\n",
    "    columnd = 'Non Citizen'\n",
    "    df3 = popfac_converter_noncit(ng_val,df_num,gender_num,etnik_label,popfac_name)\n",
    "\n",
    "    df = combiner(df1,df2,df3)\n",
    "    df_popfac_sem = pd.concat([df,df_popfac_sem],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f2caf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1 = df_jr4_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80dc31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['G3','NG','CIT_NONCIT','KU_5','J']\n",
    "df_popfac_sem[x]= df_popfac_sem[x].astype(int)\n",
    "df_popfac_sem[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d54a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['G4','NG','CIT_NONCIT','KU_5','J']\n",
    "df_popfac_sab[x] = df_popfac_sab[x].astype(int)\n",
    "df_popfac_sab[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554b5cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['G5','NG','CIT_NONCIT','KU_5','J']\n",
    "df_popfac_sar[x] = df_popfac_sar[x].astype(int)\n",
    "df_popfac_sar[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc7cea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = ['NG','CIT_NONCIT','KU_5','J']\n",
    "for col in x:\n",
    "    df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "df1[x] = df1[x].astype(int)\n",
    "df1[x].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f5fd6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df1 = df_jr4_a\n",
    "df_final_1 = df1.merge(df_popfac_sem, how ='outer' , on=['G3','NG','CIT_NONCIT','KU_5','J'])\n",
    "df_final_2 = df_final_1.merge(df_popfac_sab, how ='outer', on=['G4','NG','CIT_NONCIT','KU_5','J'])\n",
    "df_final_3 = df_final_2.merge(df_popfac_sar, how='outer' ,on=['G5','NG','CIT_NONCIT','KU_5','J'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52b154",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_final_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#delete row yang NOID == nan \n",
    "noid = df_final_3.columns[0]\n",
    "df_final_3.dropna(subset=[noid], inplace=True)\n",
    "\n",
    "#merge popfac sem sab & sar if nan value \n",
    "df_final_3['POPFAC'] = df_final_3[['POPFAC_SEM', 'POPFAC_SAB', 'POPFAC_SAR']].apply(lambda x: '|'.join(x.dropna().astype(str)), axis=1)\n",
    "df_final_3.drop(['POPFAC_SEM', 'POPFAC_SAB', 'POPFAC_SAR'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede7d2a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read files \n",
    "df_adw = df_aw3_a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9c18f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw = df_final_3.merge(df_adw, how='outer', on=['ST','NG','RIN_STRATA'])\n",
    "df_merged_popfac_adw.dropna(subset=[noid],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a902e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw['POPFAC'] = df_merged_popfac_adw['POPFAC'].replace('',np.nan).fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50b919",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw['POPFAC']= df_merged_popfac_adw['POPFAC'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295f9d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged_popfac_adw['PEMBERAT_1'] = df_merged_popfac_adw['ADJUSTED_WEIGHT']*df_merged_popfac_adw['POPFAC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83276300",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_new = df_merged_popfac_adw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f6081",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_pivotted = df_jr4_new.pivot_table(index='NG',columns='CIT_NONCIT',values='PEMBERAT_1', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5105fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c8b5d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_jr4_pivotted_temp = df_jr4_pivotted.reset_index(drop=False)\n",
    "df_jr4_pivotted_temp_2 = df_jr4_pivotted_temp.reset_index(drop=True)\n",
    "df1x = df_jr4_pivotted_temp_2.loc[:,('NG',1.0)]\n",
    "df1y = df_jr4_pivotted_temp_2.loc[:,('NG',2.0)]\n",
    "df1x['CIT_NONCIT'] = 1\n",
    "df1y['CIT_NONCIT'] = 2\n",
    "df1x = df1x.rename(columns={1.0:'ANGGARAN_SAMPEL_STB'})\n",
    "df1y = df1y.rename(columns={2.0:'ANGGARAN_SAMPEL_STB'})\n",
    "\n",
    "df1x= df1x.reset_index().rename_axis(None,axis=1)\n",
    "df1y = df1y.reset_index().rename_axis(None,axis=1)\n",
    "df11 = df1x.loc[:,('NG','ANGGARAN_SAMPEL_STB','CIT_NONCIT')]\n",
    "df22 = df1y.loc[:,('NG','ANGGARAN_SAMPEL_STB','CIT_NONCIT')]\n",
    "df_jr4_pivoted = pd.concat([df11,df22])\n",
    "# df1x = df1x.reset_index(drop=True).rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e63da0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read excel T03 (Bppd) \n",
    "latest_bppd_files = read_anything(bppd_storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45d763",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merge into df_bppd_user filled \n",
    "\n",
    "latest_bppd_files_2 = df_jr4_pivoted.merge(latest_bppd_files,how='outer',on=['NG','CIT_NONCIT'])\n",
    "\n",
    "final_bppd = latest_bppd_files_2.loc[:,('NG','CIT_NONCIT','ANGGARAN_SAMPEL_STB','ANGGARAN_PENDUDUK(BPPD)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54544c12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#merge with \n",
    "df_wpop = df_jr4_new.merge(final_bppd,how='outer',on=['NG','CIT_NONCIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35765492",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['ID_POP'] = df_wpop['NG'].astype(str) + df_wpop['CIT_NONCIT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0eeda9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['ID_POP'] = df_wpop['ID_POP'].astype(str).str.replace(r'\\.0$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed946bc5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the columns to string type\n",
    "df_wpop['ANGGARAN_PENDUDUK(BPPD)'] = df_wpop['ANGGARAN_PENDUDUK(BPPD)'].astype(str)\n",
    "df_wpop['ANGGARAN_SAMPEL_STB'] = df_wpop['ANGGARAN_SAMPEL_STB'].astype(str)\n",
    "\n",
    "# Removing commas from 'ANGGARAN_PENDUDUK(BPPD)' column\n",
    "df_wpop['ANGGARAN_PENDUDUK(BPPD)'] = df_wpop['ANGGARAN_PENDUDUK(BPPD)'].str.replace(',', '')\n",
    "\n",
    "# Removing commas from 'ANGGARAN_SAMPEL_STB' column\n",
    "df_wpop['ANGGARAN_SAMPEL_STB'] = df_wpop['ANGGARAN_SAMPEL_STB'].str.replace(',', '')\n",
    "\n",
    "# Converting the columns to integers\n",
    "df_wpop['ANGGARAN_PENDUDUK(BPPD)'] = df_wpop['ANGGARAN_PENDUDUK(BPPD)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f09824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['ANGGARAN_SAMPEL_STB'] = df_wpop['ANGGARAN_SAMPEL_STB'].astype(float)\n",
    "\n",
    "\n",
    "# Jana Pemberat Final \n",
    "# \n",
    "# \n",
    "# 1. from pop_temp_files , merge with df_jr4_new  based on idpop , to bring weight pop inside \n",
    "# 2. generate column pemberat final (weight pop) x pemberat first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab6a69",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['WEIGHT_POP'] = (df_wpop['ANGGARAN_PENDUDUK(BPPD)'])/(df_wpop['ANGGARAN_SAMPEL_STB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f562675",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#weight pop = anggaran penduduk bppd / anggaranÂ sampelÂ (STB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f36f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop['PEMBERAT_FINAL'] = (df_wpop['WEIGHT_POP'] * df_wpop['PEMBERAT_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14624fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "years_t01 = years_t01.astype(str)\n",
    "months_t01 = months_t01.astype(str)\n",
    "\n",
    "\n",
    "# ### Ingest to DATABASE DIRECTLY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e52571",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "schema='production_micro_fc_stb_monthly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae46aba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://admin:admin@10.251.49.51:5432/postgres')\n",
    "connection = engine.connect()\n",
    "print(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98007fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "end_time = time.time() # get the end time \n",
    "time_running = end_time - start_time  # Calculate the time difference\n",
    "minutes = time_running / 60  # Convert time_running to minutes\n",
    "print(f'it took {minutes} minutes to run the whole process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f80ff3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #Handle duplicates row if run twice \n",
    "\n",
    "# WITH duplicates_cte AS (\n",
    "#     SELECT column1, column2, ..., \n",
    "#            ROW_NUMBER() OVER(PARTITION BY column1, column2, ... ORDER BY column1) AS row_num\n",
    "#     FROM your_table\n",
    "# )\n",
    "# DELETE FROM your_table\n",
    "# WHERE (column1, column2, ...) IN (\n",
    "#     SELECT column1, column2, ...\n",
    "#     FROM duplicates_cte\n",
    "#     WHERE row_num > 1\n",
    "# );\n",
    "\n",
    "#Select distinct \n",
    "#drop duplicate once query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e8313",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#add months , year and quarter in dataframe to ensure we can append properly and detect duplicate rows \n",
    "df_wpop['BULAN'] = months_t01\n",
    "df_wpop['TAHUN'] = years_t01\n",
    "df_wpop['KUARTER'] = quarter_t01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84da49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sanitize_column_name(name):\n",
    "    # Remove all non-alphanumeric characters except underscores\n",
    "    name = re.sub(r'\\W+', '', name)\n",
    "    \n",
    "    # Remove leading digits if present\n",
    "    name = re.sub(r'^\\d+', '', name)\n",
    "    \n",
    "    # Ensure the name doesn't start with an underscore\n",
    "    if name.startswith('_'):\n",
    "        name = name[1:]\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d2ed3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop.columns = df_wpop.columns.map(sanitize_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b0e3e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7035c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_wpop.to_sql('JR4M'+months_t01+'Y'+years_t01+'_FC',con=engine,schema=schema,index=False,if_exists='replace')\n",
    "\n",
    "\n",
    "# ### Remove files in bppd_database path to avoid clash process in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d88798",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#to remove data directly\n",
    "def clear_garbage(path):\n",
    "    file_avaialble = [x for x in os.listdir(path)]\n",
    "\n",
    "    try:\n",
    "        for x in file_avaialble:\n",
    "            os.remove(path+'/'+x)\n",
    "            y = str(x).upper()\n",
    "            print(f'{y} excess files from processing has been relocated, contact vendor if you require the files for quality check ')\n",
    "    except Exception as e:\n",
    "        print(f'Error relocating the files: {path} - {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b8c26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#move data to clear \n",
    "def mover(path, destination_folder):\n",
    "    files_available = [x for x in os.listdir(path)]\n",
    "\n",
    "    try:\n",
    "        for file_name in files_available:\n",
    "            source_file = os.path.join(path, file_name)\n",
    "            destination_file = os.path.join(destination_folder, file_name)\n",
    "            shutil.move(source_file, destination_file)\n",
    "            y = str(file_name).upper()\n",
    "            print(f'{y} excess files from processing has been relocated to {destination_folder}. Contact the vendor if you require the files for quality check.')\n",
    "    except Exception as e:\n",
    "        print(f'Error relocating the files: {path} - {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7968275",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = bppd_database\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = popfac_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = t01_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = jr2_raw_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = bppd_storage_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = temp_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = jr4_raw_path\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = path_t02\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "# path = bppd_database\n",
    "# clear_garbage(path)\n",
    "\n",
    "# path = popfac_path\n",
    "# clear_garbage(path)\n",
    "\n",
    "# path = t01_path\n",
    "# clear_garbage(path)\n",
    "\n",
    "# path = jr2_raw_path\n",
    "# clear_garbage(path)\n",
    "\n",
    "# path = bppd_storage_path\n",
    "# clear_garbage(path)\n",
    "\n",
    "# path = temp_path\n",
    "# clear_garbage(path)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
