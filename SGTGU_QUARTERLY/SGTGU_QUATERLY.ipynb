{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef398be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import traceback \n",
    "import sys \n",
    "import warnings\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time() # get start time \n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa5e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read whatever files types being ingested \n",
    "def read_anything(path,num):\n",
    "    #Get all files avaialble in the path (path shall be only 1 file at a time to manage this )\n",
    "    get_working_files = [x for x in os.listdir(path)]\n",
    "#     get_working_files = x\n",
    "    #make this as a function later \n",
    "    if len(get_working_files) == 0:\n",
    "        print('My goodman, no files either csv nor excel found, please recheck in path the existence')\n",
    "        return None\n",
    "    \n",
    "    #Excel found \n",
    "    time_start = time.time()\n",
    "#     get_types_available = os.path.splitext(get_working_files[0])[1]\n",
    "#     file_name = os.path.splitext(get_working_files[0])[0]\n",
    "    for file_name in get_working_files:\n",
    "        get_types_available = os.path.splitext(file_name)[1]\n",
    "        if get_types_available.endswith('.xlsx'):\n",
    "            \n",
    "            time_start = time.time()\n",
    "            print('We found excel files, hence we will read it and save to df_master, hold a moment ....')\n",
    "            df_master = pd.read_excel(path+'/'+file_name,dtype=str,header=num).dropna(how='all')\n",
    "\n",
    "            int_columns = []\n",
    "            for col in df_master.columns:\n",
    "                if df_master[col].notnull().all() and df_master[col].str.isdigit().all():\n",
    "                    int_columns.append(col)\n",
    "\n",
    "            df_master[int_columns] = df_master[int_columns].astype(int)\n",
    "            time_end = time.time()\n",
    "\n",
    "            diff_time = time_end - time_start\n",
    "            print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "            return df_master\n",
    "\n",
    "        elif get_types_available.endswith('.csv'):\n",
    "            print('We found csv files, hence we will read it and save to df_master, hold a moment ....')\n",
    "            time_start = time.time()  \n",
    "            try:\n",
    "                df_master = pd.read_csv(os.path.join(path,file_name), skip_blank_lines=True,dtype=str,header=num).dropna(how='all')\n",
    "            except UnicodeDecodeError:\n",
    "                # If 'utf-8' fails, try 'ISO-8859-1' encoding\n",
    "                df_master = pd.read_csv(os.path.join(path,file_name), encoding='ISO-8859-1', skip_blank_lines=True,dtype=str,header=num).dropna(how='all')\n",
    "            int_columns = []\n",
    "            for col in df_master.columns:\n",
    "                if df_master[col].notnull().all() and df_master[col].str.isdigit().all():\n",
    "                    int_columns.append(col)\n",
    "            df_master[int_columns] = df_master[int_columns].astype(int)\n",
    "            time_end = time.time()\n",
    "            diff_time = time_end - time_start\n",
    "            print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "            return df_master\n",
    "        \n",
    "    print('No suitable files (csv or excel) found in the specified path. Continuing to search...')\n",
    "    return df_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f0c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get user input from UI in db input \n",
    "# Schema name and condition values\n",
    "db_params = {\n",
    "    'host': '10.251.49.51',\n",
    "    'database': 'postgres',\n",
    "    'user': 'admin',\n",
    "    'password': 'admin'\n",
    "}\n",
    "\n",
    "schema_name = 'reference_data'\n",
    "table_name = 'USER_INPUT'\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# cursor.execute((f'SELECT * from {schema_name}.\"{table_name}\" ORDER BY timestamp_column ')\n",
    "cursor.execute(f'''\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS row_num\n",
    "        FROM {schema_name}.\"{table_name}\"\n",
    "    ) AS numbered\n",
    "''')\n",
    "rows= cursor.fetchall()\n",
    "\n",
    "columns = [x[0] for x in cursor.description]\n",
    "df = pd.DataFrame(rows,columns=columns)\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "               \n",
    "#add function to truncate table each time new data coming in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e92926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c60f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the latest user input data numbered by max row \n",
    "max_row_num = df['row_num'].max()\n",
    "max_row = df[df['row_num'] == max_row_num]\n",
    "#MODULE 0 : QUERY SQL TABLE BASED ON USER INPUT VALUE FROM FRONT END \n",
    "QUARTER = max_row['quarter'].values[0]\n",
    "YEAR = max_row['year'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5e0543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are cleaning SGTGU Quarter 1 & Year 2021 Survey Data \n"
     ]
    }
   ],
   "source": [
    "print(f'We are cleaning SGTGU Quarter {QUARTER} & Year {YEAR} Survey Data ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f2075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get t01 latest files path\n",
    "current_path = os.getcwd()\n",
    "files_input_path_t01 = os.path.join(current_path, 'FILES INPUT', 'T01')\n",
    "raw_data_t01 = glob.glob(os.path.join(files_input_path_t01,  '*.xlsx'))\n",
    "get_t01_latest = max(raw_data_t01, key=os.path.getmtime)\n",
    "\n",
    "# files_input_path_raw = os.path.join(current_path, 'FILES INPUT', 'RAW_DATA')\n",
    "# raw_data_raw = glob.glob(os.path.join(files_input_path_raw,  '*.xlsx'))\n",
    "# get_raw_latest = max(raw_data_raw, key=os.path.getmtime)\n",
    "bin_path =  os.path.join(current_path, 'BIN')\n",
    "\n",
    "files_input_path_raw = os.path.join(current_path, 'FILES INPUT', 'RAW_DATA')\n",
    "\n",
    "\n",
    "#set output path \n",
    "files_output_path_result = os.path.join(current_path, 'FILES OUTPUT')\n",
    "files_output_path_result_final = os.path.join(current_path, 'FILES OUTPUT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a15e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all files as per list t01 and structure into proper dataframe and exclude all unecessary row \n",
    "#Files that need to be read is in sheet number 2 and start reading it from row 6 to avoid messy structure\n",
    "\n",
    "#t01 \n",
    "# df_t01 = pd.read_excel(get_t01_latest,dtype=str,header=5,sheet_name='HEADER_MAIN')\n",
    "df_t01 = pd.read_excel(get_t01_latest,dtype=str,header=5,sheet_name='HEADER_MAIN')\n",
    "\n",
    "#Read all files as per list t02 and structure into proper dataframe and exclude all unecessary row \n",
    "#t02\n",
    "# df_t02 = pd.read_excel(get_t01_latest,dtype=str,sheet_name='MSIC2008_MAIN')\n",
    "df_t02 = pd.read_excel(get_t01_latest,dtype=str,sheet_name='MSIC2008_MAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febe7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t01['NEW_ORDER BY VAR'] = df_t01['NEW_ORDER BY VAR'].astype(int)\n",
    "df_t01_sort = df_t01.sort_values('NEW_ORDER BY VAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47675bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "881f0c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found csv files, hence we will read it and save to df_master, hold a moment ....\n",
      "My performance reading EJOB_SGT_Suku_2_Tahun_2021.csv file took : 28.93561291694641 seconds\n"
     ]
    }
   ],
   "source": [
    "#Read all files as per list raw and structure into proper dataframe and exclude all unecessary row\n",
    "#user to confirm file structure in here. and ile format \n",
    "\n",
    "df_raw = read_anything(files_input_path_raw,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b91e770e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files success. We are organizing the header according to the files uploaded\n"
     ]
    }
   ],
   "source": [
    "print('Reading files success. We are organizing the header according to the files uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8ddea27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare header between t01 against df_raw  \n",
    "\n",
    "df_raw_col_list = df_raw.columns.to_list()\n",
    "df_old_name_list = df_t01_sort['ORI_EJOB HEADER'].to_list()\n",
    "df_new_name_list = df_t01_sort['NEW_DATASET HEADER'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3780984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_rename = df_raw.rename(columns=dict(zip(df_old_name_list,df_new_name_list)))\n",
    "df_raw_rename_reorg =df_raw_rename.reindex(columns=df_new_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0e6b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns are renamed and reorganized successfully according to header references provided.\n"
     ]
    }
   ],
   "source": [
    "df_raw_rename_reorg_check = df_raw_rename_reorg.columns.to_list()\n",
    "result= []\n",
    "j = zip(df_raw_rename_reorg_check, df_new_name_list)\n",
    "for x, y in j: \n",
    "    result.append(x != y)\n",
    "    \n",
    "if any(result):\n",
    "    print(\"Columns are not renamed and reorganized properly.Please recheck reference file uploaded \")\n",
    "else:\n",
    "    print(\"Columns are renamed and reorganized successfully according to header references provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db1acf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are examining MSIC 2008 to fill SECTION_SMPL & SECTION}\n"
     ]
    }
   ],
   "source": [
    "print('We are examining MSIC 2008 to fill SECTION_SMPL & SECTION}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f6573b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of column so to rearrange back column from t02 back to raw main dataframe \n",
    "section_column_number = df_raw_rename_reorg.columns.get_loc('SECTION')\n",
    "section_smpl_column_number = df_raw_rename_reorg.columns.get_loc('SECTION_SMPL')\n",
    "msic2008_column_number = df_raw_rename_reorg.columns.get_loc('MSIC2008')\n",
    "msic2008_smpl_column_number = df_raw_rename_reorg.columns.get_loc('MSIC2008_SMPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da713da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1252ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add portion to create SECTION_SMPL & SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ffd7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t02_copy_1 = df_t02.copy()\n",
    "df_t02_copy_2 = df_t02.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87cdf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create for SECTION_SMPL & MSIC2008_SMPL for join with df raw and fill up SECTION SMPL column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1003f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t02_copy_1 = df_t02_copy_1.rename(columns={'without \\'0\\'':'MSIC2008_SMPL'})\n",
    "df_t02_smpl = df_t02_copy_1[['MSIC2008_SMPL','SECTION']]\n",
    "df_t02_smpl = df_t02_smpl.rename(columns={'SECTION':'SECTION_SMPL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afdc53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success in generating specific column <bound method IndexOpsMixin.tolist of Index(['MSIC2008_SMPL', 'SECTION_SMPL'], dtype='object')> for t02\n"
     ]
    }
   ],
   "source": [
    "output_smpl = df_t02_smpl.columns.to_list\n",
    "print(f'success in generating specific column {output_smpl} for t02' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d9b4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create for SECTION & MSIC2008 for join with df raw and fill up SECTION SMPL column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29754678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t02_copy_2 = df_t02_copy_2.rename(columns={'without \\'0\\'':'MSIC2008'})\n",
    "df_t02_non = df_t02_copy_2[['MSIC2008','SECTION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ff5929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success in generating specific column      MSIC2008 SECTION\n",
      "0        1111       A\n",
      "1        1112       A\n",
      "2        1113       A\n",
      "3        1119       A\n",
      "4        1120       A\n",
      "...       ...     ...\n",
      "1169    96099       S\n",
      "1170    97000       T\n",
      "1171    98100       T\n",
      "1172    98200       T\n",
      "1173    99000       U\n",
      "\n",
      "[1174 rows x 2 columns] for t02\n"
     ]
    }
   ],
   "source": [
    "output_non = df_t02_non.columns.to_list\n",
    "print(f'success in generating specific column {df_t02_non} for t02' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "331d6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete SECTION & SECTION_SMPL in df_raw_rename_reorg\n",
    "\n",
    "df_raw_rename_reorg_dropped_section_smpl = df_raw_rename_reorg.drop(['SECTION','SECTION_SMPL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ca2e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_raw_data_v1 = df_t02_non.merge(df_raw_rename_reorg_dropped_section_smpl, how ='right', on='MSIC2008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f8985a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t02_smpl['MSIC2008_SMPL'] = df_t02_smpl['MSIC2008_SMPL'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db36ef9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_raw_data_v2 = df_t02_smpl.merge(df_final_raw_data_v1, how ='right', on='MSIC2008_SMPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ba2b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_move = ['SECTION_SMPL','SECTION','MSIC2008','MSIC2008_SMPL']\n",
    "new_indexes = [section_smpl_column_number,section_column_number,msic2008_smpl_column_number,msic2008_column_number,]\n",
    "\n",
    "columns = df_final_raw_data_v2.columns.to_list()\n",
    "\n",
    "#remove the column in list \n",
    "for column in columns_to_move:\n",
    "    columns.remove(column)\n",
    "    \n",
    "for column,index in zip(columns_to_move,new_indexes):\n",
    "    columns.insert(index,column)\n",
    "\n",
    "df_final_raw_data = df_final_raw_data_v2.reindex(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1c0539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_raw_data.to_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ceb2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_final_raw_data[['NEWSSID','REGISTERED_NAME','TRADING_NAME']]\n",
    "df = df_final_raw_data.copy()\n",
    "year = df.loc[1,'YEAR']\n",
    "quarter = df.loc[1,'QUARTER']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ee2d7",
   "metadata": {},
   "source": [
    "#### MODULE 4 FILL UP STRATA_EMPL COLUMNS ACCORDING TO FILTER OF STRATA_EMPL & SECTION_SMPL \n",
    "\n",
    "IF SECTION_SMPL == C AND F1310 == RANGE(0,5) -> FILL COLUMN STRATA_EMPL= 4\n",
    "IF SECTION_SMPL == C AND F1310 == RANGE(5,75) -> FILL COLUMN STRATA_EMPL= 3\n",
    "IF SECTION_SMPL == C AND F1310 == RANGE(75,201) -> FILL COLUMN STRATA_EMPL= 2\n",
    "IF SECTION_SMPL == C AND F1310 >= 201 -> FILL COLUMN STRATA_EMPL= 1\n",
    "\n",
    "IF SECTION_SMPL != C AND F1310 == RANGE(0,5) -> FILL COLUMN STRATA_EMPL= 4\n",
    "IF SECTION_SMPL != C AND F1310 == RANGE(5,30) -> FILL COLUMN STRATA_EMPL= 3\n",
    "IF SECTION_SMPL != C AND F1310 == RANGE(30,76) -> FILL COLUMN STRATA_EMPL= 2\n",
    "IF SECTION_SMPL != C AND F1310 >= 76 -> FILL COLUMN STRATA_EMPL= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9bb27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dynamic column ending with F1310 \n",
    "colx = [col for col in df_final_raw_data.columns if 'F1310' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ab3d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update strata_empl value based on filtering of F1310 and SECTION_SMPL\n",
    "\n",
    "condition_a_1 = (df_final_raw_data['SECTION_SMPL'] == 'C') & (df_final_raw_data[colx[0]] < 5)\n",
    "condition_a_2 = (df_final_raw_data['SECTION_SMPL'] == 'C') & (df_final_raw_data[colx[0]].isin(range(5,75)))\n",
    "condition_a_3 = (df_final_raw_data['SECTION_SMPL'] == 'C') & (df_final_raw_data[colx[0]].isin(range(75,201)))\n",
    "condition_a_4 = (df_final_raw_data['SECTION_SMPL'] == 'C') & (df_final_raw_data[colx[0]] >= 201)\n",
    "\n",
    "condition_list = [\n",
    "    condition_a_1,\n",
    "    condition_a_2,\n",
    "    condition_a_3,\n",
    "    condition_a_4\n",
    "]\n",
    "default_value = np.nan\n",
    "choices = [4,3,2,1]\n",
    "df_final_raw_data['STRATA_EMPL'] = np.select(condition_list,choices, default=default_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c128f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update strata_empl value based on filtering of F1310 and SECTION_SMPL\n",
    "\n",
    "condition_b_1 = ((df_final_raw_data['SECTION_SMPL'] != 'C') & (df_final_raw_data[colx[0]] < 5)) #& pd.notnull(df_final_raw_data[colx[0]\n",
    "condition_b_2 = ((df_final_raw_data['SECTION_SMPL'] != 'C') & (df_final_raw_data[colx[0]].isin(range(5,30))))\n",
    "condition_b_3 = ((df_final_raw_data['SECTION_SMPL'] != 'C') & (df_final_raw_data[colx[0]].isin(range(30,76))))\n",
    "condition_b_4 = ((df_final_raw_data['SECTION_SMPL'] != 'C') & (df_final_raw_data[colx[0]] >= 76))\n",
    "\n",
    "condition_list = [\n",
    "    condition_b_1,\n",
    "    condition_b_2,\n",
    "    condition_b_3,\n",
    "    condition_b_4\n",
    "]\n",
    "default_value = np.nan\n",
    "choices = [4,3,2,1]\n",
    "df_final_raw_data['STRATA_EMPL'] = np.select(condition_list,choices, default=default_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eab924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "069ab706",
   "metadata": {},
   "source": [
    "#### MODULE 5 : \n",
    "ADDING LEADING ZERO TO FRONT SUBSTATE CODE TO COMPLETE 2 DIGITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59b6d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all column to \n",
    "df_final_raw_data['SUBSTATE_CODE'] = df_final_raw_data['SUBSTATE_CODE'].apply(lambda x: str(int(x)) if pd.notnull(x) and not isinstance(x, str) else '')\n",
    "\n",
    "# for i, x in enumerate(df_final_raw_data['SUBSTATE_CODE']):\n",
    "#     if pd.notnull(x):\n",
    "#         if not isinstance(x, str):\n",
    "#             df_final_raw_data.loc[i, 'SUBSTATE_CODE'] = str(int(x))\n",
    "#         else:\n",
    "#             df_final_raw_data.loc[i, 'SUBSTATE_CODE'] = x\n",
    "#     else:\n",
    "#         df_final_raw_data.loc[i, 'SUBSTATE_CODE'] = ''\n",
    "        \n",
    "# isinstance(object, type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbff84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Substate_code changed to dtypes = object for the basis of adding leading 0\n"
     ]
    }
   ],
   "source": [
    "checktypes = df_final_raw_data['SUBSTATE_CODE'].dtype\n",
    "print(f'Column Substate_code changed to dtypes = {checktypes} for the basis of adding leading 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d53986a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check does this meet requirement before changing value this is the unique value list: 14\n"
     ]
    }
   ],
   "source": [
    "before = df_final_raw_data['SUBSTATE_CODE'].iloc[1]\n",
    "print(f'Check does this meet requirement before changing value this is the unique value list: {before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0ae9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(df_final_raw_data['SUBSTATE_CODE']):\n",
    "    if x not in ['0','']:\n",
    "        if len(x) <=2:\n",
    "            df_final_raw_data.loc[i,'SUBSTATE_CODE']= x.zfill(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7e0b47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check does this meet requirement after changing value this is the unique value list: 14\n"
     ]
    }
   ],
   "source": [
    "after = df_final_raw_data['SUBSTATE_CODE'].iloc[1]\n",
    "print(f'Check does this meet requirement after changing value this is the unique value list: {after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaeb5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38595fb5",
   "metadata": {},
   "source": [
    "ADDING LEADING ZERO TO FRONT NEWSSID TO COMPLETE 12 DIGITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1a33fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2 Change flaot to str \n",
    "# for i, x in enumerate(df_final_raw_data['NEWSSID']):\n",
    "#     if pd.notnull(x) and x not in ['0','']:\n",
    "#         if not isinstance(x,str):\n",
    "#             df_final_raw_data.loc[i,'NEWSSID'] = str(int(x))\n",
    "#         else:\n",
    "#             df_final_raw_data.loc[i,'NEWSSID'] = x\n",
    "#     else:\n",
    "#          df_final_raw_data.loc[i, 'SUBSTATE_CODE'] = ''   \n",
    "\n",
    "df_final_raw_data['NEWSSID'] = df_final_raw_data['NEWSSID'].apply(lambda x: str(int(x)) if pd.notnull(x) and not isinstance(x, str) else '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "534ae29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column NEWSSID changed to dtypes = object for the basis of adding leading 0\n"
     ]
    }
   ],
   "source": [
    "checktypes = df_final_raw_data['NEWSSID'].dtype\n",
    "print(f'Column NEWSSID changed to dtypes = {checktypes} for the basis of adding leading 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5e5aca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NEWSSID does this meet requirement before changing value this is the unique value list: 3975399\n"
     ]
    }
   ],
   "source": [
    "before = df_final_raw_data['NEWSSID'].iloc[1]\n",
    "print(f'Check for NEWSSID does this meet requirement before changing value this is the unique value list: {before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecaa99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(df_final_raw_data['NEWSSID']):\n",
    "    if x not in ['','0']:\n",
    "        if len(x) <= 12 :\n",
    "            df_final_raw_data.loc[i,'NEWSSID'] = x.zfill(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6790dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NEWSSID does this meet requirement before changing value this is the unique value list: 000003975399\n"
     ]
    }
   ],
   "source": [
    "after = df_final_raw_data['NEWSSID'].iloc[1]\n",
    "\n",
    "print(f'Check for NEWSSID does this meet requirement before changing value this is the unique value list: {after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f9fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7eca68",
   "metadata": {},
   "source": [
    "ADDING LEADING ZERO TO FRONT MSIC2008_SMPL TO COMPLETE 5 DIGITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66c786b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column MSIC2008_SMPL changed to dtypes = object for the basis of adding leading 0\n"
     ]
    }
   ],
   "source": [
    "df_final_raw_data['MSIC2008_SMPL'] = df_final_raw_data['MSIC2008_SMPL'].apply(lambda x: str(int(x)) if pd.notnull(x) and not isinstance(x, str) else '')\n",
    "checktypes = df_final_raw_data['MSIC2008_SMPL'].dtype\n",
    "print(f'Column MSIC2008_SMPL changed to dtypes = {checktypes} for the basis of adding leading 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39ffb37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for MSIC2008_SMPL does this meet requirement before changing value this is the unique value list: 61300\n"
     ]
    }
   ],
   "source": [
    "before = df_final_raw_data['MSIC2008_SMPL'].iloc[1]\n",
    "print(f'Check for MSIC2008_SMPL does this meet requirement before changing value this is the unique value list: {before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f6275f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(df_final_raw_data['MSIC2008_SMPL']):\n",
    "    if x not in ['','0']:\n",
    "        if len(x) <= 5 :\n",
    "            df_final_raw_data.loc[i,'MSIC2008_SMPL'] = x.zfill(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bd6e241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for MSIC2008_SMPL does this meet requirement before changing value this is the unique value list: 000000061300\n"
     ]
    }
   ],
   "source": [
    "after = df_final_raw_data['MSIC2008_SMPL'].iloc[1]\n",
    "\n",
    "print(f'Check for MSIC2008_SMPL does this meet requirement before changing value this is the unique value list: {after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86335706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "644c167a",
   "metadata": {},
   "source": [
    "ADDING LEADING ZERO TO FRONT MSIC2008 TO COMPLETE 5 DIGITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c4f7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column MSIC2008 changed to dtypes = object for the basis of adding leading 0\n"
     ]
    }
   ],
   "source": [
    "df_final_raw_data['MSIC2008'] = df_final_raw_data['MSIC2008'].apply(lambda x: str(int(x)) if pd.notnull(x) and not isinstance(x, str) else '')\n",
    "checktypes = df_final_raw_data['MSIC2008'].dtype\n",
    "print(f'Column MSIC2008 changed to dtypes = {checktypes} for the basis of adding leading 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "659d065b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for MSIC2008 does this meet requirement before changing value this is the unique value list: \n"
     ]
    }
   ],
   "source": [
    "before = df_final_raw_data['MSIC2008'].iloc[1]\n",
    "print(f'Check for MSIC2008 does this meet requirement before changing value this is the unique value list: {before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a32d5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(df_final_raw_data['MSIC2008']):\n",
    "    if x not in ['','0']:\n",
    "        if len(x) <= 5 :\n",
    "            df_final_raw_data.loc[i,'MSIC2008'] = x.zfill(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec953352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for one of the value from MSIC2008, does this meet requirement before changing value this is the unique value list: \n"
     ]
    }
   ],
   "source": [
    "after = df_final_raw_data['MSIC2008'].iloc[1]\n",
    "\n",
    "print(f'Check for one of the value from MSIC2008, does this meet requirement before changing value this is the unique value list: {after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f65a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9acf98b8",
   "metadata": {},
   "source": [
    "ADDING LEADING ZERO TO FRONT STATE_CODE TO COMPLETE 5 DIGITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43bda667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column STATE_CODE changed to dtypes = object for the basis of adding leading 0\n"
     ]
    }
   ],
   "source": [
    "df_final_raw_data['STATE_CODE'] = df_final_raw_data['STATE_CODE'].apply(lambda x: str(int(x)) if pd.notnull(x) and not isinstance(x, str) else '')\n",
    "checktypes = df_final_raw_data['STATE_CODE'].dtype\n",
    "print(f'Column STATE_CODE changed to dtypes = {checktypes} for the basis of adding leading 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "485a8078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for STATE_CODE does this meet requirement before changing value this is the unique value list: 11\n"
     ]
    }
   ],
   "source": [
    "before = df_final_raw_data['STATE_CODE'].iloc[1]\n",
    "print(f'Check for STATE_CODE does this meet requirement before changing value this is the unique value list: {before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58dbd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(df_final_raw_data['STATE_CODE']):\n",
    "    if x not in ['','0']:\n",
    "        if len(x) <= 5 :\n",
    "            df_final_raw_data.loc[i,'STATE_CODE'] = x.zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23c4caa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for one of the value from STATE_CODE, does this meet requirement before changing value this is the unique value list: 11\n"
     ]
    }
   ],
   "source": [
    "after = df_final_raw_data['STATE_CODE'].iloc[1]\n",
    "\n",
    "print(f'Check for one of the value from STATE_CODE, does this meet requirement before changing value this is the unique value list: {after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25c476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cc1e034",
   "metadata": {},
   "source": [
    "### Module 7 : Summation all category (1-9) for summation of total salaries & wages (Q=L+M+N+O+P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1ddd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the starting index of column F0101 in col \n",
    "coly = [col for col in df_final_raw_data.columns if 'F0101' in col]\n",
    "first_index = df_final_raw_data.columns.get_loc(coly[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfdd99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vab_list_mod_8 =  [ 'L', 'M', 'N', 'O', 'P','Q']\n",
    "#Select column name to manipulate from master dataframe \n",
    "selection_col_2 = []\n",
    "for x in df_final_raw_data.columns[first_index:]:\n",
    "    if x[:1] in vab_list_mod_8:\n",
    "        selection_col_2.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30a4dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign selection for module 7 into \n",
    "\n",
    "df_mod7_1=df_final_raw_data[selection_col_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5604021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L23206', 'L23205', 'L23204', 'M23204', 'M23205', 'M23206', 'N23204', 'N23205', 'N23206', 'O23205', 'O23204', 'O23206', 'P23206', 'P23204', 'P23205', 'Q23205', 'Q23204', 'Q23206'] are the unique partial front string to be re arranged before filtering from main dataframe process\n",
      "['04', '05', '06'] are unique partial string sorted by quarters ascending \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#wrangle the data \n",
    "#1. List all column from filtered to be appended in list_j\n",
    "list_j = []\n",
    "for x in df_mod7_1.columns:\n",
    "    y = x[:6]\n",
    "    list_j.append(y)\n",
    "#2. Get the unique list \n",
    "list_j_unique = list(set(list_j))  \n",
    "\n",
    "#3. Get the unique sorted alphabetically \n",
    "sort_alph = ['L','M','N','O','P','Q']\n",
    "#Sort the list from unique list into separated set of list to make it easier during loop\n",
    "# sample output a = ['L23204', 'M23204', 'N23204', 'O23204', 'P23204', 'Q23204']\n",
    "list_j_sort = sorted(list_j_unique,key=lambda x:(len(x),x[0]))\n",
    "\n",
    "\n",
    "print(f'{list_j_sort} are the unique partial front string to be re arranged before filtering from main dataframe process')\n",
    "\n",
    "#make a dictionary from sortation filter so we can filter based on variables \n",
    "\n",
    "group_dict = {}\n",
    "for col_name in list_j_sort:\n",
    "    end = col_name[-2:]\n",
    "#if column name existing int group_dictionary, then append in created list\n",
    "    if end in group_dict:\n",
    "        group_dict[end].append(col_name)\n",
    "        \n",
    "#if dont have , then create a new one. \n",
    "    else: \n",
    "        group_dict[end] = [col_name]\n",
    "\n",
    "dynamic_keys = sorted(group_dict.keys())\n",
    "\n",
    "print(f'{dynamic_keys} are unique partial string sorted by quarters ascending ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8bbdba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Strategy \n",
    "# Loop through column_no_q and column_w_q to filter columns based on the conditions.\n",
    "# Change the data type of columns in df[sum_this] and df[paste_here] to numeric and fill any missing values with 0.\n",
    "# Calculate the sum of each row in df[sum_this] and assign the result to sum_val.\n",
    "# Reshape sum_val to a column vector and assign it to df_output[paste_here].\n",
    "# Change the data type of columns in df_output[sum_this] and df_output[paste_here] to numeric and fill any missing values with 0.\n",
    "# Calculate the sum before the arithmetic expression for df_output[paste_here] and df_output[sum_this] using sum(axis=0) and store them in result_mod7_before and input_mod7_before, respectively.\n",
    "# Calculate the sum after the amendment for df_output[paste_here] and df_output[sum_this] using sum(axis=0) and store them in result_mod7_after and input_mod7_after, respectively.\n",
    "# Compare result_mod7_after[0] with input_mod7_after to check if the results match and print the corresponding message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "edec2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to filter data by partial string match and sum based on conditional \n",
    "def process_dataframe(df,group_num,grouping_1,df_output):\n",
    "    sum_this = []\n",
    "    paste_here = []\n",
    "    listx = group_dict[group_num]\n",
    "    exclude_q = [x for x in listx if 'Q' not in x]\n",
    "    q_only = [x for x in  listx if 'Q' in x]\n",
    "    # get the list of column available in df to separate \n",
    "    column_no_q = [x for x in df if any(q in x for q in exclude_q)]\n",
    "    column_w_q = [x for x in df if any(q in x for q in q_only)]\n",
    "    \n",
    "    #Get the list of column that is in list \n",
    "    #The sample of dictionary as follows : - \n",
    "\n",
    "    # {'04': ['L23204', 'M23204', 'N23204', 'O23204', 'P23204', 'Q23204'],\n",
    "    #  '06': ['L23206', 'M23206', 'N23206', 'O23206', 'P23206', 'Q23206'],\n",
    "    #  '05': ['L23205', 'M23205', 'N23205', 'O23205', 'P23205', 'Q23205']}\n",
    "\n",
    "    # empty frame to insert value for each function run temporary to paste value in df main \n",
    "\n",
    "    #loop to filter from df\n",
    "    for x in column_no_q:\n",
    "        for y in group_dict[group_num]:\n",
    "            if x.endswith(grouping_1) and y in x:\n",
    "                sum_this.append(x)\n",
    "\n",
    "    for x in column_w_q:\n",
    "        for y in group_dict[group_num]:\n",
    "            if x.endswith(grouping_1) and y in x:\n",
    "                paste_here.append(x)\n",
    "                \n",
    "    #change datatype from string to int and sum value from list of sumthis to get the overall value  \n",
    "    df[sum_this] = df[sum_this].apply(pd.to_numeric, errors='coerce')\n",
    "    df[sum_this] = df[sum_this].fillna(0).astype(int)\n",
    "    df[paste_here] = df[paste_here].apply(pd.to_numeric, errors='coerce')\n",
    "    df[paste_here] = df[paste_here].fillna(0).astype(int)\n",
    "    df_output[sum_this] = df_output[sum_this].apply(pd.to_numeric, errors='coerce')\n",
    "    df_output[sum_this] = df_output[sum_this].fillna(0).astype(int)\n",
    "    df_output[paste_here] = df_output[paste_here].apply(pd.to_numeric, errors='coerce')\n",
    "    df_output[paste_here] = df_output[paste_here].fillna(0).astype(int)\n",
    "\n",
    "    #get the aggreagate of all column in sum_this\n",
    "    sum_val = df[sum_this].sum(axis=1)\n",
    "    #paste the aggregate in column QXXX and ending with group_num in df_final_raw as final value \n",
    "    df_output[paste_here] = sum_val.values.reshape(-1, 1)\n",
    "\n",
    "    #get the column Q in 01 and sum the value column first from main dataframe \n",
    "\n",
    "    #change the dtyps for section affected only from df main so we can sum this \n",
    "\n",
    "    #check & test \n",
    "    #Get the sum before arimethic expression for df_main as initial value \n",
    "    result_mod7_before = df_output[paste_here].sum(axis=0)\n",
    "    input_mod7_before = df_output[sum_this].sum(axis=0)\n",
    "    input_mod7_before = input_mod7_before.sum()\n",
    "    \n",
    "    #get the sum of column after in df_mod7 and after ammendment made in \n",
    "    result_mod7_after = df_output[paste_here].sum(axis=0)\n",
    "    input_mod7_after = df_output[sum_this].sum(axis=0)\n",
    "    input_mod7_after = input_mod7_after.sum()\n",
    "\n",
    "    if result_mod7_after[0] == input_mod7_after:\n",
    "        print(f'{paste_here} successfully aggregated & match with source value which initially {result_mod7_before[0]} turned to {result_mod7_after[0]}')\n",
    "    else:\n",
    "        print(f'{paste_here} warning, result and source do not match, recheck data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5c9134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q23204_F4501'] successfully aggregated & match with source value which initially 5901030839 turned to 5901030839\n",
      "['Q23204_F4502'] successfully aggregated & match with source value which initially 6033272525 turned to 6033272525\n",
      "['Q23204_F4503'] successfully aggregated & match with source value which initially 1625083719 turned to 1625083719\n",
      "['Q23204_F4504'] successfully aggregated & match with source value which initially 632360200 turned to 632360200\n",
      "['Q23204_F4505'] successfully aggregated & match with source value which initially 512484912 turned to 512484912\n",
      "['Q23204_F4506'] successfully aggregated & match with source value which initially 63396881 turned to 63396881\n",
      "['Q23204_F4507'] successfully aggregated & match with source value which initially 213025570 turned to 213025570\n",
      "['Q23204_F4508'] successfully aggregated & match with source value which initially 2389252580 turned to 2389252580\n",
      "['Q23204_F4509'] successfully aggregated & match with source value which initially 612746194 turned to 612746194\n",
      "['Q23204_F4510'] successfully aggregated & match with source value which initially 17919242538 turned to 17919242538\n"
     ]
    }
   ],
   "source": [
    "#group num is based on dynamic_keys = ['04', '05', '06'] position which is dunamically based on quarter.\n",
    "# sum the value based on pulled dataframe and paste the value into df_main into as ouput \n",
    "grouping_1 = ('1','2','3','4','5','6','7','8','9','0')\n",
    "group_num = dynamic_keys[0]\n",
    "df = df_mod7_1\n",
    "df_output = df_final_raw_data\n",
    "\n",
    "for x in grouping_1:\n",
    "    process_dataframe(df,group_num,x,df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6323b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q23205_F5101'] successfully aggregated & match with source value which initially 1594666692 turned to 1594666692\n",
      "['Q23205_F5102'] successfully aggregated & match with source value which initially 3702567852 turned to 3702567852\n",
      "['Q23205_F5103'] successfully aggregated & match with source value which initially 1327887566 turned to 1327887566\n",
      "['Q23205_F5104'] successfully aggregated & match with source value which initially 565593911 turned to 565593911\n",
      "['Q23205_F5105'] successfully aggregated & match with source value which initially 466461838 turned to 466461838\n",
      "['Q23205_F5106'] successfully aggregated & match with source value which initially 61697801 turned to 61697801\n",
      "['Q23205_F5107'] successfully aggregated & match with source value which initially 227108900 turned to 227108900\n",
      "['Q23205_F5108'] successfully aggregated & match with source value which initially 2079500906 turned to 2079500906\n",
      "['Q23205_F5109'] successfully aggregated & match with source value which initially 571224340 turned to 571224340\n",
      "['Q23205_F5110'] successfully aggregated & match with source value which initially 10587070842 turned to 10587070842\n"
     ]
    }
   ],
   "source": [
    "#group num is based on dynamic_keys = ['04', '05', '06'] position which is dunamically based on quarter.\n",
    "# sum the value based on pulled dataframe and paste the value into df_main into as ouput \n",
    "grouping_1 = ('1','2','3','4','5','6','7','8','9','0')\n",
    "group_num = dynamic_keys[1]\n",
    "df = df_mod7_1\n",
    "df_output = df_final_raw_data\n",
    "\n",
    "for x in grouping_1:\n",
    "    process_dataframe(df,group_num,x,df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1ebd183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q23206_F5701'] successfully aggregated & match with source value which initially 1468789383 turned to 1468789383\n",
      "['Q23206_F5702'] successfully aggregated & match with source value which initially 1532700053 turned to 1532700053\n",
      "['Q23206_F5703'] successfully aggregated & match with source value which initially 3461728244 turned to 3461728244\n",
      "['Q23206_F5704'] successfully aggregated & match with source value which initially 2703749574 turned to 2703749574\n",
      "['Q23206_F5705'] successfully aggregated & match with source value which initially 453001744 turned to 453001744\n",
      "['Q23206_F5706'] successfully aggregated & match with source value which initially 60488609 turned to 60488609\n",
      "['Q23206_F5707'] successfully aggregated & match with source value which initially 210129421 turned to 210129421\n",
      "['Q23206_F5708'] successfully aggregated & match with source value which initially 2458272988 turned to 2458272988\n",
      "['Q23206_F5709'] successfully aggregated & match with source value which initially 558017738 turned to 558017738\n",
      "['Q23206_F5710'] successfully aggregated & match with source value which initially 12734253591 turned to 12734253591\n"
     ]
    }
   ],
   "source": [
    "#group num is based on dynamic_keys = ['04', '05', '06'] position which is dunamically based on quarter.\n",
    "# sum the value based on pulled dataframe and paste the value into df_main into as ouput \n",
    "grouping_1 = ('1','2','3','4','5','6','7','8','9','0')\n",
    "group_num = dynamic_keys[2]\n",
    "df = df_mod7_1\n",
    "df_output = df_final_raw_data\n",
    "\n",
    "for x in grouping_1:\n",
    "    process_dataframe(df,group_num,x,df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610085d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ac6303",
   "metadata": {},
   "source": [
    "### Module 8 : Summation all category (1-9) for summation of total separation (X=D+E+F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea8e4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vab_list_mod_9 =  ['D','E','F','X']\n",
    "#Select column name to manipulate from master dataframe \n",
    "selection_col_3 = []\n",
    "for x in df_final_raw_data.columns[first_index:]:\n",
    "    if x[:1] in vab_list_mod_9:\n",
    "        selection_col_3.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30fc2426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D23205', 'D23206', 'D23204', 'E23204', 'E23206', 'E23205', 'F23206', 'F23205', 'F23204', 'X23205', 'X23206', 'X23204'] are the unique partial front string to be re arranged before filtering from main dataframe process\n",
      "['04', '05', '06'] are unique partial string sorted by quarters ascending \n"
     ]
    }
   ],
   "source": [
    "#filter the column out from main_df \n",
    "df_vabs_mod9= df_final_raw_data[selection_col_3]\n",
    "\n",
    "#wrangle the data \n",
    "#1. List all column from filtered to be appended in list_j\n",
    "list_j = []\n",
    "for x in df_vabs_mod9.columns:\n",
    "    y = x[:6]\n",
    "    list_j.append(y)\n",
    "#2. Get the unique list \n",
    "list_j_unique = list(set(list_j))  \n",
    "\n",
    "#3. Get the unique sorted alphabetically \n",
    "sort_alph = ['D','E','F','X']\n",
    "#Sort the list from unique list into separated set of list to make it easier during loop\n",
    "# sample output a = ['L23204', 'M23204', 'N23204', 'O23204', 'P23204', 'Q23204']\n",
    "list_j_sort = sorted(list_j_unique,key=lambda x:(len(x),x[0]))\n",
    "\n",
    "\n",
    "print(f'{list_j_sort} are the unique partial front string to be re arranged before filtering from main dataframe process')\n",
    "\n",
    "#make a dictionary from sortation filter so we can filter based on variables \n",
    "\n",
    "group_dict = {}\n",
    "for col_name in list_j_sort:\n",
    "    end = col_name[-2:]\n",
    "#if column name existing int group_dictionary, then append in created list\n",
    "    if end in group_dict:\n",
    "        group_dict[end].append(col_name)\n",
    "        \n",
    "#if dont have , then create a new one. \n",
    "    else: \n",
    "        group_dict[end] = [col_name]\n",
    "\n",
    "dynamic_keys = sorted(group_dict.keys())\n",
    "\n",
    "print(f'{dynamic_keys} are unique partial string sorted by quarters ascending ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff78dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to filter data by partial string match and sum based on conditional \n",
    "def process_dataframe_2(df,group_num,grouping_1,df_output):\n",
    "    sum_this = []\n",
    "    paste_here = []\n",
    "    listx = group_dict[group_num]\n",
    "    exclude_q = [x for x in listx if 'X' not in x]\n",
    "    q_only = [x for x in  listx if 'X' in x]\n",
    "    # get the list of column available in df to separate \n",
    "    column_no_q = [x for x in df if any(q in x for q in exclude_q)]\n",
    "    column_w_q = [x for x in df if any(q in x for q in q_only)]\n",
    "    \n",
    "    #Get the list of column that is in list \n",
    "    #The sample of dictionary as follows : - \n",
    "\n",
    "    # {'04': ['L23204', 'M23204', 'N23204', 'O23204', 'P23204', 'Q23204'],\n",
    "    #  '06': ['L23206', 'M23206', 'N23206', 'O23206', 'P23206', 'Q23206'],\n",
    "    #  '05': ['L23205', 'M23205', 'N23205', 'O23205', 'P23205', 'Q23205']}\n",
    "\n",
    "    # empty frame to insert value for each function run temporary to paste value in df main \n",
    "\n",
    "    #loop to filter from df\n",
    "    for x in column_no_q:\n",
    "        for y in group_dict[group_num]:\n",
    "            if x.endswith(grouping_1) and y in x:\n",
    "                sum_this.append(x)\n",
    "\n",
    "    for x in column_w_q:\n",
    "        for y in group_dict[group_num]:\n",
    "            if x.endswith(grouping_1) and y in x:\n",
    "                paste_here.append(x)\n",
    "                \n",
    "    #change datatype from string to int and sum value from list of sumthis to get the overall value  \n",
    "    df[sum_this] = df[sum_this].apply(pd.to_numeric, errors='coerce')\n",
    "    df[sum_this] = df[sum_this].fillna(0).astype(int)\n",
    "    df[paste_here] = df[paste_here].apply(pd.to_numeric, errors='coerce')\n",
    "    df[paste_here] = df[paste_here].fillna(0).astype(int)\n",
    "    df_output[sum_this] = df_output[sum_this].apply(pd.to_numeric, errors='coerce')\n",
    "    df_output[sum_this] = df_output[sum_this].fillna(0).astype(int)\n",
    "    df_output[paste_here] = df_output[paste_here].apply(pd.to_numeric, errors='coerce')\n",
    "    df_output[paste_here] = df_output[paste_here].fillna(0).astype(int)\n",
    "\n",
    "    #get the aggreagate of all column in sum_this\n",
    "    sum_val = df[sum_this].sum(axis=1)\n",
    "    \n",
    "    #paste the aggregate in column QXXX and ending with group_num in df_final_raw as final value \n",
    "    df_output[paste_here] = sum_val.values.reshape(-1, 1)\n",
    "    \n",
    "#     sum_val_df = pd.DataFrame({col: sum_val for col in paste_here})\n",
    "\n",
    "#     df_output[paste_here] = sum_val_df\n",
    "\n",
    "    #get the column Q in 01 and sum the value column first from main dataframe \n",
    "\n",
    "    #change the dtyps for section affected only from df main so we can sum this \n",
    "\n",
    "    #check & test \n",
    "    #Get the sum before arimethic expression for df_main as initial value \n",
    "    result_mod7_before = df_output[paste_here].sum(axis=0)\n",
    "    input_mod7_before = df_output[sum_this].sum(axis=0)\n",
    "    input_mod7_before = input_mod7_before.sum()\n",
    "    \n",
    "    #get the sum of column after in df_mod7 and after ammendment made in \n",
    "    result_mod7_after = df_output[paste_here].sum(axis=0)\n",
    "    input_mod7_after = df_output[sum_this].sum(axis=0)\n",
    "    input_mod7_after = input_mod7_after.sum()\n",
    "\n",
    "    if result_mod7_after[0] == input_mod7_after:\n",
    "        print(f'{paste_here} successfully aggregated & match with source value which initially {result_mod7_before[0]} turned to {result_mod7_after[0]}')\n",
    "    else:\n",
    "        print(f'{paste_here} warning, result and source do not match, recheck data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "598b89d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X23204_F6201'] successfully aggregated & match with source value which initially 506 turned to 506\n",
      "['X23204_F6202'] successfully aggregated & match with source value which initially 1191 turned to 1191\n",
      "['X23204_F6203'] successfully aggregated & match with source value which initially 1518 turned to 1518\n",
      "['X23204_F6204'] successfully aggregated & match with source value which initially 1726 turned to 1726\n",
      "['X23204_F6205'] successfully aggregated & match with source value which initially 767 turned to 767\n",
      "['X23204_F6206'] successfully aggregated & match with source value which initially 465 turned to 465\n",
      "['X23204_F6207'] successfully aggregated & match with source value which initially 1254 turned to 1254\n",
      "['X23204_F6208'] successfully aggregated & match with source value which initially 9539 turned to 9539\n",
      "['X23204_F6209'] successfully aggregated & match with source value which initially 4712 turned to 4712\n",
      "['X23204_F6210'] successfully aggregated & match with source value which initially 21364 turned to 21364\n"
     ]
    }
   ],
   "source": [
    "#group num is based on dynamic_keys = ['04', '05', '06'] position which is dunamically based on quarter.\n",
    "# sum the value based on pulled dataframe and paste the value into df_main into as ouput \n",
    "grouping_1 = ('1','2','3','4','5','6','7','8','9','0')\n",
    "group_num = dynamic_keys[0]\n",
    "df = df_vabs_mod9\n",
    "df_output = df_final_raw_data\n",
    "\n",
    "for x in grouping_1:\n",
    "    process_dataframe_2(df,group_num,x,df_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc4a7743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X23205_F6301'] successfully aggregated & match with source value which initially 223 turned to 223\n",
      "['X23205_F6302'] successfully aggregated & match with source value which initially 374 turned to 374\n",
      "['X23205_F6303'] successfully aggregated & match with source value which initially 653 turned to 653\n",
      "['X23205_F6304'] successfully aggregated & match with source value which initially 483 turned to 483\n",
      "['X23205_F6305'] successfully aggregated & match with source value which initially 1491 turned to 1491\n",
      "['X23205_F6306'] successfully aggregated & match with source value which initially 581 turned to 581\n",
      "['X23205_F6307'] successfully aggregated & match with source value which initially 308 turned to 308\n",
      "['X23205_F6308'] successfully aggregated & match with source value which initially 4111 turned to 4111\n",
      "['X23205_F6309'] successfully aggregated & match with source value which initially 1766 turned to 1766\n",
      "['X23205_F6310'] successfully aggregated & match with source value which initially 9987 turned to 9987\n"
     ]
    }
   ],
   "source": [
    "#group num is based on dynamic_keys = ['04', '05', '06'] position which is dunamically based on quarter.\n",
    "# sum the value based on pulled dataframe and paste the value into df_main into as ouput \n",
    "grouping_1 = ('1','2','3','4','5','6','7','8','9','0')\n",
    "group_num = dynamic_keys[1]\n",
    "df = df_vabs_mod9\n",
    "df_output = df_final_raw_data\n",
    "\n",
    "for x in grouping_1:\n",
    "    process_dataframe_2(df,group_num,x,df_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82ca7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X23206_F6401'] successfully aggregated & match with source value which initially 175 turned to 175\n",
      "['X23206_F6402'] successfully aggregated & match with source value which initially 405 turned to 405\n",
      "['X23206_F6403'] successfully aggregated & match with source value which initially 688 turned to 688\n",
      "['X23206_F6404'] successfully aggregated & match with source value which initially 417 turned to 417\n",
      "['X23206_F6405'] successfully aggregated & match with source value which initially 740 turned to 740\n",
      "['X23206_F6406'] successfully aggregated & match with source value which initially 309 turned to 309\n",
      "['X23206_F6407'] successfully aggregated & match with source value which initially 434 turned to 434\n",
      "['X23206_F6408'] successfully aggregated & match with source value which initially 4852 turned to 4852\n",
      "['X23206_F6409'] successfully aggregated & match with source value which initially 1559 turned to 1559\n",
      "['X23206_F6410'] successfully aggregated & match with source value which initially 9593 turned to 9593\n"
     ]
    }
   ],
   "source": [
    "#group num is based on dynamic_keys = ['04', '05', '06'] position which is dunamically based on quarter.\n",
    "# sum the value based on pulled dataframe and paste the value into df_main into as ouput \n",
    "grouping_1 = ('1','2','3','4','5','6','7','8','9','0')\n",
    "group_num = dynamic_keys[2]\n",
    "df = df_vabs_mod9\n",
    "df_output = df_final_raw_data\n",
    "\n",
    "for x in grouping_1:\n",
    "    process_dataframe_2(df,group_num,x,df_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82b2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dff40af",
   "metadata": {},
   "source": [
    "### Module 9. Summation all category (1-9) for variable A, B, C, D, E, F, G, H, I, J, L, M, N, O, P, R & X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3aa36081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plan \n",
    "# Refer based on upload files header \n",
    "# identify starting and ending of soalan baru bertambah \n",
    "# Filter out column for variable in list \n",
    "\n",
    "# Variables involved A, B, C, D, E, F, G, H, I, J, L, M, N, O, P, R & X\n",
    "# Range Criteria (SUM of 1-9) = 10 \n",
    "\n",
    "\n",
    "# from main df -> extract to variable that involve only (df_vabs)\n",
    "# from df_vabs run the condition required and replace sum value into column 10 \n",
    "# from df_vabs extract only column 10 into df_sum\n",
    "# replace the value for all rows that have the same column name through looping, dont merge \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "535cce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A23205', 'A23204', 'A23206', 'B23206', 'B23204', 'B23205', 'C23206', 'C23205', 'C23204', 'D23206', 'D23205', 'D23204', 'E23206', 'E23204', 'E23205', 'F23206', 'F23205', 'F23204', 'G23205', 'G23206', 'G23204', 'H23204', 'H23206', 'H23205', 'I23205', 'I23204', 'I23206', 'J23204', 'J23206', 'J23205', 'L23206', 'L23205', 'L23204', 'M23205', 'M23204', 'M23206', 'NEWSSI', 'N23205', 'N23204', 'N23206', 'O23206', 'O23204', 'O23205', 'P23206', 'P23204', 'P23205', 'Q23206', 'Q23205', 'Q23204', 'X23206', 'X23205', 'X23204'] are the unique partial front string to be re arranged before filtering from main dataframe process\n",
      "['04', '05', '06', 'SI'] are unique partial string sorted by quarters ascending \n"
     ]
    }
   ],
   "source": [
    "#Strategy New \n",
    "# Filter affected column based on alphabet inclusive of ammended column X & Q which in module 7 & 8 \n",
    "# Filter partial string that is unique \n",
    "\n",
    "#1. Filter list of column based on variables first \n",
    "vab_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P','Q', 'R ','X']\n",
    "selection_col = ['NEWSSID']\n",
    "for x in df_final_raw_data.columns[first_index:]:\n",
    "\n",
    "    if x[:1] in vab_list:\n",
    "        selection_col.append(x)\n",
    "\n",
    "# print(f'This are columns selected {selection_col} for aggreagation process')\n",
    "\n",
    "\n",
    "\n",
    "df_vabs = df_final_raw_data[selection_col]\n",
    "\n",
    "#1. List all column from filtered to be appended in list_j\n",
    "list_j = []\n",
    "for x in df_vabs.columns:\n",
    "    y = x[:6]\n",
    "    list_j.append(y)\n",
    "    \n",
    "#2. Get the unique list \n",
    "list_j_unique = list(set(list_j))  \n",
    "\n",
    "#3. Get the unique sorted alphabetically \n",
    "sort_alph = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P','Q', 'R ','X']\n",
    "#Sort the list from unique list into separated set of list to make it easier during loop\n",
    "# sample output a = ['L23204', 'M23204', 'N23204', 'O23204', 'P23204', 'Q23204']\n",
    "list_j_sort = sorted(list_j_unique,key=lambda x:(len(x),x[0]))\n",
    "\n",
    "\n",
    "print(f'{list_j_sort} are the unique partial front string to be re arranged before filtering from main dataframe process')\n",
    "\n",
    "#make a dictionary from sortation filter so we can filter based on variables \n",
    "\n",
    "group_dict = {}\n",
    "for col_name in list_j_sort:\n",
    "    end = col_name[-2:]\n",
    "#if column name existing int group_dictionary, then append in created list\n",
    "    if end in group_dict:\n",
    "        group_dict[end].append(col_name)\n",
    "        \n",
    "#if dont have , then create a new one. \n",
    "    else: \n",
    "        group_dict[end] = [col_name]\n",
    "\n",
    "dynamic_keys = sorted(group_dict.keys())\n",
    "\n",
    "print(f'{dynamic_keys} are unique partial string sorted by quarters ascending ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f61abbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From unique dynamic keys ['04', '05', '06', 'SI'] are unique partial string sorted by quarters ascending \n",
    "#dynamic_keys\n",
    "#From ['A23204', 'A23206', 'A23205', 'B23205', 'B23206', 'B23204', 'C23205', 'C23206', 'C23204', 'D23206', 'D23205', 'D23204', 'E23206', 'E23204', 'E23205', 'F23204', 'F23205', 'F23206', 'G23205', 'G23206', 'G23204', 'H23204', 'H23205', 'H23206', 'I23206', 'I23204', 'I23205', 'J23205', 'J23204', 'J23206', 'L23206', 'L23204', 'L23205', 'M23205', 'M23206', 'M23204', 'NEWSSI', 'N23204', 'N23205', 'N23206', 'O23206', 'O23204', 'O23205', 'P23204', 'P23205', 'P23206', 'Q23206', 'Q23204', 'Q23205', 'X23205', 'X23206', 'X23204'] are the unique partial front string to be re arranged before filtering from main dataframe process\n",
    "#group_dict\n",
    "\n",
    "#Loop based on groupdict[0 to max index]\n",
    "# filter column ending with 01 to 09 = col_sum_0109\n",
    "# filter column ending with 10 = col_paste_here_10 \n",
    "# Axxx04_xxxx01 to Axxx04_xxxx09 .sum(axis=1) = sum_val \n",
    "# sum_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf760a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function\n",
    "def aggregate_data(df, df_output, group_num):\n",
    "    z = [x for x in df if any(q in x for q in y)]\n",
    "    ending_to_sum = ('1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "    ending_sum = ('0')\n",
    "\n",
    "    listx = group_dict[group_num]\n",
    "    total_items = len(listx)\n",
    "\n",
    "    #from listx\n",
    "    listx = group_dict[group_num]\n",
    "    total_items = len(listx)\n",
    "    # now we want to loop according to sequence A (01-09).sum()\n",
    "    for n in range(total_items):\n",
    "        listx = group_dict[group_num][n]\n",
    "        for x in listx:\n",
    "            ending_to_sum_col = []\n",
    "            ending_sum_col = []\n",
    "            # tosum_filter_list = [value for value in z if any(value.startswith(prefix) for prefix in listx) and value.endswith(ending_to_sum)]\n",
    "            tosum_filter_list = [value for value in z if value.startswith(listx) and value.endswith(ending_to_sum)]\n",
    "            sum_column = [value for value in z if value.startswith(listx) and value.endswith(ending_sum)]\n",
    "\n",
    "            # append in column temporary for we sum it in main_df \n",
    "            ending_to_sum_col.append(tosum_filter_list)\n",
    "            ending_sum_col.append(sum_column)\n",
    "\n",
    "    #         ending_to_sum_col = ending_to_sum_col[0]\n",
    "    #         ending_sum_col = ending_sum_col[0]\n",
    "            ending_to_sum_col = ending_to_sum_col[0] if ending_to_sum_col else None\n",
    "            ending_sum_col = ending_sum_col[0] if ending_sum_col else None\n",
    "            # in the loop according to list listx A until X we want to get sum and paste the value directly main_df or output_df \n",
    "            df_output[ending_to_sum_col] = df_output[ending_to_sum_col].apply(pd.to_numeric, errors='coerce')\n",
    "            df_output[ending_to_sum_col] = df_output[ending_to_sum_col].fillna(0).astype(int)\n",
    "\n",
    "            df_output[ending_sum_col] = df_output[ending_sum_col].apply(pd.to_numeric, errors='coerce')\n",
    "            df_output[ending_sum_col] = df_output[ending_sum_col].fillna(0).astype(int)\n",
    "\n",
    "            #recheck before sum \n",
    "\n",
    "            result_mod7_before = df_output[ending_sum_col].sum(axis=0)\n",
    "            input_mod7_before = df_output[ending_to_sum_col].sum(axis=0)\n",
    "            input_mod7_before = input_mod7_before.sum()\n",
    "\n",
    "            sum_1to9 = df_output[ending_to_sum_col].sum(axis=1)\n",
    "            df_output[ending_sum_col] = sum_1to9.values.reshape(-1, 1)\n",
    "    #         recheck main_df after sum\n",
    "\n",
    "            result_mod7_after = df_output[ending_sum_col].sum(axis=0)\n",
    "            input_mod7_after = df_output[ending_to_sum_col].sum(axis=0)\n",
    "            input_mod7_after = input_mod7_after.sum()\n",
    "\n",
    "        if result_mod7_after[0] == input_mod7_after:\n",
    "            print(f'{ending_sum_col} successfully aggregated & match with source value which initially source : {result_mod7_before[0]} from {input_mod7_before} &  result : {result_mod7_after[0]} from {input_mod7_after}')\n",
    "        else:\n",
    "            print(f'{ending_sum_col} warning, result and source do not match, recheck data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097b232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9260ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A23204_F0110'] successfully aggregated & match with source value which initially source : 1971601 from 1971601 &  result : 1971601 from 1971601\n",
      "['B23204_F0210'] successfully aggregated & match with source value which initially source : 30009 from 30009 &  result : 30009 from 30009\n",
      "['C23204_F0310'] successfully aggregated & match with source value which initially source : 15394 from 15394 &  result : 15394 from 15394\n",
      "['D23204_F0410'] successfully aggregated & match with source value which initially source : 16035 from 16035 &  result : 16035 from 16035\n",
      "['E23204_F0510'] successfully aggregated & match with source value which initially source : 2029 from 2029 &  result : 2029 from 2029\n",
      "['F23204_F0610'] successfully aggregated & match with source value which initially source : 3614 from 3614 &  result : 3614 from 3614\n",
      "['G23204_F2310'] successfully aggregated & match with source value which initially source : 1993991 from 1993991 &  result : 1993991 from 1993991\n",
      "['H23204_F2410'] successfully aggregated & match with source value which initially source : 6357 from 6357 &  result : 6357 from 6357\n",
      "['I23204_F2510'] successfully aggregated & match with source value which initially source : 937759 from 937759 &  result : 937759 from 937759\n",
      "['J23204_F2610'] successfully aggregated & match with source value which initially source : 324266 from 324266 &  result : 324266 from 324266\n",
      "['L23204_F4110'] successfully aggregated & match with source value which initially source : 1115811765 from 14000713653 &  result : 14000713653 from 14000713653\n",
      "['M23204_F4210'] successfully aggregated & match with source value which initially source : 661042134 from 661042134 &  result : 661042134 from 661042134\n",
      "['N23204_F4310'] successfully aggregated & match with source value which initially source : 653909224 from 653909224 &  result : 653909224 from 653909224\n",
      "['O23204_F4410'] successfully aggregated & match with source value which initially source : 319691121 from 319691121 &  result : 319691121 from 319691121\n",
      "['P23204_F5910'] successfully aggregated & match with source value which initially source : -1947670008 from 2347297288 &  result : 2347297288 from 2347297288\n",
      "['Q23204_F4510'] successfully aggregated & match with source value which initially source : 802784236 from 5097751532 &  result : 5097751532 from 5097751532\n",
      "['X23204_F6210'] successfully aggregated & match with source value which initially source : 21678 from 21678 &  result : 21678 from 21678\n"
     ]
    }
   ],
   "source": [
    "# variables list \n",
    "df = df_vabs\n",
    "df_output = df_final_raw_data\n",
    "group_num = dynamic_keys[0]\n",
    "aggregate_data(df, df_output, group_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3de8a9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A23205_F0710'] successfully aggregated & match with source value which initially source : 3652873 from 3652873 &  result : 3652873 from 3652873\n",
      "['B23205_F0810'] successfully aggregated & match with source value which initially source : 30870 from 30870 &  result : 30870 from 30870\n",
      "['C23205_F0910'] successfully aggregated & match with source value which initially source : 6881 from 6881 &  result : 6881 from 6881\n",
      "['D23205_F1010'] successfully aggregated & match with source value which initially source : 7384 from 7384 &  result : 7384 from 7384\n",
      "['E23205_F1110'] successfully aggregated & match with source value which initially source : 1215 from 1215 &  result : 1215 from 1215\n",
      "['F23205_F1210'] successfully aggregated & match with source value which initially source : 1391 from 1391 &  result : 1391 from 1391\n",
      "['G23205_F2910'] successfully aggregated & match with source value which initially source : 1991606 from 1991606 &  result : 1991606 from 1991606\n",
      "['H23205_F3010'] successfully aggregated & match with source value which initially source : 5925 from 5925 &  result : 5925 from 5925\n",
      "['I23205_F3110'] successfully aggregated & match with source value which initially source : 868186 from 868186 &  result : 868186 from 868186\n",
      "['J23205_F3210'] successfully aggregated & match with source value which initially source : 322085 from 322085 &  result : 322085 from 322085\n",
      "['L23205_F4710'] successfully aggregated & match with source value which initially source : 5010289921 from 9305257217 &  result : 9305257217 from 9305257217\n",
      "['M23205_F4810'] successfully aggregated & match with source value which initially source : 646971178 from 646971178 &  result : 646971178 from 646971178\n",
      "['N23205_F4910'] successfully aggregated & match with source value which initially source : 89533334 from 89533334 &  result : 89533334 from 89533334\n",
      "['O23205_F5010'] successfully aggregated & match with source value which initially source : 453494551 from 453494551 &  result : 453494551 from 453494551\n",
      "['P23205_F6010'] successfully aggregated & match with source value which initially source : 101453526 from 101453526 &  result : 101453526 from 101453526\n",
      "['Q23205_F5110'] successfully aggregated & match with source value which initially source : 6301742510 from 6301742510 &  result : 6301742510 from 6301742510\n",
      "['X23205_F6310'] successfully aggregated & match with source value which initially source : 9990 from 9990 &  result : 9990 from 9990\n"
     ]
    }
   ],
   "source": [
    "# variables list \n",
    "df = df_vabs\n",
    "df_output = df_final_raw_data\n",
    "group_num = dynamic_keys[1]\n",
    "aggregate_data(df, df_output, group_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5007743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A23206_F1310'] successfully aggregated & match with source value which initially source : 1913466 from 1913466 &  result : 1913466 from 1913466\n",
      "['B23206_F1410'] successfully aggregated & match with source value which initially source : 31372 from 31372 &  result : 31372 from 31372\n",
      "['C23206_F1510'] successfully aggregated & match with source value which initially source : 3924 from 3924 &  result : 3924 from 3924\n",
      "['D23206_F1610'] successfully aggregated & match with source value which initially source : 6113 from 6113 &  result : 6113 from 6113\n",
      "['E23206_F1710'] successfully aggregated & match with source value which initially source : 1735 from 1735 &  result : 1735 from 1735\n",
      "['F23206_F1810'] successfully aggregated & match with source value which initially source : 1731 from 1731 &  result : 1731 from 1731\n",
      "['G23206_F3510'] successfully aggregated & match with source value which initially source : 4662727 from 4662727 &  result : 4662727 from 4662727\n",
      "['H23206_F3610'] successfully aggregated & match with source value which initially source : 5263 from 5263 &  result : 5263 from 5263\n",
      "['I23206_F3710'] successfully aggregated & match with source value which initially source : 810648 from 810648 &  result : 810648 from 810648\n",
      "['J23206_F3810'] successfully aggregated & match with source value which initially source : 295398 from 295398 &  result : 295398 from 295398\n",
      "['L23206_F5310'] successfully aggregated & match with source value which initially source : 3064096605 from 11654031197 &  result : 11654031197 from 11654031197\n",
      "['M23206_F5410'] successfully aggregated & match with source value which initially source : 632376140 from 632376140 &  result : 632376140 from 632376140\n",
      "['N23206_F5510'] successfully aggregated & match with source value which initially source : 85187765 from 85187765 &  result : 85187765 from 85187765\n",
      "['O23206_F5610'] successfully aggregated & match with source value which initially source : 415343813 from 415343813 &  result : 415343813 from 415343813\n",
      "['P23206_F6110'] successfully aggregated & match with source value which initially source : 119938839 from 119938839 &  result : 119938839 from 119938839\n",
      "['Q23206_F5710'] successfully aggregated & match with source value which initially source : 4316943162 from 4316943162 &  result : 4316943162 from 4316943162\n",
      "['X23206_F6410'] successfully aggregated & match with source value which initially source : 9579 from 9579 &  result : 9579 from 9579\n"
     ]
    }
   ],
   "source": [
    "# variables list \n",
    "df = df_vabs\n",
    "df_output = df_final_raw_data\n",
    "group_num = dynamic_keys[2]\n",
    "aggregate_data(df, df_output, group_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e6b195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = str(df_final_raw_data['QUARTER'].unique()[0])\n",
    "Y = str(df_final_raw_data['YEAR'].unique()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81e9e365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlalchemy.engine.base.Connection object at 0x00000156D843A020>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://admin:admin@10.251.49.51:5432/postgres')\n",
    "connection = engine.connect()\n",
    "print(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7240d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema='production_micro_frd_sgtgu_quarterly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f20a523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_final_raw_data.to_excel(files_output_path_result_final+'FRD_'+Q+'_'+Y'.xlsx')\n",
    "df_final_raw_data.to_sql(f'FRDQ{Q}Y{Y}',con=engine,schema=schema,if_exists='replace',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1865658a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRDQ2Y2021 Ingested into Database\n"
     ]
    }
   ],
   "source": [
    "print(f'FRDQ{Q}Y{Y} Ingested into Database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f3b7c2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took 2.946709926923116 minutes to run the whole process\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "end_time = time.time() # get the end time \n",
    "time_running = end_time - start_time  # Calculate the time difference\n",
    "minutes = time_running / 60  # Convert time_running to minutes\n",
    "print(f'it took {minutes} minutes to run the whole process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7897ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#to remove data directly\n",
    "def clear_garbage(path):\n",
    "    file_avaialble = [x for x in os.listdir(path)]\n",
    "\n",
    "    try:\n",
    "        for x in file_avaialble:\n",
    "            os.remove(path+'/'+x)\n",
    "            y = str(x).upper()\n",
    "            print(f'{y} excess files from processing has been relocated, contact vendor if you require the files for quality check ')\n",
    "    except Exception as e:\n",
    "        print(f'Error relocating the files: {path} - {e}')\n",
    "\n",
    "\n",
    "#move data to clear \n",
    "def mover(path, destination_folder):\n",
    "    files_available = [x for x in os.listdir(path)]\n",
    "\n",
    "    try:\n",
    "        for file_name in files_available:\n",
    "            source_file = os.path.join(path, file_name)\n",
    "            destination_file = os.path.join(destination_folder, file_name)\n",
    "            shutil.move(source_file, destination_file)\n",
    "            y = str(file_name).upper()\n",
    "            print(f'{y} excess files from processing has been relocated to {destination_folder}. Contact the vendor if you require the files for quality check.')\n",
    "    except Exception as e:\n",
    "        print(f'Error relocating the files: {path} - {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c18ff0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORTING_DOCUMENTS.XLSX excess files from processing has been relocated to C:\\Users\\User\\Master\\0101_data_engineering\\01_projects\\0125_data_transformation\\SGTGU_QUARTERLY\\BIN. Contact the vendor if you require the files for quality check.\n",
      "EJOB_SGT_SUKU_2_TAHUN_2021.CSV excess files from processing has been relocated to C:\\Users\\User\\Master\\0101_data_engineering\\01_projects\\0125_data_transformation\\SGTGU_QUARTERLY\\BIN. Contact the vendor if you require the files for quality check.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = files_input_path_t01\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "path = files_input_path_raw\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n",
    "\n",
    "\n",
    "\n",
    "path = files_output_path_result\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
