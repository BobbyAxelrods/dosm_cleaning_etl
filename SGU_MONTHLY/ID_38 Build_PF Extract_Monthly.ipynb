{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa986b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import traceback \n",
    "import sys \n",
    "import warnings\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time() # get start time \n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7c3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "current_workingpath = os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478c2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read whatever files types being ingested \n",
    "def read_anything(path,num):\n",
    "    #Get all files avaialble in the path (path shall be only 1 file at a time to manage this )\n",
    "    get_working_files = [x for x in os.listdir(path)]\n",
    "#     get_working_files = x\n",
    "    #make this as a function later \n",
    "    if len(get_working_files) == 0:\n",
    "        print('My goodman, no files either csv nor excel found, please recheck in path the existence')\n",
    "        return None\n",
    "    \n",
    "    #Excel found \n",
    "    time_start = time.time()\n",
    "#     get_types_available = os.path.splitext(get_working_files[0])[1]\n",
    "#     file_name = os.path.splitext(get_working_files[0])[0]\n",
    "    for file_name in get_working_files:\n",
    "        get_types_available = os.path.splitext(file_name)[1]\n",
    "        if get_types_available.endswith('.xlsx'):\n",
    "            \n",
    "            time_start = time.time()\n",
    "            print('We found excel files, hence we will read it and save to df_master, hold a moment ....')\n",
    "            df_master = pd.read_excel(path+'/'+file_name,dtype=str,header=num).dropna(how='all')\n",
    "\n",
    "            int_columns = []\n",
    "            for col in df_master.columns:\n",
    "                if df_master[col].notnull().all() and df_master[col].str.isdigit().all():\n",
    "                    int_columns.append(col)\n",
    "\n",
    "            df_master[int_columns] = df_master[int_columns].astype(int)\n",
    "            time_end = time.time()\n",
    "\n",
    "            diff_time = time_end - time_start\n",
    "            print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "            return df_master\n",
    "\n",
    "        elif get_types_available.endswith('.csv'):\n",
    "            print('We found csv files, hence we will read it and save to df_master, hold a moment ....')\n",
    "            time_start = time.time()  \n",
    "            try:\n",
    "                df_master = pd.read_csv(os.path.join(path,file_name), skip_blank_lines=True,dtype=str,header=num).dropna(how='all')\n",
    "            except UnicodeDecodeError:\n",
    "                # If 'utf-8' fails, try 'ISO-8859-1' encoding\n",
    "                df_master = pd.read_csv(os.path.join(path,file_name), encoding='ISO-8859-1', skip_blank_lines=True,dtype=str,header=num).dropna(how='all')\n",
    "            int_columns = []\n",
    "            for col in df_master.columns:\n",
    "                if df_master[col].notnull().all() and df_master[col].str.isdigit().all():\n",
    "                    int_columns.append(col)\n",
    "            df_master[int_columns] = df_master[int_columns].astype(int)\n",
    "            time_end = time.time()\n",
    "            diff_time = time_end - time_start\n",
    "            print(f'My performance reading {file_name} file took : {diff_time} seconds')\n",
    "            return df_master\n",
    "        \n",
    "    print('No suitable files (csv or excel) found in the specified path. Continuing to search...')\n",
    "    return df_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670c122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f991ecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found excel files, hence we will read it and save to df_master, hold a moment ....\n",
      "My performance reading JR5M07Y2021_DATA RAW.xlsx file took : 6.598858833312988 seconds\n"
     ]
    }
   ],
   "source": [
    "# Change SGU xlsx month here\n",
    "# SGU_Raw = \"G:/Works/!!Intern/DATA SGU SI/SGU Data/JR5_DATA RAW/JR5M01Y2021_DATA RAW.xlsx\"\n",
    "# SGU_sheet_name = \"RX3502_JR5 \"\n",
    "\n",
    "#get current path and join into folder \n",
    "\n",
    "current_path = os.getcwd()\n",
    "filesinput_jr5 = os.path.join(current_path, 'INPUT_RAWDATA_JR5')\n",
    "bin_path = os.path.join(current_path, 'BIN')\n",
    "\n",
    "\n",
    "#Read anything code here \n",
    "df01 = read_anything(filesinput_jr5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15070b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGU_raw = pd.read_excel(SGU_Raw, sheet_name=SGU_sheet_name)\n",
    "string_columns = ['NG', 'DP', 'DB', 'BP', 'BP2', 'Converted BP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6268a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df01 = pd.read_excel(SGU_Raw, sheet_name=SGU_sheet_name, \n",
    "#                      dtype={col: str for col in string_columns})\n",
    "df01[string_columns] = df01[string_columns].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b19c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace \n",
    "df01.columns = df01.columns.str.strip()\n",
    "\n",
    "# replace space with underscore\n",
    "df01 = df01.rename(columns=lambda x: x.replace(\" \", \"\"))\n",
    "df01 = df01.rename(columns=lambda x: x.replace(\"_\", \"\"))\n",
    "\n",
    "# uppercasing column headers\n",
    "df01.columns = df01.columns.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a3f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_lengths(df01, columns, required_lengths):\n",
    "    for col, req_len in zip(columns, required_lengths):\n",
    "        df01[col] = df01[col].astype(str).str.zfill(req_len)\n",
    "        df01[col] = df01[col].str[:req_len]\n",
    "    \n",
    "    return df01\n",
    "\n",
    "columns = ['NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']\n",
    "required_lengths = [2, 2, 3, 3, 3, 3, 1, 4, 2, 1, 3, 2, 2, 1, 4, 2]\n",
    "\n",
    "df01 = check_column_lengths(df01, columns, required_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74747c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']\n",
    "df01['ID_38'] = df01.loc[:, columns_to_convert].astype(str).apply(''.join, axis=1)\n",
    "df01['ID_38'] = df01['ID_38'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7493e6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 NOID generated for JR5 Raw and all NOID 38 are unique \n"
     ]
    }
   ],
   "source": [
    "#check if unique \n",
    "df_sorted = df01.sort_values(by='ID_38')\n",
    "unique_A = df_sorted['ID_38'].drop_duplicates()\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# filtered_df = df_sorted[df_sorted['ID_38'] != 38]\n",
    "# counts = filtered_df['ID_38'].value_counts().sort_index()\n",
    "# a = counts.index\n",
    "# result = pd.DataFrame({'Non_ID_38': counts.index, 'Count': counts.values})\n",
    "# out = result.to_string(index=False)\n",
    "# if a.empty:\n",
    "#     print('38ID generated and there are standard with 38 digits ')\n",
    "# else: \n",
    "#     print('')\n",
    "counts = df_sorted['ID_38'].value_counts().sort_index()\n",
    "result = pd.DataFrame({'ID_38': counts.index, 'Count': counts.values})\n",
    "# print(result.to_string(index=False))\n",
    "\n",
    "filtered_counts = result[result['Count'] > 2]\n",
    "if filtered_counts.empty:\n",
    "    print('38 NOID generated for JR5 Raw and all NOID 38 are unique ')\n",
    "else: \n",
    "    print('Recheck since there are NOID that are not unique and duplicated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e712b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change STB xlsx month here\n",
    "# STB_Raw = \"G:/Works/!!Intern/DATA SGU SI/STB Data/JR42021BULANAN_FINAL_CSV/dsB012021STB.xlsx\"\n",
    "# STB_sheet_name = \"Data\"\n",
    "\n",
    "\n",
    "#pickup data from STB Final data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa7506d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get user input from UI in db input (For which months and year )\n",
    "# Schema name and condition values\n",
    "db_params = {\n",
    "    'host': '10.251.49.51',\n",
    "    'database': 'postgres',\n",
    "    'user': 'admin',\n",
    "    'password': 'admin'\n",
    "}\n",
    "\n",
    "schema_name = 'reference_data'\n",
    "table_name = 'USER_INPUT'\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# cursor.execute((f'SELECT * from {schema_name}.\"{table_name}\" ORDER BY timestamp_column ')\n",
    "cursor.execute(f'''\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS row_num\n",
    "        FROM {schema_name}.\"{table_name}\"\n",
    "    ) AS numbered\n",
    "''')\n",
    "rows= cursor.fetchall()\n",
    "\n",
    "columns = [x[0] for x in cursor.description]\n",
    "df = pd.DataFrame(rows,columns=columns)\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "df = df.fillna(0).astype('int')\n",
    "#add function to truncate table each time new data coming in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db4c9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the latest user input data numbered by max row \n",
    "max_row_num = df['row_num'].max()\n",
    "max_row = df[df['row_num'] == max_row_num]\n",
    "#MODULE 0 : QUERY SQL TABLE BASED ON USER INPUT VALUE FROM FRONT END \n",
    "MONTHS = max_row['month'].values[0]\n",
    "YEAR = max_row['year'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a637e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query table that contain months and years selectfion \n",
    "\n",
    "import psycopg2\n",
    "\n",
    "def get_tables_with_quarter_year(cursor, schema_name, months, year):\n",
    "    # Get a list of all table names in the specified schema\n",
    "    cursor.execute(f\"SELECT table_name FROM information_schema.tables WHERE table_schema = '{schema_name}'\")\n",
    "    table_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    # List to store the tables that meet the condition\n",
    "    result_tables = []\n",
    "\n",
    "    for table_name in table_names:\n",
    "        # Check if the table has the specified columns 'halfyear' and 'year'\n",
    "        cursor.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_schema = '{schema_name}' AND table_name = '{table_name}'\")\n",
    "        column_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        if 'MONTHS' in column_names and 'YEARS' in column_names:\n",
    "            # Execute a query to check if the table has any records with quarter=1 and year=2021\n",
    "            cursor.execute(f'''SELECT 1 FROM {schema_name}.\"{table_name}\" WHERE \"MONTHS\" = {MONTHS} AND \"YEARS\" = {YEAR} LIMIT 1''')\n",
    "            if cursor.fetchone() is not None:\n",
    "                result_tables.append(table_name)\n",
    "\n",
    "    return result_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "223dc36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables with months: 7 and year: 2021:\n",
      "['JR4M07Y2021']\n"
     ]
    }
   ],
   "source": [
    "# engine_specific = create_engine('postgresql+psycopg2://admin:admin@10.251.49.51:5432/postgres',connect_args={'options':'-csearch_path={}'.format('production_indicator_viz')})\n",
    "\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'host': '10.251.49.51',\n",
    "    'database': 'postgres',\n",
    "    'user': 'admin',\n",
    "    'password': 'admin'\n",
    "}\n",
    "\n",
    "# Schema name and condition values\n",
    "\n",
    "schema_name = 'production_micro_final_stb_monthly'\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get the tables that meet the condition\n",
    "result_tables = get_tables_with_quarter_year(cursor, schema_name, MONTHS, YEAR)\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Tables with months: {MONTHS} and year: {YEAR}:\")\n",
    "print(result_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed6226ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JR4M07Y2021 has been stored in dictionary for quality check\n"
     ]
    }
   ],
   "source": [
    "# Store data in dicitonary for wrangling \n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#store all dataframe in after loop in a dictionary \n",
    "dataframe_dict = {}\n",
    "\n",
    "#From result table , query table name and store the value in DF \n",
    "for table_name in result_tables:\n",
    "    query = f'''SELECT * FROM {schema_name}.\"{table_name}\"'''\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "    columns = [y[0] for y in cursor.description]\n",
    "    df = pd.DataFrame(data,columns=columns)\n",
    "    \n",
    "    dataframe_dict[table_name] = df\n",
    "    \n",
    "    print(f'{table_name} has been stored in dictionary for quality check')\n",
    "    \n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95a878e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df02 = dataframe_dict[result_tables[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69d12937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df02 = pd.read_excel(STB_Raw, sheet_name=STB_sheet_name,\n",
    "#                    dtype={col: str for col in string_columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7976a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JR4 FINAL HAVE BEEN STANDARDIZE AND PASSED QUALITY CHECK\n"
     ]
    }
   ],
   "source": [
    "# remove whitespace \n",
    "df02.columns = df02.columns.str.strip()\n",
    "\n",
    "# replace space with underscore\n",
    "df02 = df02.rename(columns=lambda x: x.replace(\" \", \"\"))\n",
    "df02 = df02.rename(columns=lambda x: x.replace(\"_\", \"\"))\n",
    "# uppercasing column headers\n",
    "df02.columns = df02.columns.str.upper()\n",
    "\n",
    "print('JR4 FINAL HAVE BEEN STANDARDIZE AND PASSED QUALITY CHECK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3db5b7d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STB_raw = pd.read_excel(STB_Raw, sheet_name=STB_sheet_name)\n",
    "\n",
    "\n",
    "string_columns = ['NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']\n",
    "df02[string_columns] = df02[string_columns].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "144682f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_lengths(df02, columns, required_lengths):\n",
    "    for col, req_len in zip(columns, required_lengths):\n",
    "        df02[col] = df02[col].astype(str).str.zfill(req_len)\n",
    "        df02[col] = df02[col].str[:req_len]\n",
    "    \n",
    "    return df02\n",
    "\n",
    "columns = ['NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']\n",
    "required_lengths = [2, 2, 3, 3, 3, 3, 1, 4, 2, 1, 3, 2, 2, 1, 4, 2]\n",
    "\n",
    "df02 = check_column_lengths(df02, columns, required_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecec44b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NP  Count\n",
      "101   8122\n",
      "102   5179\n",
      "103   4751\n",
      "104   8627\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df02.sort_values(by='NP')\n",
    "unique_A = df_sorted['NP'].drop_duplicates()\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "counts = df_sorted['NP'].value_counts().sort_index()\n",
    "result = pd.DataFrame({'NP': counts.index, 'Count': counts.values})\n",
    "print(result.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09d15c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['NG', 'DP', 'DB', 'BP', 'BP2', 'CONVERTEDBP', 'ST', 'NOTK', 'NOIR', 'S', 'NP', 'PKIS', 'HMIS', 'J', 'KET', 'B']\n",
    "df02['ID_38'] = df02.loc[:, columns_to_convert].astype(str).apply(''.join, axis=1)\n",
    "df02['ID_38'] = df02['ID_38'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ff0ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 NOID generated for JR4 Final and all NOID 38 are unique \n"
     ]
    }
   ],
   "source": [
    "df_sorted = df02.sort_values(by='ID_38')\n",
    "unique_A = df_sorted['ID_38'].drop_duplicates()\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "counts = df_sorted['ID_38'].value_counts().sort_index()\n",
    "result = pd.DataFrame({'ID_38': counts.index, 'Count': counts.values})\n",
    "filtered_counts = result[result['Count'] > 2]\n",
    "if filtered_counts.empty:\n",
    "    print('38 NOID generated for JR4 Final and all NOID 38 are unique ')\n",
    "else: \n",
    "    print('Recheck since there are NOID that are not unique and duplicated')\n",
    "# print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ec1da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df01['ID_38'] = df01['ID_38'].astype(str)\n",
    "df02['ID_38'] = df02['ID_38'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0efb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df01['NAMA'] = df01['NAMA'].str.upper()\n",
    "df02['NAMA'] = df02['NAMA'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb7459e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df01 = df01.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df02 = df02.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5481d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dict={}\n",
    "dataframe_dict['df01'] = df01\n",
    "dataframe_dict['df02'] = df02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57b7983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tables = ['df01','df02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e3e8478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df01: (7198, 80) where the \n",
      "Shape of df02: (26679, 154) where the \n",
      "The total rows after merging all data from both dataframe for months : 7 year 2021 are : 33877 rows\n"
     ]
    }
   ],
   "source": [
    "#get the shape for df01 & df02 to compare column status \n",
    "column_size= []\n",
    "rowtotal = 0\n",
    "for table_name in result_tables:\n",
    "    if table_name in dataframe_dict:\n",
    "        table_shape = dataframe_dict[table_name].shape\n",
    "        column_size.append(table_shape[1])\n",
    "        rowtotal += table_shape[0] \n",
    "        print(f'Shape of {table_name}: {table_shape} where the ')\n",
    "    else:\n",
    "        print(f'{table_name} not found in the dataframe_dict.')\n",
    "print(f'The total rows after merging all data from both dataframe for months : {MONTHS} year {YEAR} are : {rowtotal} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08bad7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From multi dataframe , the biggest column is 154 \n"
     ]
    }
   ],
   "source": [
    "get_max_size = max(column_size)\n",
    "print(f'From multi dataframe , the biggest column is {get_max_size} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b94e2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_dataframe = pd.concat(updated_dataframe_dict.values(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdf063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df01.sort_values(by='NAMA')\n",
    "unique_A = df_sorted['NAMA'].drop_duplicates()\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "counts = df_sorted['NAMA'].value_counts().sort_index()\n",
    "result = pd.DataFrame({'NAMA': counts.index, 'Count': counts.values})\n",
    "# print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83ce9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sorted = df02.sort_values(by='NAMA')\n",
    "unique_A = df_sorted['NAMA'].drop_duplicates()\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "counts = df_sorted['NAMA'].value_counts().sort_index()\n",
    "result = pd.DataFrame({'NAMA': counts.index, 'Count': counts.values})\n",
    "# print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06a8648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df02.sort_values(by='PEMBERATFINAL')\n",
    "unique_A = df_sorted['PEMBERATFINAL'].drop_duplicates()\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "counts = df_sorted['PEMBERATFINAL'].value_counts().sort_index()\n",
    "result = pd.DataFrame({'PEMBERATFINAL': counts.index, 'Count': counts.values})\n",
    "# print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5a21a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check completed and no issue \n"
     ]
    }
   ],
   "source": [
    "# Iterate over each row in 'df01'\n",
    "for index, row in df01.iterrows():\n",
    "    # Get the values from columns in 'df01'\n",
    "    idnum = row['ID_38']\n",
    "    no_id = row['NAMA']\n",
    "\n",
    "    # Find the corresponding row(s) in 'df02' based on the matching values in 'NAMA' column\n",
    "    replacement_rows = df02.loc[df02['ID_38'] == idnum]\n",
    "\n",
    "    # Check if any matching row(s) were found in 'df02' based on 'NAMA' column\n",
    "    if not replacement_rows.empty:\n",
    "        # Get the values from columns in 'df02'\n",
    "        pemberat_final = replacement_rows.iloc[0]['PEMBERATFINAL']\n",
    "\n",
    "        # Update the values in 'df01' with the matching values from 'df02'\n",
    "        df01.at[index, 'PEMBERATFINAL'] = pemberat_final\n",
    "\n",
    "#         print(f\"Matching ID found for row {index}\")\n",
    "    else:\n",
    "        # Find the corresponding row(s) in 'df02' based on the matching values in 'NO ID' column\n",
    "        replacement_rows = df02.loc[df02['NAMA'] == no_id]\n",
    "\n",
    "        # Check if any matching row(s) were found in 'df02' based on 'NO ID' column\n",
    "        if not replacement_rows.empty:\n",
    "            # Get the values from columns in 'df02'\n",
    "            pemberat_final = replacement_rows.iloc[0]['PEMBERATFINAL']\n",
    "\n",
    "            # Update the values in 'df01' with the matching values from 'df02'\n",
    "            df01.at[index, 'PEMBERATFINAL'] = pemberat_final\n",
    "\n",
    "#             print(f\"Matching ID found based on 'NAMA' for row {index}\")\n",
    "        else:\n",
    "            print(f\"No matching ID found for row {index}\")\n",
    "print('Check completed and no issue ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70bc7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "duplicates = df01[df01.duplicated('ID_38')]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"No duplicates found.\")\n",
    "else:\n",
    "    print(\"Duplicates found:\")\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9da15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7baae19e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df01['MONTHS'] = MONTHS\n",
    "df01['YEARS'] = YEAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbb644aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_naming = f'JR5M{MONTHS}Y{YEAR}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3590cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlalchemy.engine.base.Connection object at 0x0000017DC56266E0>\n"
     ]
    }
   ],
   "source": [
    "schema = 'production_micro_fc_sgu_monthly'\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://admin:admin@10.251.49.51:5432/postgres')\n",
    "connection = engine.connect()\n",
    "print(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1f163ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingested into database \n"
     ]
    }
   ],
   "source": [
    "df01.to_sql(file_naming,con=engine,schema=schema, index=False,if_exists='replace')\n",
    "print('Data ingested into database ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d121bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_folder = \"G:/Works/!!Intern/DATA SGU SI/SGU Data/SGU Output/Bulanan/\"\n",
    "# filename = os.path.basename(SGU_Raw)\n",
    "# file_naming = filename[:16]\n",
    "# output_file = f\"{file_naming}_FC.csv\"\n",
    "# df01.to_csv(os.path.join(output_folder, output_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14709dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#move data to clear \n",
    "def mover(path, destination_folder):\n",
    "    files_available = [x for x in os.listdir(path)]\n",
    "\n",
    "    try:\n",
    "        for file_name in files_available:\n",
    "            source_file = os.path.join(path, file_name)\n",
    "            destination_file = os.path.join(destination_folder, file_name)\n",
    "            shutil.move(source_file, destination_file)\n",
    "            y = str(file_name).upper()\n",
    "            print(f'{y} excess files from processing has been relocated to {destination_folder}. Contact the vendor if you require the files for quality check.')\n",
    "    except Exception as e:\n",
    "        print(f'Error relocating the files: {path} - {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f57d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JR5M07Y2021_DATA RAW.XLSX excess files from processing has been relocated to C:\\Users\\User\\Master\\0101_data_engineering\\01_projects\\0125_data_transformation\\SGU_MONTHLY\\BIN. Contact the vendor if you require the files for quality check.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = filesinput_jr5\n",
    "destination_folder = bin_path\n",
    "mover(path, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cbcf03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took 1.557175350189209 minutes to run the whole process\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "end_time = time.time() # get the end time \n",
    "time_running = end_time - start_time  # Calculate the time difference\n",
    "minutes = time_running / 60  # Convert time_running to minutes\n",
    "print(f'it took {minutes} minutes to run the whole process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda657b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
